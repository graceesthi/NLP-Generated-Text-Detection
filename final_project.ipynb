{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4_g1yxfefP2r"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ueztbJVrVsP5"
   },
   "source": [
    "# Data\n",
    "We will use the Hybrid subset of Vijini et al. in which some sentences of human-written abstracts where replaced by automatically-generated text. Experiments on the fully-generated subsets (or any other dataset) may provide bonus points.\n",
    "\n",
    "There are no train-test split provided in the paper but we keep 80% to train and 20% to test, following Vijini et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bQgqgPz3VsP6",
    "outputId": "9ae74a8d-daf0-4807-9c6d-27e7d63fd695",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !curl -L -o main.zip https://github.com/vijini/GeneratedTextDetection/archive/refs/heads/main.zip\n",
    "# !tar -xf main.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N2CEFL-PVsP6"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UDIRAm1AVsP7"
   },
   "outputs": [],
   "source": [
    "root = Path(\"GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cRAWm2mrVsP7"
   },
   "outputs": [],
   "source": [
    "train_texts, train_labels, test_texts, test_labels = [], [], [], []\n",
    "for path in root.glob(\"*.txt\"):\n",
    "    with open(path, 'rt') as file:\n",
    "        text = file.read()\n",
    "    label = int(path.name.endswith(\"generatedAbstract.txt\"))\n",
    "    doc_id = int(path.name.split(\"_\")[0].split(\".\")[-1])\n",
    "    if doc_id < 10522:\n",
    "        test_texts.append(text)\n",
    "        test_labels.append(label)\n",
    "    else:\n",
    "        train_texts.append(text)\n",
    "        train_labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4QVUO6vAVsP8",
    "outputId": "086f3677-8f50-4671-9d2a-c0fcbe06697d"
   },
   "outputs": [],
   "source": [
    "len(train_texts), len(train_labels), len(test_texts), len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QwMK9c6iVsP8",
    "outputId": "fcd141a0-28fd-41de-a270-45e55976f820"
   },
   "outputs": [],
   "source": [
    "train_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "luLbPRciVsP8",
    "outputId": "07c3409e-9d04-4ff0-f76f-38777bf47315"
   },
   "outputs": [],
   "source": [
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wxv54Za-VsP9",
    "outputId": "31762824-95b5-4721-c220-6b47f05ae1b5"
   },
   "outputs": [],
   "source": [
    "train_texts[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cJpoiYt5VsP9",
    "outputId": "6e350cde-c76d-41aa-a21d-9c99ceaef624"
   },
   "outputs": [],
   "source": [
    "train_labels[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDetectionDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def train_model(model, train_loader, val_loader, device, epochs=3):\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "    \n",
    "    best_accuracy = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f'\\nEpoch {epoch + 1}/{epochs}')\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc='Training'):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_losses.append(loss.item())\n",
    "            \n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            train_preds.extend(preds)\n",
    "            train_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        train_accuracy = np.mean(np.array(train_preds) == np.array(train_labels))\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc='Validation'):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "                \n",
    "                loss = outputs.loss\n",
    "                logits = outputs.logits\n",
    "                \n",
    "                val_losses.append(loss.item())\n",
    "                \n",
    "                preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "                val_preds.extend(preds)\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_accuracy = np.mean(np.array(val_preds) == np.array(val_labels))\n",
    "        \n",
    "        print(f'\\nTraining Loss: {np.mean(train_losses):.4f}')\n",
    "        print(f'Training Accuracy: {train_accuracy:.4f}')\n",
    "        print(f'Validation Loss: {np.mean(val_losses):.4f}')\n",
    "        print(f'Validation Accuracy: {val_accuracy:.4f}')\n",
    "        \n",
    "        if val_accuracy > best_accuracy:\n",
    "            best_accuracy = val_accuracy\n",
    "            # Save the best model if needed\n",
    "            # torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    test_preds = []\n",
    "    test_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc='Testing'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            \n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            test_preds.extend(preds)\n",
    "            test_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(test_labels, test_preds))\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(test_labels, test_preds))\n",
    "\n",
    "def main():\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Initialize tokenizer and model\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        'bert-base-uncased',\n",
    "        num_labels=2\n",
    "    ).to(device)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = TextDetectionDataset(train_texts, train_labels, tokenizer)\n",
    "    test_dataset = TextDetectionDataset(test_texts, test_labels, tokenizer)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=8)\n",
    "    \n",
    "    # Train the model\n",
    "    train_model(model, train_loader, test_loader, device)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    evaluate_model(model, test_loader, device)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
