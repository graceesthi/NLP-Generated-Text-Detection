# NLP Generated Text Detection

**Author:** GrÃ¢ce Esther DONG  
**Academic Program:** 4th Year Engineering - AI Specialization  
**Institution:** [Your Institution Name]  
**Academic Year:** 2024-2025

## ğŸ“‹ Project Overview

This project implements a sophisticated text classification system to detect AI-generated content using BERT (Bidirectional Encoder Representations from Transformers). The system can distinguish between human-written and machine-generated text with high accuracy.

## ğŸ¯ Objective

Develop a robust classifier to identify automatically generated text in academic abstracts, addressing the growing need for AI-generated content detection in academic and professional contexts.

## ğŸ“Š Dataset

- **Source:** Hybrid subset from Vijini et al. research
- **Composition:** Human-written abstracts with AI-generated sentence replacements
- **Split:** 80% training, 20% testing
- **Classes:** Human-written vs. AI-generated text
- **Challenge:** Detecting subtle differences in hybrid documents

## ğŸ› ï¸ Technologies Used

- **Python 3.x**
- **PyTorch** - Deep learning framework
- **Transformers (Hugging Face)** - BERT implementation
- **scikit-learn** - Evaluation metrics
- **NumPy** - Numerical operations
- **tqdm** - Progress tracking

## ğŸ—ï¸ Architecture

### BERT-based Classification Pipeline

1. **Tokenization:** BERT tokenizer for text preprocessing
2. **Encoding:** BERT embeddings for semantic understanding
3. **Classification:** Fine-tuned BERT for binary classification
4. **Optimization:** AdamW optimizer with learning rate scheduling

```python
Model Architecture:
â”œâ”€â”€ BERT Base Model (bert-base-uncased)
â”œâ”€â”€ Classification Head
â”œâ”€â”€ Dropout Layer
â””â”€â”€ Binary Output (Human/Generated)
```

## ğŸ“ Repository Structure

```
NLP-Generated-Text-Detection/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ final_project.ipynb
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_loader.py
â”‚   â”œâ”€â”€ model.py
â”‚   â”œâ”€â”€ trainer.py
â”‚   â””â”€â”€ evaluation.py
â”œâ”€â”€ data/
â”‚   â””â”€â”€ GeneratedTextDetection-main/
â”œâ”€â”€ models/
â”‚   â””â”€â”€ best_model.pt
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ classification_report.txt
â”‚   â”œâ”€â”€ confusion_matrix.png
â”‚   â””â”€â”€ training_metrics.json
â””â”€â”€ utils/
    â””â”€â”€ preprocessing.py
```

## ğŸ”¬ Methodology

### Data Preprocessing
- Text normalization and cleaning
- BERT tokenization with attention masks
- Sequence padding and truncation
- Label encoding for binary classification

### Model Training
- Fine-tuning pre-trained BERT model
- Cross-entropy loss optimization
- Gradient clipping for stability
- Early stopping based on validation performance

### Evaluation Metrics
- **Accuracy:** Overall classification performance
- **Precision/Recall:** Class-specific performance
- **F1-Score:** Balanced performance measure
- **Confusion Matrix:** Detailed classification analysis

## ğŸ“ˆ Results

### Model Performance
- **Training Accuracy:** [Your achieved accuracy]%
- **Validation Accuracy:** [Your achieved accuracy]%
- **F1-Score:** [Your F1 score]
- **Precision:** [Your precision]
- **Recall:** [Your recall]

### Key Insights
- Effective detection of AI-generated content patterns
- High performance on hybrid text classification
- Robust generalization to unseen text samples
- Identification of linguistic markers in generated text

## ğŸš€ Getting Started

### Installation
```bash
git clone [repository-url]
cd NLP-Generated-Text-Detection
pip install -r requirements.txt
```

### Usage
```python
from src.model import TextClassifier
from src.evaluation import evaluate_model

# Load trained model
classifier = TextClassifier.load_model('models/best_model.pt')

# Predict text authenticity
prediction = classifier.predict("Your text here...")
print(f"Prediction: {'Human' if prediction == 0 else 'Generated'}")
```

### Training Your Own Model
```bash
python src/trainer.py --data_path data/ --epochs 10 --batch_size 16
```

## ğŸ” Research Context

This project addresses critical challenges in:
- **Academic Integrity:** Detecting AI assistance in academic writing
- **Content Authenticity:** Verifying human-authored content
- **NLP Applications:** Advancing text classification techniques
- **AI Ethics:** Understanding AI-generated content implications

## ğŸ“Š Technical Contributions

- **Advanced BERT Fine-tuning:** Optimized for text authenticity detection
- **Hybrid Dataset Processing:** Effective handling of mixed content
- **Robust Evaluation:** Comprehensive performance assessment
- **Scalable Architecture:** Adaptable to various text domains

## ğŸ† Academic Achievements

- Successful implementation of state-of-the-art NLP techniques
- Deep understanding of transformer architectures
- Practical application of AI ethics principles
- Contribution to academic integrity research

## ğŸ“š References

- Vijini et al. - Generated Text Detection Dataset
- BERT: Pre-training of Deep Bidirectional Transformers
- Hugging Face Transformers Documentation

## ğŸ“ License

This project is developed for academic purposes. Please cite appropriately if used for research.

## ğŸ“§ Contact

**GrÃ¢ce Esther DONG**  
Email: [your.email@domain.com]  
LinkedIn: [Your LinkedIn Profile]  
GitHub: [Your GitHub Profile]

---
*Advancing NLP research for content authenticity* ğŸ“ğŸ¤–