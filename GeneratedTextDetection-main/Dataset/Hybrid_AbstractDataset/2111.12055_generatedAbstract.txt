GPU compilers are complex software programs with many optimizations specific to target hardware. These optimizations are often controlled by heuristics hand-designed by compiler experts using time- and resource-intensive processes. In this paper,  we use a single layer of neural machine translation (NMT) to improve a NMT-like model that can capture features of words. We also show that an additional layer of translation information can improve this model, by using a more fluent corpus of NMT features instead of simple, word-level ones. We show that our machine learning-based compiler autotuning framework matches or surpasses the frame rates for 98% of graphics benchmarks with an average uplift of 1.6% up to 15.8%.