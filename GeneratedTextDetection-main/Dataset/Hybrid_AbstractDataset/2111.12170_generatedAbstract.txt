Recent advancements in self-supervised learning have reduced the gap between su- pervised and unsupervised representation learning. However, most self-supervised and deep clustering techniques rely heavily on data augmentation, rendering them ineffective for many learning tasks where insufficient domain knowledge exists for performing augmentation. We propose an approach to augment these approaches with new domain knowledge (Kappa et al ., 2016; Hwang et al., 2016). In this paper , we provide an overview of a multi-domain modeling approach. Our method builds upon the existing deep clustering frameworks and requires no separate student model. The proposed method outperforms existing domain agnostic (augmentation-free) algorithms on CIFAR-10. We empirically demonstrate that knowledge distillation can improve unsupervised representation learning by extracting richer ‘dark knowledge’ from the model than using predicted labels alone. Preliminary experiments also suggest that self-distillation improves the convergence of DeepCluster-v2.