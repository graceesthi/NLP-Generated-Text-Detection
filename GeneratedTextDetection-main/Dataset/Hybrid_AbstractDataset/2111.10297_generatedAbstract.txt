Learning a Markov Decision Process (MDP) from a fixed batch of trajectories is a non-trivial task whose outcome’s quality depends on both the amount and the diversity of the sampled regions of the state-action space. Yet, many MDPs are endowed with invariant reward and transition functions with respect to some transformations of the current state and action. Being able to detect and exploit these structures could benefit not only the learning of the MDP but also the computation of its subsequent optimal control policy. In this work we propose a novel approach that uses a modified version of the MDP in order to acquire such invariant relations. We tested the proposed approach in a discrete toroidal grid environment and in two notorious environments of OpenAI’s Gym Learning Suite. The results demonstrate that our MDP is robust enough to adapt to a variety of configurations, including nonlinear transitions, and a significant improvement in performance with the use of a transition-oriented algorithm.