In recent years, researchers have made significant progress in devising reinforcement-learning algorithms for optimizing linear temporal logic (LTL) objectives and LTL-like objec- tives. Despite these advancements, there are fundamental lim- itations to how well this problem can be solved that previous studies have alluded to but, to our knowledge, have not ex- amined in depth. In this paper, we address theoretically the hardness of learning with general LTL objectives. We for- malize the problem under the probably approximately correct learning in Markov decision processes (PAC-MDP) frame- work, a standard framework for measuring sample complex- ity in reinforcement learning. In this formalization, we prove that the optimal policy for any LTL formula is PAC-MDP- learnable only if the formula is in the most limited class in the LTL hierarchy, consisting of only finite-horizon-decidable properties. Practically, our result implies that it is impossi- ble for a reinforcement-learning algorithm to obtain a PAC- MDP guarantee on the performance of its learned policy after finitely many interactions with an unconstrained environment for non-finite-horizon-decidable LTL objectives.