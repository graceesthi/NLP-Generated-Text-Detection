Models of human motion commonly focus either on tra- jectory prediction or action classification but rarely both. The marked heterogeneity and intricate compositionality of human motion render each task vulnerable to the data degradation and distributional shift common to real-world scenarios. A sufficiently expressive generative model of ac- tion could in theory enable data conditioning and distribu- tional resilience within a unified framework applicable to both tasks. Here we propose a novel architecture based on hierarchical variational autoencoders and deep graph con- volutional neural networks for generating a holistic model of action over multiple time-scales. We show this Hierar- chical Graph-convolutional Variational Autoencoder (HG- VAE) to be capable of generating coherent actions, detect- ing out-of-distribution data, and imputing missing data by gradient ascent on the model’s posterior. Trained and eval- uated on H3.6M and the largest collection of open source human motion data, AMASS, we show HG-VAE can facil- itate downstream discriminative learning better than base- line models.