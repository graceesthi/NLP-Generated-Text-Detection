Active inference is a state-of-the-art framework for modelling the brain that explains a wide range of mechanisms such as habit for- mation, dopaminergic discharge and curiosity. However, recent im- plementations suffer from an exponential (space and time) com- plexity class when computing the prior over all the possible policies up to the time horizon. Fountas et al. (2020) used Monte Carlo tree search to address this problem, leading to very good results in two different tasks. Additionally, Champion et al. (2021a) pro- posed a tree search approach based on structure learning. This was enabled by the development of a variational message passing ap- proach to active inference (Champion et al., 2021b), which enables compositional construction of Bayesian networks for active infer- ence. However, this message passing tree search approach, which we call branching-time active inference (BTAI), has never been tested empirically. In this paper, we propose a tree-based BTA I approach that provides probabilistic posterior classification of the current tree. In this context, we show that both improved prior preferences and deeper search help mitigate the vulnerability to lo- cal minima. Then, we compare BTAI to standard active inference (AI) on a graph navigation task. We show that our tree-based BTA I outperforms current methods, as demonstrated by the superior graph navigation performance on the Parabola (Cohn et al ., 2013). Finally, we evaluate the relative performance of the two tree based methods on the test dataset, including the Parabola test dataset. However, BTAI explores the space of policies more efficiently, successfully scaling to larger graphs.