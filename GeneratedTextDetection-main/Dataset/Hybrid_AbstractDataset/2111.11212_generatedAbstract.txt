In computational reinforcement learning, a growing body of work seeks to express an agent’s model of the world through predictions about future sensations. In this manuscript we focus on predictions expressed as General Value Functions: tem- porally extended estimates of the accumulation of a future signal. One challenge is determining from the infinitely many predictions that the agent could possibly make which might support decision-making. In this work, we propose a novel methodology for discovering and modeling these general values, which is also grounded in the intuition of general-value functions and the need for predictors to express their knowledge in more flexible and efficient manner. To that end, we introduce a partially observable domain suited to this investigation. We then demonstrate that through interaction with the environment an agent can independently select predictions that resolve the partial-observability, resulting in performance similar to expertly chosen value functions. By learning, rather than manually specifying these pre- dictions, we enable the agent to identify useful predictions in a self-supervised manner, taking a step towards truly autonomous systems.