Bayesian optimization is a powerful paradigm to op- timize black-box functions based on scarce and noisy data. Its data efficiency can be further improved by transfer learning from related tasks. While recent trans- fer models meta-learn a prior based on large amount of data, in the low-data regime methods that exploit the closed-form posterior of Gaussian processes (GPs) have an advantage. In this setting, several analytically tractable transfer-model posteriors have been proposed, but the relative advantages of these methods are not well understood. In this paper, we propose a novel model that achieves the best results in a highly-explicit domain of non-linear equations . As part of the analysis, we develop a novel closed-form boosted GP transfer model that fits be- tween existing approaches in terms of complexity. We evaluate the performance of the different approaches in large-scale experiments and highlight strengths and weaknesses of the different transfer-learning methods.