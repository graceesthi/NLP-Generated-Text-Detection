Prediction+optimization is a common real-world paradigm where we have to pre- dict problem parameters before solving the optimization problem. However, the criteria by which the prediction model is trained are often inconsistent with the goal of the downstream optimization problem. Recently, decision-focused pre- diction approaches, such as SPO+ and direct optimization, have been proposed to fill this gap. However, they cannot directly handle the soft constraints with the max operator required in many real-world objectives. This paper proposes a novel approach to this problem that avoids the difficulty of prediction by directly learning to map the inferred target word sequence to an  approximation for  prediction. We propose a new non -linear model that allows the model to optimize the target word sequence directly (without  learning from the output of its  representation, like the model trained on) rather than relying on the output from the target word. We evaluate our method in three applications extended with soft constraints: synthetic linear programming, portfolio optimization, and resource provisioning, demon- strating that our method outperforms traditional two-staged methods and other decision-focused approaches.