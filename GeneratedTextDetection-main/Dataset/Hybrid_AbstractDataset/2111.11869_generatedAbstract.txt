Machine learning models, especially deep models, may unintentionally remember information about their training data. Malicious attackers can thus pilfer some property about training data by attacking the model via membership inference attack or model inversion attack. Some regula- tions, such as the EU’s GDPR, have enacted ”The Right to Be Forgotten” to protect users’ data privacy, enhancing in- dividuals’ sovereignty over their data. Therefore, removing training data information from a trained model has become a critical issue. In this paper, we present a novel class of discriminative information extraction techniques , in which we use a discriminative framework for learning neural representations of hidden states. We have experimented on five commonly used datasets, and the experimental re- sults show the efficiency of our method.