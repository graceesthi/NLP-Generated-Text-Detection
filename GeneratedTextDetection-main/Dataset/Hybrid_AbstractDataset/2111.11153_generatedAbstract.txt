The lottery ticket hypothesis has sparked the rapid development of pruning al- gorithms that perform structure learning by identifying a sparse subnetwork of a large randomly initialized neural network. The existence of such ’winning tick- ets’ has been proven theoretically but at suboptimal sparsity levels. Contemporary pruning algorithms have furthermore been struggling to identify sparse lottery tickets for complex learning tasks. Is this suboptimal sparsity merely an arti- fact of existence proofs and algorithms or a general limitation of the pruning ap- proach? And, if very sparse tickets exist, are current algorithms able to find them or are further improvements needed to achieve effective network compression? To answer these questions systematically, we develop a novel algorithm for determining sparse non-le ast squares. For three common challenges in machine learning, we hand-craft extremely sparse network topologies, plant them in large neural networks, and evaluate state-of- the-art lottery ticket pruning methods. We find that current limitations of pruning algorithms to identify extremely sparse tickets are likely of algorithmic rather than fundamental nature and anticipate that our planting framework will facilitate fu- ture developments of efficient pruning algorithms, as we have addressed the issue of missing baselines in the field raised by Frankle et al. (2021).