Adversarial examples can easily degrade the clas- sification performance in neural networks. Empirical methods for promoting robustness to such examples have been proposed, but often lack both analytical insights and formal guarantees. Recently, some robustness certificates have appeared in the literature based on system theoretic notions. This work pro- poses a possible solution: if we can produce such a certificate that has robustness and confidence annotations, then we can extract comparable CLAS-style information without relying on the CLAS annotations. We also propose an equivalent spectral norm bound for this certificate which is scalable to neural networks with multiple layers. We demonstrate the improved performance against adversarial attacks on a feed-forward neural network trained on MNIST and an Alexnet trained using CIFAR-10.