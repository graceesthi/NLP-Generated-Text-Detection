A plethora of attack methods have been proposed to gener- ate adversarial examples, among which the iterative methods have been demonstrated the ability to find a strong attack. However, the computation of an adversarial perturbation for a new data point requires solving a time-consuming optimiza- tion problem from scratch. To generate a stronger attack, it normally requires updating a data point with more iterations. In this paper, we propose a method to update the perturbed examples as described in [13], which obtains a strong version of the data point that will consistently outperform all other methods. We conduct extensive experiments, and the empirical results demonstrate that state-of-the-art deep neural networks are vulnerable to meta perturbations. We further show that these perturbations are not only image-agnostic, but also model- agnostic, as a single perturbation generalizes well across un- seen data points and different neural network architectures.