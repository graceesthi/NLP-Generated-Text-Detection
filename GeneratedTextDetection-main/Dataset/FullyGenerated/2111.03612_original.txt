Abstract— Through anonymisation and accessibility, social media platforms have facilitated the proliferation of hate speech, prompting increased research in developing automatic methods to identify these texts. This paper explores the classification of sexism in text using a variety of deep neural network model architectures such as Long-Short-Term Memory (LSTMs) and Convolutional Neural Networks (CNNs). These networks are used in conjunction with transfer learning in the form of Bidirectional Encoder Representations from Transformers (BERT) and DistilBERT models, along with data augmentation, to perform binary and multiclass sexism classification on the dataset of tweets and gabs from the sEXism Identification in Social neTworks (EXIST) task in IberLEF 2021. The models are seen to perform comparatively to those from the competition, with the best performances seen using BERT and a multi-filter CNN model. Data augmentation further improves these results for the multi-class classification task. This paper also explores the errors made by the models and discusses the difficulty in automatically classifying sexism due to the subjectivity of the labels and the complexity of natural language used in social media. Keywords— Sexism classification, social media, natural language processing, neural networks, machine learning, BERT, transfer learning. INTRODUCTION Social media has completely altered the way communities are formed and utilised, which provides incredible advantages while also having severe repercussions. Due to the ‘online disinhibition effect’ (Suler, 2004, p. 1), when users are provided with an anonymised and accessible platform, they engage in behaviours they would not partake in when interacting face-to-face (Wright et al., 2019). A significant example of this is the hate speech produced and propagated through social media platforms. Hate speech is defined as language which is ‘insulting, degrading, defaming, negatively stereotyping, or inciting hatred, discrimination or violence against people in virtue of their race, ethnicity, nationality, religion, sexual orientation, disability, gender identity’ (Brown, 2017, p. 1). The prevalence of hate speech in everyday life has increased in correlation with social media usage, particularly during the COVID-19 pandemic, with internet usage levels having increased between 50% to 70% as of early April 2020 (UN Women, 2020). Online hate speech, especially targeted discrimination, has been associated with an increase in hate crimes offline (Hatzipanagos, 2018; Laub, 2019; Relia et al., 2019); therefore, the ability to successfully tackle this issue within the virtual space itself is vital. Sexism refers to a sub-classification of hate speech where the targeted people are typically female. Women are more likely to report having experienced sexual harassment online (16% vs. 5%) or being cyber-stalked (13% vs. 9%) compared to men (Vogels, 2021); with 1 in 10 women reporting having experienced cyber harassment since the age of 15 in the European Union (UN Women, 2020). Women and girls are specifically seen to face a digital gender divide1, especially with the COVID-19 pandemic being the first major one in the age of social media2. While social media platforms like Twitter do ban hate speech3, these policies are enforced primarily through manual methods which cannot scale up to counteract the data being produced (Waseem and Hovy, 2016; Zhang and Luo, 2019). Hence, more automatic methods are essential to successfully tackle this problem. This paper looks at creating and improving neural network models to perform binary and multiclass sexism classification using the dataset provided for the first shared task on sEXism Identification in Social neTworks (EXIST) at IberLEF 2021 (Rodríguez-Sánchez et al., 2021). In this paper, sexism refers to hate speech against women specifically, with the dataset consisting of texts obtained from Twitter and Gab. This paper presents a variety of models including Long-Short-Term Memory networks (LSTMs), Bidirectional Long-Short-Term Memory networks (Bi-LSTMs), and Convolutional Neural Networks (CNNs) tested against this dataset, with the best performance for both tasks seen with a model utilising Bidirectional Encoder Representations from Transformers (BERT) contextual embeddings in conjunction with a CNN containing three filter sizes of 4, 6, and 8. Data augmentation is also seen to improve the model’s performance for the multi-class classification task. The performance metrics are reported for each model with a discussion of the errors presented to explore the difficulty in classifying a subjective topic like sexism in natural language texts. The rest of this paper is organised as follows. In Section 2, related work in the field of hate speech and sexism classification are looked at, Section 3 talks about the dataset used for the experiments, Section 4 talks about system architecture and details the models used in this paper, Section 5 presents and discusses the results, Section 6 talks about error analysis and Section 7 concludes the paper. RELATED WORK Research has been conducted regarding hate speech on various social media platforms like Twitter (Davidson et al., 2017; Shushkevich and Cardiff, 2018; Rodríguez-Sánchez et al., 2020), Facebook, (Vigna et al., 2017; Raiyani et al., 2018; Mandl et al., 2019) and Reddit (Qian et al., 2019). Initial research regarding classifying hate speech was feature based, with approaches based on bag-of-words (BoW), character- level, and word-level n-grams (Kwok and Wang, 2013; Mehdad and Tetreault, 2016; Waseem and Hovy, 2016). Then, traditional machine learning methods like support vector machines (SVM) and logistic regression were used. One of the first uses of machine learning to detect offensive tweets was presented by Xiang et al. (2012) where logistic regression was used as opposed to the utilisation of pattern- based approaches (Gianfortoni et al., 2011; Mondal et al., 2017). One of the first instances of using neural networks and word embeddings to tackle hate speech classification was done by Djuric et al. (2015) where a continuous BoW model was used to learn paragraph2vec embeddings, with these embeddings then used to train a binary classifier. For the 2018 IberEval Automatic Misogyny Identification (AMI) tasks, which required classification of English and Spanish or English and Italian tweets, the majority of the participants utilised SVMs and Ensemble of Classifiers (EoC) (Ahluwalia et al., 2018; Fersini et al., 2018; Pamungkas et al., 2018; Shushkevich and Cardiff, 2018). Good results were also seen through the usage of Bi-LSTMs and Conditional Random Fields (CRFs) on the same task (Goenaga et al., 2018). An alternative architecture was proposed by Zhang and Luo (2019) with two deep neural network models to tackle hate speech classification on Twitter datasets. These models consist of CNN and Gated Recurrent Unit (GRU) architectures with the results outperforming the best methods at the time. More recently, the introduction of BERT has led to new state-of-the-art performances across a range of natural language processing tasks, including text classification (Devlin et al., 2018). BERT is a multi-layer bidirectional transformer encoder which notably uses bidirectional self- attention to learn contextual information between words and sub-words within a text (Alammar, 2018). BERT has been pre-trained using BooksCorpus (800M words) and English Wikipedia (2500M words) on masked language modelling and next sentence prediction (Devlin et al., 2018). This causes the embeddings taken from the model to contain useful contextual information that can be fine-tuned for specific tasks. Rodríguez-Sánchez et al. (2020) show BERT being used to give the best performance on the task of identifying sexist content through fine tuning pre-trained mBERT-Base parameters with a fully connected layer. Multi-label sexism classification was first seen in a paper by Parikh et al. (2020) where a BERT based neural architecture was used along with distributional and word level embeddings. Samghabadi et al. (2020) also show BERT being used without fine-tuning to identify aggression and misogyny in English, Hindi, and Bengali tweets with positive results from the model. Limited research has been conducted on the automatic classification of subtle expressions of sexism encompassing a broad range of categories, compared to the sole use of profanities or explicit hatred against women. Rodríguez- Sánchez et al. (2020) collected instances of various types of sexism, ranging from subtle inequality to explicit violence to create a dataset to then be used in an automatic classification task. The range of expressions collated is similar to the dataset used in this paper. An important point to note is that hate speech and sexism is defined differently across these papers, with offensive language often considered to be equivalent (Davidson et al., 2017). Another challenge for automatic sexism classification is the lack of an established benchmark dataset. Detecting sexist expressions is a challenge for human coders as well, with racist or homophobic tweets often considered to be hate speech while sexist or derogatory terms are found to be offensive as opposed to hateful (Waseem and Hovy, 2016; Davidson et al., 2017). To tackle these issues of subtlety and context, the DistilBERT, which is a lightweight version of BERT with 40% fewer parameters (Sanh et al., 2019), and BERT models used in this paper are fine-tuned with additional layers to allow them to learn contextual embeddings and perform effectively on the sexism classification tasks. CONCLUSION AND FUTURE WORK The increased use of social media has enabled hate speech, including sexist speech, to easily propagate and affect people globally. With online hate speech linked to offline violence, it is essential to successfully classify speech as hateful through automatic methods. This paper presented a variety of deep neural networks using BERT and DistilBERT to differentiate sexist tweets and gabs from non-sexist ones, as well as further classify sexist text into types of sexism using the EXIST dataset. The best model for the binary classification task used BERT along with a CNN architecture using filter sizes of 4, 6, and 8 to achieve an accuracy of 76.2% while the best accuracy from the competition was 77%. The same model along with data augmentation achieved the best performance on the multi-class classification task with an F1 score of 51.9% which was lower than the best F1 score (56%) from the competition. Due to the subjectivity involved in annotating sexist text as well as the complexity of natural language in tweets and gabs, this task proved to be challenging for the models to achieve ideal results on. Categories which contain explicit hatred like ‘sexual-violence’ were seen to be labelled more accurately than more subtle instances of sexism such as those seen under ‘ideological-inequality’. Profanities are often used on social media platforms without necessarily insinuating sexist speech, such as in casual conversation or song lyrics and hence were not very helpful features for classification. Similarly, speech can be sexist without using any explicit and specific words to indicate this. Due to the varying perceptions of sexism by humans, some labels within the dataset were found to have been potentially mislabelled, which creates further challenges for using the dataset to train models. The type of texts (tweets versus gabs) was not seen to affect the BERT + MultiCNN model’s performance significantly for either task. This could be attributed to the use of the convolutional layer with different filter sizes with max pooling. The model was seen to perform better for text of length 500+ characters and <100 characters, although the impact of length would need to be further examined to confirm this by creating more balanced datasets for this purpose. Another avenue for further exploration could be using additional features such as the gender or ethnicity of authors as stated by Waseem and Hovy (2016), although this information may be challenging to obtain. The models trained on the EXIST dataset could also be tested on a different sexism dataset to observe the generalisability of the models across different annotated data. Finally, a benchmark annotated dataset for sexism would also allow for better development and comparison of models.