ABSTRACT Estimating the quality of machine translation systems has been an ongoing challenge for researchers in this field. Many previous attempts at using round-trip translation as a measure of quality have failed, and there is much disagreement as to whether it can be a viable method of quality estimation. In this paper, we revisit round-trip translation, proposing a system which aims to solve the previous pitfalls found with the approach. Our method makes use of recent advances in language representation learning to more accurately gauge the similarity between the original and round-trip translated sentences. Experiments show that while our approach does not reach the performance of current state of the art methods, it may still be an effective approach for some language pairs. Keywords Quality Estimation · Machine Translation ·Introduction The purpose of a quality estimation (QE) system is to determine the quality of a translation without using a reference translation. Traditionally, translation evaluation metrics like BLEU, NIST, METEOR, and TER would be used to assess a machine translation (MT) system’s translation by comparing the translation output to a reference translation created by an expert. However, quality estimation systems eliminate the need for the time-consuming task of manually annotating sentence translations to create this reference. QE “is concerned about predicting the quality of a system’s output for a given input, without any information about the expected output” [Specia et al., 2010]. Quality can be measured in a number of different ways, but in this paper, we will explore the use of round-trip translation in conjunction with sentence encoding models as a means of measuring this. Our approach is to translate an existing translated sentence back into its original language, generate sentence embedding vectors for both the original and round-trip translated sentences, and measure the similarity between the two vectors as a proxy for translation quality. Our justification behind this approach is that if we use an accurate model to translate the sentence back into the original language, the meaning of the sentence will be maintained. Then when we measure the similarity between the original sentence and the round-trip translated sentence, this score will correlate to the quality of the translation model that created the sentence pairs in the first place. Similar to the approach used in Somers [2005], evaluation metrics, such as BLEU, could then be used to evaluate the similarity of these two same language sentences. Nevertheless, we hypothesise that our approach of using sentence embeddings will prove far more useful than these as it can take the semantics of the sentence into consideration along with judging the sentence as a whole rather than word by word. We also think our approach will be an effective way to get around the poor prediction encountered by Somers [2005]. While alternate methods using more complex models have been used in recent years, we hope that the simplicity of our proposition and the novelty of using sentence encoding will bring new insights to this approach and will re-encourage further exploratory research in this area. The remainder of the paper is organised as follows: In Section 2 we look at existing literature, in Section 3 we present our methods, Section 4 details the data used, Section 5 shows and discusses the results and finally, Section 6 concludes our research and proposes future directions for building on our research. Related Work From researching previous works, we saw from QE systems like QuEst [Specia et al., 2013] and QuEst++ [Specia et al., 2015] that machine learning algorithms, like support vector machines and decision trees, have proved quite successful as quality estimation tools in situations where a lot of work on feature extraction was already done. While techniques like this are no longer state-of-the-art, they paved a useful stepping stone for new approaches. Neural-based QE systems, such as POSTECH [Kim et al., 2017], which require no feature extraction, have become more popular in research in recent years. One downfall of these methods is that they require a lot more data and take far longer to train. This need for large amounts of parallel data can cause problems which is where systems like TransQuest [Ranasinghe et al., 2020] come in. This model bridges the data gap by leveraging pre-trained cross-lingual transformers that are then fine-tuned to the quality estimation task using the smaller amount of training data available. Early attempts at using round-trip translation (RTT) as a means of quality estimation proved unsuccessful, resulting in researchers labeling it as ‘good for nothing’ [Somers, 2005]. Its utility as a viable way to estimate the quality of a machine translation system has been pulled into question by problems such as those highlighted in O’Connell [2001] which assert that round-trip translation relies heavily on a model accurately translating the translated sentence back to its source language. Evaluation metrics cannot tell if translation errors occurred during the first translation or during the translation back to the original language. Moreover, if an error does occur during the first translation, it may cause even more problems when it is translated back. Some researchers have attempted to address this problem by using multiple machine translation models to translate the sentences back to their original language so that errors in one translation system are reduced by the collection of models used, however, in Zaanen and Zwarts [2006], they still concluded that round-trip translation was not a good way to measure machine translation quality. One common theme we saw throughout these research papers was that prior research mainly focused on the round-trip translation and did not focus too heavily on how they were evaluating the similarity between these sentences. Evaluation metrics like BLEU are limited as they only measure the lexical similarity and do not take the semantics of the sentences into consideration. Maintaining the semantic information of a sentence is something we should want to ensure that a machine translation system does when evaluating its translations. While Somers [2005] does conclude that the results given by his RTT experiment do not look promising, he also recognises in his conclusion that there is a big reliance on BLEU and F-Score throughout his paper and he says that he would like to replicate his experiments using human rating of intelligibility. During the course of our work, Moon et al. [2020] was published which very closely resembles our approach. They report a Pearson R correlation of 0.95 on the WMT19 metrics task evaluation set for English-German sentence pairs. We were unable to compare our approach on this data, as the ground truth direct assessment scores were not made publicly available. Conclusion In this paper, we have investigated the efficacy of round-trip translation as an estimate of translation quality and proposed a novel use of sentence embeddings to measure the similarity between source and round-trip translated sentences. The aim of our approach was to use the semantic information retained through round-trip translation as a proxy for translation quality. Experimentation shows increased performance of sentence embedding based similarity measures over traditional lexical metrics. We also highlight current pitfalls of the round-trip translation approach and suggest directions for future research on this topic.