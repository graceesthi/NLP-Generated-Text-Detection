Abstract Transformer-based language models are applied to a wide range of applications in natural language processing. However, they are inefficient and difficult to deploy. In recent years, many compression algorithms have been proposed to increase the implementation efficiency of large Transformer-based models on target hardware. In this work we present a new method for training sparse pre-trained Transformer language models by integrating weight pruning and model distillation. These sparse pre-trained models can be used to transfer learning for a wide range of tasks while maintaining their sparsity pattern. We demonstrate our method with three known architectures to create sparse pre-trained BERT-Base, BERT-Large and DistilBERT. We show how the compressed sparse pre-trained models we trained transfer their knowledge to five different downstream natural language tasks with minimal accuracy loss. Moreover, we show how to further compress the sparse models’ weights to 8bit precision using quantization-aware training. For example, with our sparse pre-trained BERT-Large fine-tuned on SQuADv1.1 and quantized to 8bit we achieve a compression ratio of 40X for the encoder with less than 1% accuracy loss. To the best of our knowledge, our results show the best compression-to-accuracy ratio for BERT-Base, BERT-Large, and DistilBERT. Introduction Transformer-based pre-trained language models (LM) such as BERT [Devlin et al., 2019] and RoBERTa [Liu et al., 2019] have become the standard approach for a wide range of natural language processing (NLP) tasks. Recently, we witness the emergence of models, larger by several orders of magnitude, such as GPT-2 [Radford et al., 2019], T-NLG [Rosset, 2020], GPT-3 [Brown et al., 2020], and Switch-C [Fedus et al., 2021]. These models advance the state-of-the-art results in several NLP tasks such as question answering and text classification. However, this trend toward bigger models raises several concerns. As the computational and memory resources required to run inference increase with the model’s size, it becomes very expensive and challenging to deploy these models in production environments and on edge devices. Moreover, these large amounts of computational resources incur a steep environmental cost [Strubell et al., 2019]. Model compression of large LM is a growing field of study as a result of these concerns. Weight pruning is a compression method that has been shown to be very effective at reducing the memory footprint of a model [Han et al., 2015, Zhu and Gupta, 2018]. However, weight pruning of large Transformer-based LMs to high sparsity ratios requires specialized pruning methods [Sanh et al., 2020, Chen et al., 2020, Gordon et al., 2020, Lagunas et al., 2021]. Moreover, most of the pruning methods require task specific modifications and tuning to produce quality results. Gordon et al. [2020] found that, in terms of accuracy, it does not matter whether BERT is pruned during the pre-training phase or during the transfer learning phase. This suggests that a LM can be pruned once during pre-training and then fine-tuned to any downstream task without task-specific tuning. In this paper, we present a new method, Prune Once for All (Prune OFA), that leverages weight pruning and model distillation to produce pre-trained Transformer-based language models with a high sparsity ratio. We apply our method to BERT-Base, BERT-Large and DistilBERT [Sanh et al., 2019] to produce sparse pre-trained models for these model architectures. We then show how these sparse models can be fine-tuned to produce task-specific sparse models with minimal accuracy loss for SQuADv1.1 [Rajpurkar et al., 2016] as well as for four tasks from the GLUE Benchmark [Wang et al., 2018]. We also show that it is possible to further compress the models using quantization-aware training to achieve state-of-the-art results in terms of compression-to-accuracy ratio. The main contributions of this work are threefold: 1) We introduce a new architecture-agnostic method of training sparse pre-trained language models. 2) We demonstrate how to fine-tune these sparse models on downstream tasks to create sparse and quantized models, removing the burden of pruning and tuning for a specific language task. 3) We publish our compression research library with example scripts to reproduce our work for other architectures, along with our sparse pre-trained models presented in this paper. Related work Large language models are over-parameterized and difficult to deploy. Therefore, the problem of compressing these models with minimum accuracy loss for downstream tasks is widely explored. Sanh et al. [2020] suggests the Movement Pruning method designed especially for transfer learning. Neural Magic implements Gradual Magnitude Pruning.1 Both methods suggest pruning BERT-Base while fine-tuning to downstream tasks paired with model distillation, and present results showing 90% sparsity for several tasks. However, both methods require a long fine-tuning time as well as tuning pruning related hyper-parameters for every task. Our method, on the other hand, requires no tuning of special pruning hyper-parameters per task because we prune the model once for all tasks. Furthermore, we present better or comparable results at a much lower computation budget at the transfer learning phase. Gordon et al. [2020] explored the effect of weight pruning during transfer learning and concluded that pruning BERT-Base at the pre-training phase does not degrade the performance of the model compared to pruning at fine-tuning. We improve upon the suggested method and present better results at a much higher sparsity ratio. Chen et al. [2020] explored the Lottery Ticket Hypothesis [Frankle and Carbin, 2018] for BERT pre-trained models. More specifically, they analyzed the possibility of finding winning tickets in a BERT-Base pre-trained model that transfer to other downstream tasks. The authors concluded that winning tickets found while pre-training on a Masked-LM task, transfer well to other downstream tasks. Lagunas et al. [2021] presented a structured pruning method, removing rows, columns and attention heads, while achieving less than 1% loss in F1 for a BERT architecture on SQuADv1.1. Mishra et al. [2021] performed structured 2:4 pruning on BERT while further pre-training BERT; The method produced a 50% sparse model which can be fine-tuned without accuracy loss. Michel et al. [2019] explored the significance of each head in the multi-head attention mechanism of BERT and presented a method for pruning attention heads with their associated weights. Other works propose knowledge distillation to compress Transformer models to a smaller dense counter part that can be tuned to downstream tasks [Sanh et al., 2019, Jiao et al., 2020, Sun et al., 2020]. Quantization of Transformer-based language models is also a well known method for compression. Shen et al. [2020] proposes a method to quantize BERT at a different bit-width per layer. Other works implement quantization-aware training to quantize BERT to 8bits [Kim et al., 2021, Zafrir et al., 2019]. Zhang et al. [2020] created a method of producing a ternary weight BERT. Kim and Hassan [2020] presented a compression pipeline for Transformer models that includes model distillation, quantization and head pruning. Conclusion and future work We introduced Prune OFA, an architecture-agnostic method for producing sparse pre-trained language models. We also showed how these sparse models can be used to obtain fine-tuned sparse models without the burden of task-specific pruning. Our results suggest that using these sparse pre-trained models for transfer learning produces results with minimal performance degradation loss w.r.t their dense counterpart for a variety of NLP tasks. We further demonstrated that integrating quantization can lead to more efficient sparse and quantized models at a small cost to the model’s accuracy. A possible direction for future research is to explore whether a large and sparse pre-trained model is better at capturing and transferring natural language knowledge than a smaller dense model of the same architecture with similar non-zero parameters count. We hope that the release of our code and sparse pre-trained models to the community will help develop more efficient models.