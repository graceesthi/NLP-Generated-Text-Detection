Abstract While sentence anomalies have been applied periodically for testing in NLP, we have yet to establish a picture of the precise status of anomaly information in representations from NLP models. In this paper we aim to fill two primary gaps, focusing on the domain of syntactic anomalies. First, we explore fine- grained differences in anomaly encoding by designing probing tasks that vary the hierar- chical level at which anomalies occur in a sen- tence. Second, we test not only modelsÃ¢ÂÂ ability to detect a given anomaly, but also the gener- ality of the detected anomaly signal, by exam- ining transfer between distinct anomaly types. Results suggest that all models encode some information supporting anomaly detection, but detection performance varies between anoma- lies, and only representations from more re- cent transformer models show signs of general- ized knowledge of anomalies. Follow-up anal- yses support the notion that these models pick up on a legitimate, general notion of sentence oddity, while coarser-grained word position in- formation is likely also a contributor to the ob- served anomaly detection. Introduction As the NLP community works to understand what is being learned and represented by current mod- els, a notion that has made sporadic appearances is that of linguistic anomaly. Analyses of language models have often tested whether models prefer grammatical over ungrammatical completions (e.g. Linzen et al., 2016), while analyses of sentence embeddings have probed for syntax and semantics by testing detection of sentence perturbations (Con- neau et al., 2018). Such work tends to exploit anomaly detection as a means of studying linguis- tic phenomena, setting aside any direct questions about encoding of anomaly per se. However, mod- elsÃ¢ÂÂ treatment of anomaly is itself a topic that raises important questions. After all, it is not obvious that we should expect models to encode information like Ã¢ÂÂthis sentence contains an anomalyÃ¢ÂÂ, nor is it obvious which types of anomalies we might expect models to pick up on more or less easily. Nonethe- less, anomalies are easy to detect for humans, and their detection is relevant for applications such as automatic error correction (Ge et al., 2018), so it is of value to understand how anomalies operate in our models, and what impacts anomaly encoding. In the present work we seek to fill this gap with a direct examination of anomaly encoding in sen- tence embeddings. We begin with fine-grained testing of the impact of anomaly type, designing probing tasks with anomalies at different levels of syntactic hierarchy to examine whether model representations better support detection of certain types of anomaly. Then we examine the general- ity of anomaly encoding by testing transfer perfor- mance between distinct anomaliesÃ¢ÂÂhere our ques- tion is, to the extent that we see successful anomaly detection, does this reflect encoding of a more gen- eral signal indicating Ã¢ÂÂthis sentence contains an anomalyÃ¢ÂÂ, or does it reflect encoding of simpler cues specific to a given anomaly? We focus on syn- tactic anomalies because the hierarchy of sentence structure is conducive to our fine-grained anomaly variation. (Sensitivity to syntactic anomalies has also been studied extensively as part of the human language capacity (Chomsky, 1957; Fodor et al., 1996), strengthening precedent for prioritizing it.) We apply these tests to six prominent sentence encoders. We find that most models support non- trivial anomaly detection, though there is sub- stantial variation between encoders. We also ob- serve differences between hierarchical classes of anomaly for some encoders. When we test for transferability of the anomaly signal, we find that for most encoders the observed anomaly detection shows little sign of generalityÃ¢ÂÂhowever, trans- fer performance in BERT and RoBERTa suggests that these more recent models may in fact pick up on a generalized awareness of syntactic anoma- lies. Follow-up analyses support the possibility that these transformer-based models pick up on a legit- imate, general notion of syntactic oddityÃ¢ÂÂwhich appears to coexist with coarser-grained, anomaly- specific word order cues that also contribute to detection performance. We make all data and code available for further testing. Related Work This paper builds on work analyzing linguistic knowledge reflected in representations and out- puts of NLP models (Tenney et al., 2019; Rogers et al., 2020; Jawahar et al., 2019). Some work uses tailored challenge sets associated with down- stream tasks to test linguistic knowledge and robust- nesset al., 2018; Poliak et al., 2018a,b; White et al., 2017; Belinkov et al., 2017b; Yang et al., 2015; Rajpurkar et al., 2016; Jia and Liang, 2017; Rajpurkar et al., 2018). Other work has used targeted classification-based probing to examine encoding of specific types of linguistic information in sentence embeddings more directly (Adi et al.,2016; Conneau et al., 2018; Belinkov et al., 2017a; Ettinger et al., 2016, 2018; Tenney et al., 2019; Klafka and Ettinger, 2020). We also ob- serve differences between hierarchical classes of anomaly for some encoders. When we test for transferability of the anomaly signal, we find that for most encoders the observed anomaly detection shows little sign of generalityÃ¢ÂÂhowever, trans- fer performance in BERT and RoBERTa suggests that these more recent models may in fact pick up on a generalized awareness of syntactic anoma- lies. Follow-up analyses support the possibility that these transformer-based models pick up on a legit- imate, general notion of syntactic oddityÃ¢ÂÂwhich appears to coexist with coarser-grained, anomaly- specific word order cues that also contribute to detection performance. We make all data and code available for further testing. Related Work This paper builds on work analyzing linguistic knowledge reflected in representations and out- puts of NLP models (Tenney et al., 2019; Klafka et al., 2020; Tenney et al., 2019; Ettinger et al., 2020), focusing on the domain of linguistic knowledge and detection performance. Some work uses tailored challenge sets associated with down- stream tasks to test linguistic knowledge and robust- ness (Dasgupta et al., 2018; Poliak et al., 2018a; Ettinger, 2019). Other work has used targeted classification-based probing to examine encoding of specific types of linguistic information in sentence embeddings more directly (Adi et al.,2016; Ettinger, 2020; Klafka et al., 2020; Yang et al., 2015; Yang et al., 2016; White et al., 2017; Ettinger, 2018). We expand on this work by designing analyses to shed light on encod- ing of syntactic anomaly information in sentence embeddings. A growing body of work has examined syntactic sensitivity in language model outputs (Chowdhury and Zamparelli, 2018; Futrell et al., 2019; Lakretz et al., 2019; Marvin and Linzen, 2018; Ettinger, 2020), and our Agree-Shift task takes inspiration from the popular number agreement task for lan- guage models (Linzen et al., 2016; Gulordava et al., 2018; Goldberg, 2019). Like this work, we fo- cus on syntax in designing our tests, but we differ from this work in focusing on model representa- tions rather than outputs, and in our specific focus on understanding how models encode information about anomalies. Furthermore, as we detail below, our Agree-Shift task differs importantly from the LM number agreement tests, and should not be compared directly to results from those tests. Our work relates most closely to studies involv- ing anomalous or erroneous sentence information (Warstadt et al., 2019; Yin et al., 2020; Hashemi and Hwa, 2016). Some work investigates impacts from random shuffling or other types of distor- tion of input text (Pham et al., 2020; Gupta et al., 2021) or of model pre-training text (Sinha et al., 2021) on downstream tasksÃ¢ÂÂbut this work does not investigate modelsÃ¢ÂÂ encoding of these anoma- lies. Warstadt et al. (2019) present and test with the CoLA dataset for general acceptability detec- tion, and among the probing tasks of Conneau et al. (2018) there are three that involve analyz- ing whether sentence embeddings can distinguish erroneous modification to sentence inputs: SOMO, BShift, and CoordInv.