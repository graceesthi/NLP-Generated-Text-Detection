ABSTRACT We propose BERMo, an architectural modification to BERT, which makes predic- tions based on a hierarchy of surface, syntactic and semantic language features. We use linear combination scheme proposed in Embeddings from Language Mod- els (ELMo) to combine the scaled internal representations from different network depths. Our approach has two-fold benefits: (1) improved gradient flow for the downstream task as every layer has a direct connection to the gradients of the loss function and (2) increased representative power as the model no longer needs to copy the features learned in the shallower layer which are necessary for the down- stream task. Further, our model has a negligible parameter overhead as there is a single scalar parameter associated with each layer in the network. Experiments on the probing task from SentEval dataset show that our model performs up to 4.65% better in accuracy than the baseline with an average improvement of 2.67% on the semantic tasks. When subject to compression techniques, we find that our model enables stable pruning for compressing small datasets like SST-2, where the BERT model commonly diverges. We observe that our approach converges 1.67× and 1.15× faster than the baseline on MNLI and QQP tasks from GLUE dataset. Moreover, our results show that our approach can obtain better parameter efficiency for penalty based pruning approaches on QQP task. INTRODUCTION The invention of Transformer (Vaswani et al. (2017)) architecture has paved new research direc- tions in the deep learning community. Descendants of this architecture, namely BERT (Devlin et al. (2019)) and GPT (Brown et al. (2020)), attain State-of-The-Art (SoTA) performance for a broad range of NLP applications. The success of these networks is primarily attributed to the two-stage training process (self-supervised pre-training and task-based fine-tuning), and the attention mech- anism introduced in Transformers. Many of the top models on various leader boards use models from the BERT family. All of the fifteen systems that surpass the human baseline on the Gen- eral Language Understanding Evaluation (GLUE) (Wang et al. (2018)) benchmark use variants of BERT or have it as one of the constituents in an ensemble, except for T5 (Raffel et al. (2019)), which uses the Transformer architecture. Further, the best-performing systems for each task in the Ontonotes (Weischedel et al. (2011)) benchmark belong to the BERT family, with the exception of Entity Typing task where Embeddings from Language Models (ELMo) (Peters et al. (2018)) tops the leaderboard. These promising results make the BERT family of models increasingly ubiquitous in solving several tasks in the various domains of machine learning like NLP, Image Recognition (Dosovitskiy et al. (2021); Jaegle et al. (2021)) and Object detection (Carion et al. (2020)). Motivation: In (Jawahar et al. (2019)), the authors manifest the lower layers of BERT to capture phrase-level information, which gets diluted with the depth. Moreover, they also demonstrate that the initial layers capture surface-level features, the middle layers deal with syntactic features, and the last few layers are responsible for semantic features. These findings indicate that BERT captures a rich hierarchy of linguistic features at different depths of the network. Intrigued by this discovery, we aim at combining the activations from different depths to obtain a richer feature representation. We find our problem formulation similar to the one presented in ELMo, where the authors illus- trate higher-level Long Short Term Memory (LSTM) states capture context-dependent aspects of word meaning or semantic features, and the lower-level LSTM states model the syntax. Inspired by ELMo, we propose BERMo by modifying the BERT architecture to increase the dependence of features from different depths to generate a rich context-dependent embedding. This approach improves the gradient flow during the backward pass and increases the representative power of the network (He et al. (2016); Huang et al. (2016)). Further, the linear combination of features from intermediate layers, proposed in ELMo, is simpler form of skip connection introduced in ResNets (He et al. (2016)). The skip connections in ResNets enable aggressive pooling in initial layers with- out affecting the gradient flow. This in turn leads to low parameters allowing these networks to have orders of magnitude lower parameters compared to the architectures without skip connections such as VGG (Simonyan & Zisserman (2014)) while achieving competitive accuracy. Since the perfor- mance improvements associated with the BERT architecture come at the cost of a large memory footprint and enormous computational resources, compression becomes necessary to deploy these networks in resource-constrained environments like drones, mobile computers and IoT devices. As we introduce skip connections in the architecture to obtain a complex feature representation, the gradient flow during the backward pass improves. Further, our model has a negligible parameter overhead as there is a single scalar parameter associated with each layer in the network.Therefore, we expect BERMo architecture to be better candidate for compression and hence, we believe the proposed model could be ideal for resource-constrained settings. Contributions: The Contributions Of This Work Can Be Summarized As Follows: i  We propose BERMo, which generates complex feature maps using linear combination of features from different depths. We evaluate the proposed model on the probing task from SentEval dataset (Conneau et al. (2018)) and find our model performs 2.67% better than the baseline on the semantic tasks (Tense, Subjnum, Objnum, Somo, Coordinv) on average. ii  We observe our approach is stable when pruning with smaller datasets like SST-2 (Wang et al. (2018)), where BERT commonly diverges. iii  We show our model supports higher pruning rates when compressing and converges 1.67× and 1.15× faster than BERT on MNLI and QQP (Wang et al. (2018)), respectively. iv  Forlosspenaltybasedpruningmethodourapproachcanobtainbetterparameterefficiency, 1.35× for QQP, than BERT model for comparable performance. v  Our approach produces comparable results to the baseline for Knowledge Distillation with marginal improvements on SQuAD dataset. Outline: The rest of the paper is organised as follows: Section 2 de- scribes ELMo (Peters et al. (2018)), BERT (Devlin et al. (2019)) and Pruning methods. Section 3 elab- orates the proposed model. The experimental setup and the results are presented in Section 4. In Section 5 we summarize our work and discuss the future possibilities. Section 6 reports the related work and we conclude our paper with Broader Impact in Section 7. BROADER IMPACT Research presented in this work improves the baseline performance on semantic tasks. This work highlights the importance of adding skip connections to the network in improving the training con- vergence and stability. We believe this work would act as a stepping stone and motivate further research in this direction, reducing the training time for these models. These improved training speeds also make room for enlarging the dataset size generally correlated with improvements in generalization performance. Further, as this work deals with reducing the training time for prun- ing, a possible application would be online pruning on resource constrained setup. Moreover, from an environmental perspective reducing training time will reduce the carbon footprint of these large language models.