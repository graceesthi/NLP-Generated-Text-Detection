Abstract We present Harmonic Memory Networks (HMem), a neural architecture for knowl- edge base completion that models entities as weighted sums of pairwise bindings between an entityÃÂ¢ÃÂÃÂs neighbors and corresponding rela- tions. Since entities are modeled as aggregated neighborhoods, representations of unseen enti- ties can be generated on the fly. We demon- strate this with two new datasets: WNGen and FBGen. Experiments show that the model is SOTA on benchmarks, and flexible enough to evolve without retraining as the knowledge graph grows. Introduction The existence of large but inexhaustive databases of specialized (e.g. WordNet) and general world (e.g. Freebase) knowledge has motivated the de- velopment of methods that allow such databases to be automatically extended using computational methodsÃ¢ÂÂknowledge base completion (KBC). Typical approaches employ an embedding-based strategy: elements of a factÃ¢ÂÂwhich in this task setting come in the form of triplets con- sist of a pair of entities and a relation, e.g. (spy kids, has actor, mike judge)Ã¢ÂÂare com- bined into a representation of the fact via some sys- tematic function, and then scored. Methods of this sort generally rely on learned embeddings of fact el- ements into low-dimensional vector spaces. Since representations must be learned from training-set instances of each component, this creates problems when such databases are to be scaled, and there- fore these methods have difficulty accommodating an open-world setting in which knowledge graphs evolve in time, since new facts inserted into the database after model training cannot be used for inference without model retraining. Furthermore, databases may be augmented in time not only with new facts about known entities, but also with new entities. In embedding-based models, new repre- sentation for such entities must be trained. We present Harmonic Memories (HMem), a neu- ral network which models entities by aggregating information about their neighborhoods using a su- perposition memory architecture, achieving gener- alization to new entities without retraining.1 The network combines two ideas. First, a represen- tation of entities as memory states consisting of superposed vector associations between learned en- tity and relation embeddings. Second, completion of memory states using a learned transformation based on Harmony-optimization methods (Smolen- sky and Legendre, 2006) (see ÃÂ§4). We refer to vector associations as bindings in the sense of the Ã¢ÂÂvariable-binding problemÃ¢ÂÂ in the philosophy of cognitive science: in neural net models of cogni- tion, how are representations of the elements of a structure bound together into structures? In this work, we investigate two solutions prominent in the cognitive science literatureÃ¢ÂÂtensor product bind- ing (Smolensky, 1990) and circular convolution (Plate, 1994)Ã¢ÂÂwhich have also both been effec- tively applied in KBC (Nickel et al., 2011, 2016). The approach is inspired by computational mod- eling of biological neural architectures for knowl- edge representation (Crawford et al., 2015), and is related to KBC methods based on convolution of graph neighborhoods (Schlichtkrull et al., 2017; Dettmers et al., 2018; Nguyen et al., 2018), in which inference is performed over representations of aggregated entity neighborhoods. Recent work has extended this idea using Graph Attention Net- works (GANs) (Nathani et al., 2018), which assign attention weights to entries in a graph neighbor- hood, these being later combined. For instance, VelicÃÂkovic ÃÂ et al. (2018) use Graph Attention to gen- erate weights for triplet representations obtained by transforming concatenated entity and relation vectors, combining the results by averaging. This is similar to our approach, with the key difference that formulating the modelÃ¢ÂÂas we doÃ¢ÂÂin terms of binding allows for clear formal analysis of certain scaling results (ÃÂ§7). We therefore gain in inter- pretability. HMem scales well in three respects. First, it allows a database with a fixed set of entities and relations to incorporate new facts into the model without parameter re-estimation. Empirically, per- formance improves in nearly every case when the neighborhoods are thus expanded. Second, it permits the addition of entities unseen in train- ing, whose representations are useless in a vec- tor embedding framework. For our model, infer- ences about these entities are possible when a sub- graph including them becomesThird, our model effectively handles nodes with high in-degree. We show that, whereas embedding- based approaches show decreased performance with highly connected nodes, our model exhibits improved performance on nodes with many neigh- bors. ÃÂÃÂ§2, ÃÂÃÂ§3 and ÃÂÃÂ§4 introduce the Harmonic Memory architecture, and ÃÂÃÂ§5 shows that our model achieves state-of-the-art results on benchmark KBCs. Introduction The existence of large but inexhaustive databases of specialized (e.g. WordNet) and general world (e.g. Freebase) knowledge has motivated the de- velopment of methods that allow such databases to be automatically extended using computational methodsÃ¢ÂÂknowledge base completion (KBC). Typical approaches employ an embedding-based strategy: elements of a factÃ¢ÂÂwhich in this task setting come in the form of triplets con- sist of a pair of entities and a relation, e.g. (spy kids, has actor, mike judge)Ã¢ÂÂare com- bined into a representation of the fact via some sys- tematic function, and then scored. Methods of this sort generally rely on learned embeddings of fact el- ements into low-dimensional vector spaces.