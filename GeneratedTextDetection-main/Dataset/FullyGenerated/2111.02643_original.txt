Abstract Pre-trained language models (PLM) have marked a huge leap in neural dialogue modeling. While PLMs are pre-trained on largescale text corpora, they are usually fine-tuned on scarce dialogue data with specific domain knowledge and dialogue styles. However, tailoring the language models while fully utilizing prior knowledge in large pre-trained models remains a challenge. In this paper, we present a novel approach for pre-trained dialogue modeling that casts the dialogue generation problem as a prompt-learning task. Instead of fine-tuning on limited dialogue data, our approach, DialogPrompt, learns continuous prompt embeddings optimized for dialogue contexts, which appropriately elicit knowledge from the large pre-trained model. To encourage the model to better utilize the prompt embeddings, the prompt encoders are designed to be conditioned on the input dialogue context. Experiments on popular conversation datasets show that our approach significantly outperforms the fine-tuning baseline and the generic prompt-learning methods. Furthermore, human evaluations strongly support the superiority of DialogPrompt in regard to response generation quality. Introduction Pre-trained language models (PLMs) such as BERT (Devlin et al., 2018) and GPT-2 (Radford et al., 2019) have achieved remarkable success in various natural language processing tasks (Sun et al., 2019). As such, there is a growing trend of using pre-trained language models for conversation modeling (Budzianowski and Vulic´, 2019; Zhang et al., 2019; Feng et al., 2021). For example, Zhang et al. (2019) proposed DialoGPT, a dialogue generation model that trains an extended GPT-2 (Radford et al., 2019) on large dialogue corpus. Feng et al. (2021) further explore the usage of DialoGPT for dialogue summarization. These pre-trained dialogue models are often pre-trained on large text corpora and fine-tuned on smaller dialogue datasets (Zhang et al., 2019). One limitation of PLM-based dialogue modeling, and even for other PLM tasks, is the trade-off between pre-training and fine-tuning (Ben-David et al., 2021). That is, the task-specific data used for fine-tuning is usually scarce and costly. As such, the reusability of prior knowledge learned in the pre-training phase can be limited during finetuning, hence some dialogue models are simply trained from scratch on the limited task-specific data. Consequently, recent works have resorted to prompt learning, a lightweight alternative to finetuning. Prompt learning keeps the PLM parameters frozen but optimizes only a small portion of task-specific prompts or related modules (Liu et al., 2021a; Shin et al., 2020; Liu et al., 2021b; Li and Liang, 2021). For example, Liu et al. (2021b) propose p-tuning, which preprends trainable prompt tokens to the input of a PLM. The trainable prompt embeddings are optimized while the PLM parameters are kept frozen. Prompt learning allows fewshot or nearly zero-shot learning for pre-trained models in new tasks with little or unlabeled data and it has been demonstrated to be substantially effective over fine-tuning in many tasks (Liu et al., 2021a; Qin and Eisner, 2021). However, applying prompt learning directly to conversation modeling is challenging. The general prompt-learning models assign universal prompt tokens to all inputs in the same task (Liu et al., 2021b). For example, prompts used for sentiment analysis share the same embeddings that are inferred from the training data (Liu et al., 2021b). In contrast, conversations are context-sensitive. Dialogue responses are affected by contextual information, such as the topic of discussion, pre-dialogue context, and participant personalities. “Blanket” prompts can restrict the expressiveness of prompt learning due to the lack of context-awareness, leading to sub-optimal performance in response generation. In this work, we present DialogPrompt, a novel prompt-based paradigm for response generation on top of large pre-trained language models. DialogPrompt prepends a sequence of prompt tokens to each dialogue context for eliciting response from large pre-trained language models. In order to construct context-aware prompts, we propose a conditional prompt encoder on top of the Transformer (Vaswani et al., 2017). The prompt tokens are initially encoded conditionally on the dialogue context. The resulting prompt encoding is then taken as the initial hidden state of the large PLM to generate responses. Compared to fine-tuning, DialogPrompt is encouraged to search proper prompts which controls the large PLMs into producing higher-quality responses directly. We evaluate DialogPrompt on popular multi-turn conversation datasets such as DailyDialog and MultiWOZ. Results show that DialogPrompt outperforms fine-tuning counterparts and other prompt tuning methods in terms of automated evaluation measures and the average length of generated responses. Human evaluation supports the superiority of our approach in generating informative and knowledgeable responses. Our contributions are summarized as follows: • To our knowledge, we are the first to propose prompt-based dialogue response generation. Our approach can better reuse knowledge from existing large-scale PLMs and produce more knowledgeable responses. • We design a novel conditional prompt encoder for encouraging context-aware prompt learning. • We extensively evaluated our approach on popular multi-turn conversation datasets and demonstrated the superiority of our approach in terms of quantitative automatic evaluations and qualitative human evaluations. Related Work This work is closely related to (1) pre-trained models for conversations, and (2) prompt learning for pre-trained language models. Pre-trained Models for Dialogue Generation. Recently, an emerging trend in dialogue generation explores the adaptation of large pre-trained language models on dialogue corpora (Golovanov et al., 2019; Zhang et al., 2019). For example, Golovanov et al. (2019) studied how pre-trained architectures can be adapted for natural language generation, comparing a number of architectural and training schemes. The state-of-the-art DialoGPT (Zhang et al., 2019) pre-trains a GPT-2 model on largescale conversation datasets and achieves a giant leap in performance against traditional conversation models. Another line of work related to exploiting the use of pre-trained models for task-oriented dialogues. For example, Budzianowski and Vulic´ (2019) proposed a task-oriented dialogue model that operates solely on text input. Their model is built on top of the TransferTransfo framework (Golovanov et al., 2019) that effectively bypasses explicit policy and language generation modules. TOD-BERT proposed by Wu et al. (2020) bridges the difference of general text and task-oriented dialogue by unifying nine human-human and multi-turn task-oriented dialogue datasets for language modeling. The model also incorporates user and system tokens into the masked language modeling and proposes a contrastive objective function to simulate the response selection task. Compared to these related works which directly fine-tune the dialogue model based on a pre-trained model, DialogPrompt is a novel paradigm for pretrained dialogue models which elicits knowledge from PLMs directly through minimal optimizing of prompt tokens. Prompt Learning for Pre-trained Language Models. There is a growing trend of automatically finding prompts to adapt pre-trained language models to downstream tasks (Shin et al., 2020; Li and Liang, 2021; Liu et al., 2021b). For example, Shin et al. (2020) proposed AutoPrompt which automatically optimizes prompts using a gradient signal. Unlike our method, AutoPrompt searches for hard prompts, thus it may be less versatile than the continuous methods. Instead, Liu et al. (2021b) proposed a continuous prompt tuning model named p-tuning. p-tuning optimizes fill-inthe-blank prompts in a continuous space, tested on GPT-2 and BERT models. A similar idea was proposed by Li and Liang (Li and Liang, 2021) who considered the tuning of prompts using a textual prefix. Specifically, they prepended a few taskspecific “soft tokens” (prefix) to the source text and tuned the hidden states of only these tokens (at all Transformer layers). Similarly, Lester et al. (2021) prepended a sequence of prompt tokens to the source text, but only the word embeddings of these tokens are optimized. Qin and Eisner (2021) proposed prompt-based learning on relation extraction tasks using data-dependent mixtures of prompt templates and parameters. Our method differs from existing prompt-based tuning methods in that we propose a novel contextaware prompt tuning mechanism that can optimize prompt encodings conditioned on dialogue contexts. Conclusion In this paper, we propose DialogPrompt, a novel prompt based response generation model. DialogPrompt prepends a prompt utterance to the dialogue context and only optimizes the prompt encoder. In order to adapt to different contexts, we propose a conditional prompt encoder that updates prompt activation based on the hidden states of context before response generation. Results on two popular conversation datasets, namely, DailyDialog and MultiWOZ show that DialogPrompt signifi- cantly outperforms fine-tuning counterparts and other prompt based models on both automatic and human evaluations. In the future, we will investigate prompt-based dialogue modeling based on more pre-trained language models.