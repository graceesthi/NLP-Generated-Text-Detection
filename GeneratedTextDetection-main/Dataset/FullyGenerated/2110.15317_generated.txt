Abstract Despite great success on many machine learn- ing tasks, deep neural networks are still vulner- able to adversarial samples. While gradient- based adversarial attack methods are well- explored in the field of computer vision, it is impractical to directly apply them in natu- ral language processing due to the discrete na- ture of text. To bridge this gap, we propose a general framework to adapt existing gradient- based method to craft textual adversarial sam- ples. In this framework, gradient-based contin- uous perturbations are added to the embedding layer and are amplified in the forward propa- gation process. Then the final perturbed latent representations are decoded with a mask lan- guage model head to obtain potential adversar- ial samples. In this paper, we instantiate our framework with Textual Projected Gradient Descent (TPGD). We conduct comprehensive experiments to evaluate our framework by per- forming transfer black-box attacks on BERT, RoBERTa and ALBERT on three benchmark datasets. Experimental results demonstrate our method achieves an overall better performance and produces more fluent and grammatical ad- versarial samples compared to strong baseline methods. All the code and data will be made public. Introduction Deep learning has achieved great success in various domains, such as computer vision (CV) (He et al., 2016;Chi et al., 2019), natural language processing (NLP) (Vaswani et al., 2017; Devlin et al., 2019) and speech recognition ( Chiu et al., 2018; Park et al., 2019). However, the powerful neural net- works are still vulnerable to adversarial samples, crafted by adding small and human-imperceptible perturbations to the inputs. (Szegedy et al., 2014; Goodfellow et al., 2015). In the field of CV, numerous adversarial attack methods have been proposed to evaluate the robust- ness of DNNs (Papernot et al., 2016a, Madry et al., 2019), and corresponding defence methods are also well-explored (Papernot et al., 2016c; Ross and Doshi-Velez, 2018). Adversarial attacks on images are defined as an optimization problem of maximiz- ing the loss function of model on specific samples, which can be approximated by the gradient ascent algorithms. However, textual adversarial attack is more chal- lenging due to the discrete and non-differentiable nature of the text space. And the methods that directly employ the gradients to craft adversarial samples is not applicable in NLP. Current practices of textual adversarial attacks that employ first-order approximation to find substitute words are less ef- fective for one-off searching and can violate the local linearization assumption (Cheng et al., 2019; Behjati et al., 2019; Xu and Du, 2020). To bridge this gap, we propose a general frame- work to adapt existing gradient-based method to NLP. We successfully obtain high-quality adversar- ial samples by conducting gradient-based search. Specifically, we employ the gradient of loss function with respect to the embeddings of input tokens to make perturbations on token embeddings rather than on the original text, thus transforming the prob- lem of searching for adversarial samples from the discrete text space to the continuous differentiable embedding space. This provides the basis for apply- ing gradient-based methods investigated in CV to craft textual adversarial samples. In this paper, we adapt PGD (Madry et al., 2019) algorithm within our framework to perform textual adversarial at- tacks, denoted as TPGD. We iteratively generate small perturbations following the gradient informa- tion and add them to the embedding layer. Then the forward propagation process will amplify the perturbations. The natural question is how can we transform the perturbed token embeddings back to the dis- crete text? Although there exist some works explor- ing the feasibility of directly perturbing token em- beddings (Miyato et al. (2017); Sato et al. (2018); Cheng et al. (2019); Behjati et al. (2019)), None of them succeed to map all points in the embed- ding space back to words, thus failing to generate human-imperceptible adversarial samples. How- ever, recent work observes that the mask language model (MLM) head can reconstruct input sentences from their hidden states with high accuracy, even after models havefine-tuned on specific tasks (Kao et al., 2021). Inspired by this, we employ a MLM head to decode the perturbed latent represen- tations. With the extensive linguistic knowledge of MLM head, the coherence and grammaticality of adversarial samples can be guaranteed. We conduct comprehensive experiments to eval- uate the effectiveness of our method by per- forming transfer black-box attacks on BERT, RoBERTa and ALBERT on three benchmark datasets. Experimental results demonstrate our method achieves an overall better performance and produces more fluent and grammatical ad- versarial samples compared to strong baseline methods. All the code and data will be made public. Introduction Deep learning has achieved great success in various domains, such as computer vision (CV) (He et al., 2016;Chi et al., 2019), natural language processing (NLP) (Vaswani et al., 2017; Devlin et al., 2019) and speech recognition ( Chiu et al., 2018; Park et al., 2019). However, the powerful neural net- works are still vulnerable to adversarial samples, crafted by adding small and human-imperceptible perturbations to the inputs. (Szegedy et al., 2014; Goodfellow et al., 2015). In the field of CV, numerous adversarial attack methods have been proposed to evaluate the robust- ness of DNNs (Papernot et al., 2016a, Madry et al., 2019), and corresponding defence methods are also well-explored (Papernot et al., 2016c; Ross and Doshi-Velez, 2018). Adversarial attacks on images are defined as an optimization problem of maximiz- ing the loss function of model on specific samples, which can be approximated by the gradient ascent algorithms. However, textual adversarial attack is more chal- lenging due to the discrete and non-differentiable nature of the text space. And the methods that directly employ the gradients to craft adversarial samples is not applicable in NLP. Current practices of textual adversarial attacks that employ first-order approximation to find substitute words are less ef- fective for one-off searching and can violate the local linearization assumption (Cheng et al., 2019; Behjati et al., 2019; Xu and Du, 2020). To bridge this gap, we propose a general frame- work to adapt existing gradient-based method to NLP. We successfully obtain high-quality adversar- ial samples by conducting gradient-based search. Specifically, we employ the gradient of loss function with respect to the embeddings of input tokens to make perturbations on token embeddings rather than on the original text, thus transforming the prob- lem of searching for adversarial samples from the discrete text space to the continuous differentiable embedding space. This provides the basis for apply- ing gradient-based methods investigated in CV to craft textual adversarial samples.