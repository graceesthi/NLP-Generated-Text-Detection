Abstract Improving user experience of a dialogue system often requires intensive developer effort to read conversation logs, run statistical analyses, and intuit the relative importance of system shortcomings. This paper presents a novel approach to automated analysis of conversation logs that learns the relationship between user-system interactions and overall dialogue quality. Unlike prior work on utterance-level quality prediction, our approach learns the impact of each interaction from the overall user rating without utterance-level annotation, allowing resultant model conclusions to be derived on the basis of empirical evidence and at low cost. Our model identifies interactions that have a strong correlation with the overall dialogue quality in a chatbot setting. Experiments show that the automated analysis from our model agrees with expert judgments, making this work the first to show that such weakly-supervised learning of utterance-level quality prediction is highly achievable. Introduction A typical life cycle of a dialogue system involves many iterative updates where developers improve the systemÃ¢ÂÂs language understanding capabilities and attempt to increase the overall user engagement. One of the most challenging aspects of executing these updates is to identify characteristics of the dialogue system that are impacting user experience the most. Doing so often involves manually crawling potentially thousands of system logs and designing statistical analyses, both of which are time consuming and unlikely to provide a holistic view of a systemÃ¢ÂÂs shortcomings. Inspired by this problem, the presented work investigates the extent to which it is possible to automatically distinguish turns within chat-oriented dialogues that have a negative effect on overall dialogue quality. The interpretation of dialogue quality is especially difficult in the chat-oriented dialogue setting due to its subjective and multi-faceted objectives. System misunderstandings and low user engagement are factors of low quality that are relatively easy to identify, but more subtle factors such as boring responses, awkward topic switches, and individual preferences can also have a substantial effect. Furthermore, the practical value of any approach to estimate the quality of individual dialogue turns is highly sensitive to the cost of collecting relevant data. Chatbots, and the settings they are placed in, can differ drastically in both their topics of conversation and interaction styles. And while conversation-level quality labels can be obtained relatively quickly by asking users to provide a rating at the end of a conversation, collecting data with turn-level labels that adequately characterizes a new chatbot or chat setting is an expensive process. In this paper we present our dialogue analysis approach, which addresses these challenges by producing quality scores for each utterance in a given conversation dataset using only conversation-level quality ratings. Unlike other work that focuses on utterance-level quality prediction using labeled data, our approach involves training a neural model to learn explicit relationships between utterancelevel features and conversation quality without the need for costly utterance-level annotations. We evaluate this approach on two conversation datasets and show high agreement between our model and experts for identifying problematic interactions. By developing an empirical technique that models the relationship between specific interactions and overall conversation quality, our work has the potential to remove much of the human effort and guesswork involved in dialogue system development. Related Work Related work has explored techniques for modelling dialogue quality on both the conversation and utterance level. Sandbank et al. (2018) present an approach for classifying low-quality conversations in commercial conversational assistants. Liang et al. (2020) argued against the feasability of conversation-level quality prediction on a Likertscale and present a pairwise comparison model instead using methods that compensated for the high noise in user scores. Choi et al. (2019) presents methods for both predicting user satisfaction and detecting conversation breakdowns at the turn level. Ghazarian et al. (2020)Ã¢ÂÂs work is similar, predicting utterance-level user engagement. Ghazarian et al. (2020) and Choi et al. (2019)Ã¢ÂÂs work is similar to ours, as they build models targeted towards utterance-level quality outcomes. However, unlike our approach, these works are reliant on costly turn-level annotations: given conversations annotated for quality on the utterance level, their approach is to train a model that can predict utterance quality on unseen conversations within a similar conversation setting. This strategy incurs a substantial cost whenever the training data needs to be updated to fit a novel conversational setting or chatbot. To avoid the cost of collecting turn-level labels, our approach is more in line with techniques such as multiple regression analysis, where fitting a model to a dataset is used to explain the relationship between features and some outcome, rather than to predict an outcome for unseen examples. In our case, our model can be fit to any dataset of conversations with conversation-level quality labels in order to estimate the quality impact of eachon the overall conversation quality. This approach has a couple advantages over existing work. First, collecting utterance-level annotations in a supervised setting is not necessary for our approach as it was for Choi et al. (2019) and Ghazarian et al. (2020). Second, our model learns empirically-derived relationships between the utterance-level quality prediction and the overall dialogue quality prediction, making this work the first to show that such weakly-supervised learning of utterance-level quality prediction is highly achievable. Introduction A typical life cycle of a dialogue system involves many iterative updates where developers improve the systemÃ¢ÂÂs language understanding capabilities and attempt to increase the overall user engagement. One of the most challenging aspects of executing these updates is to identify characteristics of the dialogue system that are impacting user experience the most. Doing so often involves manually crawling potentially thousands of system logs and designing statistical analyses, both of which are time consuming and unlikely to provide a holistic view of a systemÃ¢ÂÂs shortcomings. Inspired by this problem, the presented work investigates the extent to which it is possible to automatically distinguish turns within chat-oriented dialogues that have a negative effect on overall dialogue quality. The interpretation of dialogue quality is especially difficult in the chat-oriented dialogue setting due to its subjective and multi-faceted objectives.