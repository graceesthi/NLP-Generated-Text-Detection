Fairness is a principal social value that can be observed in civilisations around the world. A manifestations of this is in social agreements, often described in texts, such as contracts. Yet, despite the prevalence of such, a fairness metric for texts describing a social act remains wanting. To address this, we take a step back to consider the problem based on first principals. Instead of using rules or templates, we utilise social psychology literature to determine the principal factors that humans use when making a fairness assessment. We then attempt to digitise these using word embeddings into a multi-dimensioned sentence level fairness perceptions vector to serve as an approximation for these fairness perceptions. The method leverages a pro-social bias within word embeddings, for which we obtain an F1= 81.0. A second approach, using PCA and ML based on the said fairness approximation vector produces an F1 score of 86.2. We details improvements that can be made in the methodology to incorporate the projection of sentence embedding on to a subspace representation of fairness. Keywords: Text analysis, NLP, Fairness, Social Cognition, Psychometrics Introduction A lay person reading a document, which describes a person acting with malice towards another is able to predict that a conflict is likely due to the presence of an unfair act. This judgment has its roots in both social and biological factors owing to humans being a highly evolved social species (Tomasello 2014). We ask, is it possible to approximate these perceptions, and incorporate them into a form of measurement tool? One that would allow a sentence describing the interactions of two or more individuals to be approximated as being fair or unfair? While natural language processing (NLP) for textual analysis has been used across a large number of disparate domains such as personality detection (Youyou, Kosinski, and Stillwell 2015), healthcare (Gaudet-Blavignac et al. 2021; Le Glaz et al. 2021, 202; 2021), product reviews (Paruchuri et al. 2021) and dialect detection (Al Shamsi and Abdallah 2021), a search of the literature for a measure that is able to detect how fair a sentence is yields no results. Much of the work on the topic of fairness and machine learning (ML) has focused on longstanding challenges surrounding algorithmic fairness (Hellman 2020), de-biasing word embeddings (Sun et al. 2019; Badilla, Bravo-Marquez, and Pérez 2020), the use of counterfactuals (Corbett-Davies and Goel 2018; Chouldechova and Roth 2020), and problems associated with under representative data sets (Du et al. 2020). On the associated topic of morality, a number of papers have investigated the use of Moral Foundation Theory (MFT) (Graham et al. 2013) to analyse texts. MFT uses six foundations to parsimoniously explain the origins of human moral reasoning. These are Care/Harm, Fairness/Cheating, Loyalty/Betrayal, Authority/Subversion, Sanctity/Degradation, and Liberty/Oppression. The general approach has been to label a dataset of texts with these categories, and then apply a ML algorithm, e.g., long short-term memory (LSTM), support-vector machines (SVM), to learn the distinctions between each category (Rezapour, Shah, and Diesner 2019; Hoover et al. 2020; Rezapour, Dinh, and Diesner 2021). A similar approach has also been used based on pre-defined measures of moral language (Pennebaker, Francis, and Booth 2001; Araque, Gatti, and Kalimeri 2020). These approaches have proved useful as a form of topic modelling of language, yet they are unable to mark a sentence as being fair or unfair, or offer a degree of explainability as two why a classification was made, and to what degree each sentence is fair or unfair. Indeed a common challenge to ML systems of this type is that of limitations imposed by the technology on explainability (Danilevsky et al. 2020). This also presents a problem in work done using black-box ML to class documents as belonging to legally fair or unfair categories based on pre-trained legal clauses (Ruggeri et al. 2021; Lagioia et al. 2019). Their approach does not seek to identify explainable factors behind why something is perceived as fair or unfair, but instead, accept it as given that certain legal clauses are fair and others unfair, for which they implement ML methods to draw a classification boundary. Testing their API (“CLAUDETTE” 2021) with the sentence ‘the boy will hit the girl’, for example, produces the result: ‘Claudette found no potentially unfair clause’. Which may be a reflection of out-of-domain knowledge limitations. Work done by (Schramowski et al. 2019) and (Jentzsch et al. 2019) have demonstrated that language models (LM) such as Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al. 2019) hold implicit representations of moral values. They do so using vector comparisons based on a template of Do’s and Don’ts. Furthermore (Schramowski et al. 2019) replicates the moral choices found by (Jentzsch et al. 2019), then computes the variance explained by another LM, the Universal Sentence Encoder (USE) (Cer et al. 2018) with respect to Yes/No question templates on moral choices. Further work in (Izzidien 2021) replicates the finding that word embeddings contain implicit biases, and proposes to use them to make assessments of verbs. Building on these studies, we propose to harness these implicit social biases to act as a metric for an explainable assessment of sentences, specifically those related to fairness. However, in order to extract this bias, and instead of using a template of Do’s and Don’ts, we build on work in the social psychology literature on determining, which factors are able to explain acts of fairness made by humans, from which we extract a template that represents the principal perceptions, humans typically engage when making a fairness assessment. In doing so, we aim to approximate those perceptions, which we hypothesise will allow sentences to be classed according to which perception they are closer to, being fair or unfair. Although the paper does not set out to produce a fully validated and verified fairness measurement tool, it contributes to the development of one based on an approximation of the factors, humans engage when making such measurements. In effect, the metric may be considered a proxy for fairness perceptions. As such, we do not claim to be measuring a specific fairness type, e.g., distributional/outcome. However, as will be discussed, fairness evaluations engage a number of principal psychological factors. It is these factors that we attempt to approximate using a method of word embeddings. While the ML techniques used in this paper are well established, our approach to digitising the factors, and the theory behind their use in this domain is new. Not least as no such measure exists in the literature. The paper is organised as follows. The methods section is presented next, this incorporates a detailed study to determine the most explanatory psychological factors present in fairness assessments. The paper then details two approaches to digitise these psychological factors using word embeddings and ML. The results are then presented. A short discussion is followed by improvements, limitations, and the conclusion. This paper contributes originally with the following: 1) A new literature review to determine the principal factors that best explain pro-social acts, employing the dictator game (DG) to remove the confound of strategic intentions – i.e., where a pro-social act is engaged not out of fairness, but due to fear of punishment. 2) The use of the factors found in the above literature review to act as measures in multidimensional vector space. We implement an under-utilised method of vector linear algebra to define an ontological approximation of fairness perceptions. 3) The use of the above vectors as a measure of test sentences; are they closer to being: fair, or unfair, and to what degree?