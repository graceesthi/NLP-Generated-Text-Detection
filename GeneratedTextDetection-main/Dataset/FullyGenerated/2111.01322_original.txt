Abstract Meta-learning considers the problem of learn- ing an efficient learning process that can lever- age its past experience to accurately solve new tasks. However, the efficacy of meta-learning crucially depends on the distribution of tasks available for training, and this is often assumed to be known a priori or constructed from lim- ited supervised datasets. In this work, we aim to provide task distributions for meta-learning by considering self-supervised tasks automati- cally proposed from unlabeled text, to enable large-scale meta-learning in NLP. We design multiple distributions of self-supervised tasks by considering important aspects of task diver- sity, difficulty, type, domain, and curriculum, and investigate how they affect meta-learning performance. Our analysis shows that all these factors meaningfully alter the task distribu- tion, some inducing significant improvements in downstream few-shot accuracy of the meta- learned models. Empirically, results on 20 downstream tasks show significant improve- ments in few-shot learning – adding up to +4.2% absolute accuracy (on average) to the previous unsupervised meta-learning method, and perform comparably to supervised meth- ods on the FewRel 2.0 benchmark. Introduction Humans show a remarkable capability to accu- rately solve a wide range of problems efficiently – utilizing a limited amount of computation and experience. Deep learning models, by stark con- trast, can be trained to be highly accurate on a nar- row task while being highly inefficient in terms of the amount of compute and data required to reach that accuracy. Within natural language processing (NLP), recent breakthroughs in unsupervised pre- training have enabled reusable models that can be applied to many NLP tasks, however, learning of new tasks is still inefficient (Yogatama et al., 2019; Bansal et al., 2020a; Linzen, 2020). Meta-learning (Schmidhuber, 1987; Bengio et al., 1992; Thrun and Pratt, 2012) treats the learning process itself as a learning problem from data, with the goal of learning systems that can generalize to new tasks efficiently. This has the potential to produce few- shot learners that can accurately solve a wide range of new tasks. However, meta-learning requires a distribution over tasks with relevant labeled data that can be difficult to obtain, severely limiting the practical utility of meta-learning methods. In the supervised setting, in particular, meta- learning task distribution is often defined by sub- sampling from the classes in a classification prob- lem over a fixed dataset (Vinyals et al., 2016). This not only limits the applicability of meta-learning to the underlying classification problem, but also requires a diverse set of supervised datasets with a large number of classes to enable learning. Self- supervised meta-learning, on the other hand, seeks to propose tasks from unlabelled data (Hsu et al., 2019; Bansal et al., 2020b), and has great po- tential to enable numerous important applications (Hospedales et al., 2020) such as neural architec- ture search, continual learning, hyper-parameter optimization, learning in low-resource settings, etc. Existing work in meta-learning for NLP, how- ever, defaults to task distributions that tend to be overly simplistic, e.g. using existing supervised datasets (Han et al., 2018; Dou et al., 2019; Bansal et al., 2020a) or unsupervised cloze-style tasks with uniform selection of words from the vocabulary (Bansal et al., 2020b). Given the lack of explo- ration on this critical component, we propose to devise and evaluate various task distributions in the context of unsupervised meta-learning for NLP. Specifically, we explore a diverse set of ap- proaches to create task distributions that are in- ductive to better meta-training efficacy. We pro- vide empirical evidence that existing definitions of task distributions are prone to producing tasks that might not be challenging enough for the underlying model to learn useful representations, which in turn translates into poor downstream task performance. We therefore propose several new approaches that instead consider important features of the task dis- tribution including task diversity, difficulty, resem- blance to the downstream tasks, and the curriculum or the order in which tasks are presented during training. When evaluated on a suite of 20 NLP classification tasks, our best unsupervised meta- learning method leads to an absolute increase of up to +4.2% in average few-shot accuracy over unsu- pervised baseline results; and it even outperforms supervised meta-learning methods on FewRel 2.0 benchmark (Gao et al., 2019) on 5-shot evaluation. The paper is organized as follows. We start by providing some relevant background (2) on meta- learning and the unsupervised task generation ap- proach in SMLMT. Next, we introduce (3) new approaches to improve the task distribution. We then analyze (4.2) the different unsupervised distri- butions and how they relate to each other. Finally, we evaluate (4.3, 4.4) the different unsupervised methods on a wide range of NLP tasks including sentiment classification, entity typing, text classi- fication, sentence-pair classification and relation classification. Related Work Meta-learning applications in NLP have yielded improvements on specific tasks (Gu et al., 2018; Chen et al., 2018; Guo et al., 2018; Yu et al., 2018; Han et al., 2018; Dou et al., 2019). Unsupervised meta-learning has been explored in computer vi- sion (Hsu et al., 2019; Khodadadeh et al., 2019) and reinforcement learning (Gupta et al., 2018). Hsu et al. (2019) cluster images using pre-trained embeddings to create tasks. Metz et al. (2019) meta-learn an unsupervised update rule in a semi- supervised framework. Bansal et al. (2020b) de- veloped the SMLMT approach to unsupervised meta-learning in NLP. Contemporary work (Murty et al., 2021) explored the use of clustering, though focused only on natural language inference tasks. Curriculum learning (Bengio et al., 2009) in the context of meta-learning has been unexplored in NLP, prior to this work. Jabri et al. (2019) found unsupervised curriculum to be beneficial for meta- reinforcement learning. We refer to Hospedales et al. (2020) for a comprehensive review of meta- learning. Self-supervised learning has emerged as an effi- cient approach to representation learning in NLP (Howard and Ruder, 2018; Peters et al., 2018; De- vlin et al., 2018; Radford et al., 2019; Yang et al., 2019). Multi-task learning of pre-trained models has shown improved results on many tasks (Phang et al., 2018; Liu et al., 2019a), including few-shot setting. Yin et al. (2020) leveraged entailment tasks for few-shot learning. Du et al. (2020) developed self-training methods for semi-supervised few-shot learning. Recently, extremely large language mod- els have been shown to have few-shot capacities (Brown et al., 2020), while Schick and Schütze (2020) demonstrated few-shot capacities for small models in the semi-supervised setting. Meanwhile, Bansal et al. (2020a,b) showed meta-learning to be effective at improving few-shot performance in multi-task and unsupervised settings, as well as improving performance for small models. Conclusion We explored several approaches to self-supervised task distribution for meta-learning. Our results demonstrate improvements in few-shot perfor- mance over a wide-range of classification tasks. This demonstrates the utility of meta-learning from unlabeled data, opening up the possibility of large- scale meta-learning for pertinent applications in NLP such as continual learning, architecture search, learning for low-resource languages, and more.