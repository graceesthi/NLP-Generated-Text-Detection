ABSTRACT We introduce a new language representation model in finance called Financial Embedding Analysis of Sentiment (FinEAS). In financial markets, news and investor sentiment are significant drivers of security prices. Thus, leveraging the capabilities of modern NLP approaches for financial sentiment analysis is a crucial component in identifying patterns and trends that are useful for market participants and regulators. In recent years, methods that use transfer learning from large Transformer-based language models like BERT, have achieved state-of-the-art results in text classification tasks, including sentiment analysis using labelled datasets. Researchers have quickly adopted these approaches to financial texts, but best practices in this domain are not well-established. In this work, we propose a new model for financial sentiment analysis based on supervised fine-tuned sentence embeddings from a standard BERT model. We demonstrate our approach achieves significant improvements in comparison to vanilla BERT, LSTM, and FinBERT, a financial domain specific BERT. Introduction Sentiment analysis is a technique where text is classified as conveying positive or negative meaning through the use of natural language processing (NLP). In the field of finance, news and investor sentiment are significant drivers of the individual security prices and the market as a whole. Natural Language Understanding (NLU) capabilities in the financial domain are needed for automating tasks and analysis. Recently, practitioners of Natural Language Processing (NLP) who were involved in financial sentiment used a variety of cutting-edge machine learning algorithms, such as LSTMs. Researchers have quickly adopted modern NLP approaches to the financial domain, based on the latest successes in NLP, which leverage transfer learning from general Transformerbased language models. Nonetheless, there is a lack of knowledge about best practices for using Transformers in this domain. In this article, we propose an effective approach based on the use of Transformer language models that are explicitly developed for sentence-level analysis. More specifically, we build upon the findings from Sentence-BERT (Reimers and Gurevych, 2019). In this work, the authors show that vanilla BERT does not provide strong “out of the box” sentence embeddings (unlike the token embeddings, which are either state-of-the-art or close to the state-of-the-art, depending on the task). Since financial sentiment is a sentence-level task, we base our approach on Sentence-BERT. In Section 2, we briefly review related work. Then, in Section 3 we describe our proposed method. In Section 4, we present performance of our model and compare it to both common baselines and a domain-specific baseline. Finally, in Sections 5 and 6 we summarize our results and conclusions. We make the code1 and models2 publicly available. Related Work Background Researchers and practitioners started using the lexicon-based bag-of-words, with a predefined dictionary of positive and negative words. Here, one computes the sentiment score based on the number of matches of words in the text with each of the dictionaries. Indeed, this is exactly the approach taken by one of the first highly influential academic articles in the fields of text analysis in finance. In 2007, Paul Tetlock showed that the frequency of negative words in articles of the Wall Street Journal had predictive power of the future price moves of the Dow Jones Industrial Average Index and the daily volume traded on the New York Stock Exchange (Tetlock, 2007). Some work in the literature use a bag-of-words approach where the sentiment word lists are from the Loughran and McDonald financial dictionary (2009) (Loughran and McDonald, 2011). As a more advanced machine learning approach, researchers later introduced TF-IDF (Term Frequency–Inverse Document Frequency) for encoding text, and then trained supervised learning algorithms, such as SVMs (Support Vector Machines), with labelled datasets. An interesting application to Environmental, Social, and Governance (ESG) and UN Sustainable Development Goals (SDG) investing can be found in Madelyn Antoncic and Noguer-Alonso (2020). Current approaches With the advent of deep learning and its application in NLP, researchers began applying Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs) for text classification. The current state-of-the-art in text classification typically involves a purely attentional architecture, the Transformer architecture (Vaswani et al., 2017), especially by fine-tuning pre-trained models. Specifically for financial sentiment, we highlight Araci (2019a), in which the authors fine-tune a pre-trained Transformer in the Financial Phrasebank dataset (Malo et al., 2014). BERT Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2018) is a Transformer encoder pre-trained on large textual corpora without supervision. The attention mechanism of the Transformer allows obtaining contextual word embeddings, i.e. word embeddings taking into account the rest of the word embeddings in the sentence. For pre-training without supervision, BERT uses two surrogate tasks: • Masked Language Modeling (MLM): 15% of the tokens are randomly masked (i.e., replaced with the special token , and the model is asked to predicted them. To do so, the model must learn useful representations from the context. In this way, it learns to produce token-level embeddings. • Next Sentence Prediction (NSP): Each training instance consists of a sentence pair. Half of the time, the second sentence is a random sentence; in the other 50% of occurrences, the second sentence is the actual sentence that appears next to the original sentence. The model must predict, from the embeddings of the special token (class), whether the second sentence is the next one or not. In this fashion, it learns to produce sentence-level embeddings. Domain-specific models Several studies in different domains have demonstrated that domain-specific BERT models can outperform the generic BERT. Perhaps most well-known are BioBERT (Lee et al., 2019) and SciBERT (Beltagy et al., 2019) in the biomedical/scientific domain. In the case of the financial domain, starting from the original English BERT, FinBERT (Araci, 2019b) was fine-tuned on the Financial Phrasebank (Malo et al., 2014) and FiQA Task 1 sentiment scoring dataset,3 thereby achieving state-of-the-art results. Sentence-BERT In Reimers and Gurevych (2019), authors noted that the sentence embeddings obtained from vanilla BERT (the ones pre-trained with the NSP task) lack in quality. In fact, considerably simpler baselines are competitive with BERT in this regard (e.g., averaging word embeddings). Liu et al. (2019) emphasized that the NSP task was not as useful as thought, and authors suggested removing it from the BERT pre-training scheme. Consequently, Reimers and Gurevych (2019) propose the Sentence-BERT model. Starting from a pre-trained BERT checkpoint, they fine-tune it with supervision with a Siamese BERT network (meaning that they encode pairs of sentences with the same encoder), and predict the sentence entailment from the two sentence embeddings (Natural Language Inference (NLI) task). This approach results in more meaningful sentence representations. It is now well-known that pre-trained Transformers achieve state-of-the-art performance in NLP tasks (Araci, 2019a). In this article, unlike Araci (2019a), rather than starting from vanilla BERT, which is state-of-the-art for token-level embeddings but not for sentence-level tasks, we base our work on a model that has been fine-tuned for producing high-quality sentence embeddings. We believe this is a more sensible approach in the case of financial sentiment analysis. In contrast to Araci (2019a), we model financial sentiment as a continuous variable (from -1 to 1), instead of using discrete values.  Conclusion and Future Work We have demonstrated that FinEAS, a model based on BERT pre-trained on the general domain but fine-tuned for sentence-level tasks, is a sensible approach for financial sentiment classification. In conclusion, our model is simple to implement and outperforms several common baselines, including vanilla BERT and task-specific approaches. We make our code and model weights publicly available. In future work, we think it will be interesting to further explore Transformers in the financial domain, with an emphasis on models fine-tuned for sentence and/or document-level tasks.