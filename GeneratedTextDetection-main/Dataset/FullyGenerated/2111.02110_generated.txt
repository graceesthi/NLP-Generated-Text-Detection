Introduction In recent years, dialogue systems have attracted significant interests in both academia and industry. Especially the discipline of open-domain dialogue systems, aka chatbots, has gained great momentum. Yet, a long standing challenge that bothers the researchers is the lack of effective automatic evaluation metrics, which results in significant impediment in the current research (Mehri & Eskenazi, 2020). Common practice in assessing the performance of open-domain dialogue models involves extensive human evaluation on the final deployed models, which is both time- and cost- intensive. During the model development phase, researchers have to rely on standard automatic metrics, such as BLEU (Papineni et al., 2002) and perplexity, to tune the performance of their models. However, these metrics correlate poorly with human judgements (Liu et al., 2016) resulting in sub-optimal dialogue systems. Moreover, a recent trend in building open-domain chatbots involve pre-training dialogue models with a large amount of social media conversation data (Zhang et al., 2019; Adiwardana et al., 2019; Smith et al., 2020). However, the information contained in the social media conversations may be offensive and inappropriate. Indiscriminate usage of such data can result in insensitive and toxic generative models. Recently Xu et al., (2020) proposes recipes for safety in open-domain chatbots, such as unsafe utterance detection and safe utterance generation. Though these recipes help provide safe responses to toxic comments, the safe chatbot tends to avoid directly responding to these comments by switching to other topics. Sometimes, simply ignoring such comments may not be enough. Especially in the domain of customer support where the customer service personnel may occasionally receive offensive complaints, they need to answer them in a polite and appropriate way. Track Details This track consists of two tasks: 1. Participants will develop effective automatic open-ended dialogue evaluation metrics that perform robustly across a range of dialogue evaluation tasks. 2. Participants will build generative models that can respond to toxic questions with appropriate and polite sentences. 2.1. Automatic Metrics Development Effective automatic dialogue evaluation metrics possess the following two important properties as indicated in (Deriu et al., 2019): Ã¢ÂÂ Correlated to human judgements - the metrics should produce evaluation scores that well correlate to human judgements (scores) across multiple dialogue evaluation aspects. Ã¢ÂÂ Explainable - the metrics should provide constructive and explicit feedback to the generative models in terms of the quality of their generated responses. For instance, if a generative model is contradicting itself, the evaluation metrics should signal such behavior to the generative models. In this task, our goal is to seek effective automatic dialogue evaluation metrics that exhibit the above properties. These metrics can serve as a proxy to human evaluation for fast prototyping of open-domain chatbots. We have identified the following already existing datasets to test the effectiveness of the proposed evaluation metrics1Ã¢ÂÂ Ã¢ÂÂ: Ã¢ÂÂ DSTC6 human evaluation dataÃ¢ÂÂ (Hori et al., 2017) Ã¢ÂÂ DSTC7 human evaluation dataÃ¢ÂÂ (Galley et al., 2019) Ã¢ÂÂ Persona-Chatlog datasetÃ¢ÂÂ (See et al., 2019) Ã¢ÂÂ USR datasetÃ¢ÂÂ (Mehri & Eskenazi, 2020) Ã¢ÂÂ FED datasetÃ¢ÂÂ (Mehri & Eskenazi, 2020) Which all summing up to 33,998 turn-level and 3,440 dialogue-level human annotations respectively. The above list of evaluation tasks aim to examine the robustness of proposed automatic evaluation metrics and their ability to provide useful ratings across different dialogue evaluation dimensions. In addition, the proposed metrics are expected to correlate well with human judgements at both the conversation and the turn level. For each evaluation task, the Pearson correlation and Spearman correlation will be computed to compare the proposed evaluation metrics against human judgements. A final score will be calculated based on the correlation results for each evaluation item to rank the submitted evaluation metrics. Participants can propose their own metric or optionally improve two baseline evaluation metrics: D-Score (Zhang et al, 2021) or deep AM-FM (Zhang et al, 2020). A leaderboard will be provided allowing participants to check their progress based on correlations with the human evaluations and traditional objective metrics such as BLEU, ROUGE, BERTScore, etc. 2.2. Safe Chatbot Development One of the main requirements for open-domain chatbots is to be able to talk about any topic with enough deep knowledge and capability to produce a high user experience (Gabriel et al., 2020). However, not all users are cooperative and sometimes they express themselves with inappropriate words or expressions (e.g. swear words) without evenof that situation (i.e. being part of their normal behavior) or expressing toxic comments towards the chatbot or other people. In general, chatbots are designed to reject those behaviors using pre-defined apologize sentences, asking to move toward another topic or simply they are unable to handle such situations and, in the worst case scenario, agreeing with the toxic user due to the tendency of chatbots to repeat words from the user utterances or dialogue history. In this project we are attempting to provide safe responses to toxic comments, using standard safe utterance detection and safe chatbot reply phrases generation functions. Though these functions help provide safe responses to toxic comments, the safe chatbot tends to avoid directly responding to these comments by switching to other topics. Sometimes, simply ignoring such comments may not be enough. Especially in the domain of customer support where the customer service personnel may occasionally receive offensive complaints, they need to answer them in a polite and appropriate way. Track Details This track consists of two tasks: 1. Participants will develop effective automatic open-ended dialogue evaluation metrics that perform robustly across a range of dialogue evaluation tasks. 2. Participants will build generative models that can respond to toxic questions with appropriate and polite sentences. 2.1. Automatic Metrics Development Effective automatic dialogue evaluation metrics possess the following two important properties as indicated in (Deriu et al., 2019): Ã¢ÂÂ Correlated to human judgements - the metrics should produce evaluation scores that well correlate to human judgements (scores) across multiple dialogue evaluation aspects. Ã¢ÂÂ Explainable - the metrics should provide constructive and explicit feedback to the generative models in terms of the quality of their generated responses. For instance, if a generative model is contradicting itself, the evaluation metrics should signal such behavior to the generative models. In this task, our goal is to seek effective automatic dialogue evaluation metrics that exhibit the above properties. These metrics can serve as a proxy to human evaluation for fast prototyping of open-domain chatbots. 