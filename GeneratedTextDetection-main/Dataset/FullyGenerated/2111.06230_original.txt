Abstract African languages still lag in the advances of Natu- ral Language Processing techniques, one reason be- ing the lack of representative data, having a tech- nique that can transfer information between lan- guages can help mitigate against the lack of data problem. This paper trains Setswana and Sepedi monolingual word vectors and uses VecMap to cre- ate cross-lingual embeddings for Setswana-Sepedi in order to do a cross-lingual transfer. Word embeddings are word vectors that represent words as continuous floating numbers where se- mantically similar words are mapped to nearby points in n-dimensional space. The idea of word embeddings is based on the distribution hypothe- sis that states, semantically similar words are dis- tributed in similar contexts (Harris, 1954). Cross-lingual embeddings leverages monolingual embeddings by learning a shared vector space for two separately trained monolingual vectors such that words with similar meaning are represented by similar vectors. In this paper, we investigate cross- lingual embeddings for Setswana-Sepedi monolingual word vector. We use the unsupervised cross lin- gual embeddings in VecMap to train the Setswana- Sepedi cross-language word embeddings. We evalu- ate the quality of the Setswana-Sepedi cross-lingual word representation using a semantic evaluation task. For the semantic similarity task, we translated the WordSim and SimLex tasks into Setswana and Sepedi. We release this dataset as part of this work for other researchers. We evaluate the intrinsic qual- ity of the embeddings to determine if there is im- provement in the semantic representation of the word embeddings. Keywords: cross-lingual embeddings, word embed- dings, intrinsic evaluation Introduction Many African languages have insufficient language resources (data, tools, people) (Abbott & Martinus 2019, Martinus & Abbott 2019, Nekoto et al. 2020, Sefaraetal. 2021) and fall into the classification of low resource languages (Ranathunga et al. 2021) in the Natural Language Processing (NLP) field. This lack of resources makes it harder to capi- talise on the recent advances in many NLP down- stream tasks such as Neural Machine Transla- tion (Choetal. 2014), Large Language Models (Devlin et al. 2018, Howard & Ruder 2018), Q&A systems (Kwiatkowskietal. 2019), etc. There may be more downstream approaches to deal with some of these challenges such as Transfer Learning (Ruder et al. 2019), Data Augmentation (Marivate & Sefara 2020a), Multilingual Models (Hedderich et al. 2020), etc. Additionally, the lack of research attention to existing NLP tech- niques results in difficulties finding a benchmark (Abbott & Martinus 2019). In this work, we focus on word representations through word embeddings and how we can leverage one language to assist in the representation of another related language. These embeddings can then be used to develop tools for other downstream tasks. Word Embeddings are a mathematical technique to learn general language vector representations from a large amount of unlabelled text using co-occurring statistics. In recent years, monolingual word em- beddings techniques are increasingly becoming an important resource in NLP. Word embeddings are widely used in NLP problems such as sen- timent analysis (Socher et al. 2013), named-entity- recognition (Guo et al. 2014), parts-of-speech tag- ging, and document retrieval. Word2Vec is a vector training model proposed by Mikolov et al. (2013). Word2Vec produces a low-dimensional real-value vector representing the meaning of a word. The word vector represents grammatical and semantic properties, which results in words with similar se- mantic relations being close to each other. The word vector representation method incorporates the semantic relationship between words which is not possible through representations such as Bag- Of-Words of TFIDF. Word embeddings are better than both methods because they map all the words in a language into a vector space of a given dimen- sion, the words are converted into vectors and al- low multiple linear operations and have the prop- erty of preserving analogies (Mikolov et al. 2013, Pennington et al. 2014). Cross-lingual word embeddings have been receiv- ing more and more attention from the NLP com- munity, mainly because it has provided a path to effectively align two disjoint monolingual embed- dings with no bilingual dictionary for unsuper- vised techniques or no more than a small bilingual dictionary for supervised techniques (Lample et al. 2018, Artetxeetal. 2020). Cross-lingual tech- niques also enable knowledge transfer between lan- guages with rich resources and low resources. For languages lacking bilingual parallel corpus with other languages, cross-lingual embeddings can be utilised to train high-quality cross-lingual embed- dings (Lample et al. 2018). This can aid in accelerat- ing the progress of applying NLP to low-resourced languages. Artetxe et al. (2018) created the cross- lingual unsupervised or supervised word embed- ding (VecMap library) approach for training cross- lingual word embedding models. The approaches can be used to construct cross-language word vec- tors with or without a bilingual dictionary. The majority of South African languages lag bilin- gual parallel corpus with other languages. In this work, we aim to investigate how cross-lingual em- beddings could be used to improve the state of one or both languages. We used data (corpus) from different domains to train Word2Vec and fastText (Bojanowski et al. 2016) monolingual embeddings. When using VecMap, the two embeddings are aligned. VecMap requires two monolingual word vectors from source and target (Artetxe et al. 2018). To evaluate the effectiveness of the cross-lingual em- bedding for Setswana and Sepedi, we use intrinsic evaluation (Bakarov 2018) through Setswana and Sepedi versions of WordSim (Finkelstein et al. 2001) and Simlex (Hill et al. 2015). This is following on an approach that has been used for Yoruba and Twi (Alabi et al. 2019). We also release the dataset for this benchmark of human semantic similarity task. This paper is structured as follows; the next sec- tion is a review of related work that is done on cross-lingual word vectors. Followed by data col- lection in Section 3. Section 4 discusses methodol- ogy followed to train cross-lingual word vectors us- ing VecMap. The evaluation of the word vectors is discussed in Section 5. Section 5.1 explains the re- sults while Section 6 discusses the findings and fi- nally, conclusions and future work can be found in Section 7. Background Work and Related Cross-lingual word embeddings (CLWEs) are be- coming popular in NLP for two reasons: Cross- lingual word embeddings can transfer knowledge from rich-resourced languages to low-resourced; The technique can also infer the semantics of words in a multiple language environment. Conneau et al. (2018) show that word embeddings spaces can be aligned without any cross-lingual supervision. The alignment is based on solely unaligned datasets of each language. Using adversarial training, they were able to initialise a linear mapping between a source and a target space, which they use to create a synthetic parallel dictionary. First, they propose a simple criterion that is used as an unsupervised validation matric. Second, they propose the sim- ilarity measure cross-domain similarity local scal- ing (CSLS), which mitigates the hubness problem and increases the word translation accuracy. The hubness problem is defined by Dinu et al. (2015) as: ”neighbourhoods of the mapped ele- ments are strongly polluted by hubs, vec- tors that tend to be near a high propor- tion of items, pushing their correct labels down the neighbour list.” In the work done by Adams et al. (2017), the re- search looked at applying CLWEs to Yongning Na, a Sino-Tibetan language. The research focused on determining if the quality of CLWEs depends on having large amounts of data in multiple lan- guages and if initialising the parameters of neu- ral network language models (NMLM) can im- prove language modelling in a low-resourced con- text. The research scaled down the available mono- lingual data of the target language to about 1000 sentences. The quality of intrinsic embedding was assessed by taking into consideration correla- tion with human judgement on the WordSim353 (Finkelstein et al. 2001) test set. They went further to perform language modelling experiments by ini- tialising the parameters for long short-term mem- ory (LSTM) (Hochreiter & Schmidhuber 1997) by training across different language pairs. The re- search showed that CLWEs are resilient even when target language training data is scaled-down and that initialisation of NMLM parameters leads to good performance. Artetxe & Schwenk (2019) introduced an architec- ture that can be used to learn multilingual sentence representations for more than 90 languages. The languages belonged to 30 different families. The re- search used a single BiLSTM encoder with a shared Byte Pair Encoding (BPE) vocabulary coupled with an auxiliary decoder and trained on parallel corpora. They learn a classifier using English annotated data only and transfer it to any language without modification. The research mainly focused on vector rep- resentations of sentences that are general for the in- put language and the NLP task. Alabi et al. (2019) worked on massive vs. curated embeddings for low-resourced languages: the case of Yoru` ba ́ and Twi. Authors compare two types of word embeddings obtained from curated cor- pora and a language-dependent processing. They move further to collect high quality and noisy data for the two languages. They quantify that im- provements that is based on the quality of data and not only on the amount of data. In their ex- periments, they use different architectures to learn word representations both from characters and sur- face forms. They evaluate multilingual BERT on a down stream task, specifically named entity recog- nition and WordSim-353 word pairs dataset. Feng et al. (2018) investigates a cross-lingual knowl- edge transfer technique to improve the seman- tic representation of low-resourced languages and improving low resource named-entity recognition. In their research, neural networks are used to do knowledge transfer from high resource language us- ing bilingual lexicons to improve low resource word representation. They automatically learn semantic projections using a lexicon extension strategy that is designed to address out-of lexicon problem. Fi- nally, they regard word-level entity type distribu- tion features as an external language independent knowledge and incorporate them into their neural architecture. The experiment is done on two low resource languages (Dutch and Spanish) to demon- strate the effectiveness of these additional semantic representations. Banerjee et al. (2021) show that initialising the em- bedding layer of Unsupervised Neural Machine Translation (UNMT) models with cross-lingual embeddings shows significant improvements in BLEU score. Authors show that freezing the em- bedding layer weights lead to better gains com- pared to updating the embedding layer weights during training. They experimented using De- noising Autoencoder (DAE) and Masked Sequence to Sequence (MASS) for three different unrelated language pairs (for English-Hindi, English-Bengali, and English-Gujarati). The analysis shows the im- portance of using cross-lingual embedding as com- pared to other techniques. The literature shows that there is a substantial amount of work done on cross-lingual transfer and empirical proof that the method improves the per- formance of models. The literature does not relay solely on intrinsic evaluation but the solutions are applied to some downstream tasks. In the next sec- tion, we detail the data used for conducting experi- ments. Conclusion In this paper, VecMap was used to align Setswana- Sepedi to the same vector space. Through this work, we wanted to use cross-lingual (VecMap) technique to enable knowledge transfer between languages with rich resources and low resources. The results show that it is possible to align two monolingual embeddings to get cross-lingual embeddings. We mapped Setswana to Sepedi and used Spearman’s to check correlation. Interestingly we get different re- sults on fastText and word2Vec-based embeddings though we used the same data to train the embed- dings. In future work, it would be interesting to use the cross-lingual embedding on a downstream task like translation or sentiment analysis specifically for low-resourced languages.