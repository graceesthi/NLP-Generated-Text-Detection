Abstract Recent methods in speech and language tech- nology pretrain very LARGE models which are fine-tuned for specific tasks. However, the benefits of such LARGE models are often lim- ited to a few resource rich languages of the world. In this work, we make multiple con- tributions towards building ASR systems for low resource languages from the Indian sub- continent. First, we curate 17,000 hours of raw speech data for 40 Indian languages from a wide variety of domains including education, news, technology, and finance. Second, using this raw speech data we pretrain several vari- ants of wav2vec style models for 40 Indian languages. Third, we analyze the pretrained models to find key features: codebook vec- tors of similar sounding phonemes are shared across languages, representations across lay- ers are discriminative of the language family, and attention heads often pay attention within small local windows. Fourth, we fine-tune this model for downstream ASR for 9 languages and obtain state-of-the-art results on 3 public datasets, including on very low-resource lan- guages such as Sinhala and Nepali. Our work establishes that multilingual pretraining is an effective strategy for building ASR systems for the linguistically diverse speakers of the Indian subcontinent. Introduction The Indian subcontinent is one of the most linguis- tically diverse regions in the world as well as one of the most populous regions in the world - little won- der that it is called a ‘subcontinent’. It is home to around 2 billion people from 7 countries who speak 100+ major languages (and thousands of minority languages and dialects) belonging to four major language families1,2. Of these, the Indo-Aryan and Dravidian languages are spoken by a large section of the population. These language families have intermingled over a large period of time giving rise to the Indian linguistic area/sprachbund where lan- guages across these families share many features (Emeneau, 1956; Subba ̄ra ̄o, 2012; Kunchukuttan and Bhattacharyya, 2020). Building high-quality ASR models for such a large and diverse pool of languages is challenging, even if we restrict ourselves to 30 odd languages which have more than a million speakers. Many modern ASR models rely on large amounts of la- beled data for each language to build high-quality ASR systems. Such approaches are expensive and not scalable, thus limiting the reach of ASR tech- nologies to some languages and a section of the population. In addition to these challenges on avail- ability of labeled data, Indian languages also face a set of common challenges that need to be addressed. Most Indic scripts have a larger character set than English. The complex inflectional systems of In- dian languages make modelling phonotactics more challenging. The rich inflectional/agglutinative na- ture of Indian languages results in larger vocabu- lary sizes, presenting challenges to incorporating language models. At the same time there are oppor- tunities from a unified perspective. Collection of unlabeled data for pretraining can be undertaken as a joint effort since many sources might be shared amongst these languages. A largely overlapping phoneme inventory, logically overlapping character sets and syntactic similarity can help utilize linguis- tic similarity at various levels to build multilingual models where transfer learning can be effective. In this context, it is important to take note of recent work that has demonstrated the benefits of unsupervised pretraining and multilingual fine- tuning to significantly improve ASR quality for low-resource languages (Baevski et al., 2020b; Conneau et al., 2020a). In particular, the wav2vec model has established two key results for English ASR. One, an end-to-end DNN architecture borrowing the popular Transformer architecture from NLP establishes SOTA results. And two, pre- training on a large corpus of data reduces the re- quirement for labeled fine-tuning from hundreds of hours to few hours and to even tens of min- utes. It is worthwhile to explore if these findings from English ASR transfer to Indic ASR, espe- cially given the diversity and above mentioned challenges with Indic languages. More specifi- cally, would a wav2vec-like model establish SOTA results on available benchmarks across Indic lan- guages? Further, would large pretraining preclude the need for collecting large amounts of labeled data? And finally, would pretraining a multilingual model across Indian languages provide positive benefits across these related languages? In order to answer these questions, we make the following contributions: • We curate 17,314 hours of raw audio data for pretraining across 40 languages from 4 language families, making it one of the largest and most diverse collections of Indic language data. This data has been curated in a short time frame from public sources which have a permissive license. It indicates that the feasibility of collecting large amount of pretraining data and further efforts can be made to significantly expand this collection. • Starting from the wav2vec 2.0 model, we perform extensive ablation studies on architecture choices, pretraining and fine-tuning strategies, language models and choice of lexicon to arrive at a training and decoding regimen that works well for Indian languages. • Our ASR models achieve SOTA performance on 9 Indian languages on 3 publicly available bench- marks with small fine-tuning datasets. These re- sults indicate that end-to-end ASR systems based on multilingual pretraining with the wav2vec model hold promise for Indic languages. • Our ablation studies reveal that the accuracy of the ASR system on Indic languages sensitively de- pends on the size of the pretraining corpus, amount of labelled data for fine-tuning, and access to task- specific lexicon. In summary, we establish that the recent ad- vances of pretraining wav2vec models transfer to Indic ASR and achieve SOTA results against mod- els proposed over multiple years. However, unlike in the reported results of English ASR, we observe that the WER reported for Indic ASR is signifi- cantly higher and sensitively depends on availability of resources: pretraining corpus, fine-tuning data, and task-specific language information. This suggests that the ASR task on Indic languages re- mains far from being solved and requires model innovation and continued efforts on curating re- sources. We publicly release all the artifacts of our work to spur further work in the area of Indic ASR. This includes: (a) sources of pretraining data along with scripts for their collection and pre-processing, (b) pretraining, fine-tuning and decoding scripts, (c) language models, and (d) our best ASR models. Conclusion We report results of applying two recent and suc- cessful ideas from English ASR to Indic ASR: use of wav2vec like model architecture and use of un- labelled data to pretrain the model. We implement this with a curated dataset on Indic languages and a range of ablation studies on architecture, pretrain- ing, fine-tuning, and decoding choices. Through this, we obtain state-of-the-art results on 9 Indic languages across 3 datasets. While advancing ASR systems for the next billion users from the sub- continent, our results highlight the need for larger resources and benchmarks across more languages. All the models developed as a part of this work, viz., the pretrained model, the language-specific fine-tuned models and the language models along with the Fairseq and KenLM scripts and configu- ration files used for building them will be publicly released. We hope that these models will help in advancing the state of the art for Indian Speech Technology.