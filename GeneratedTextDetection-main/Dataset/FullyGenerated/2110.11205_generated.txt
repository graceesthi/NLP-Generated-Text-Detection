ABSTRACT While deep learning through empirical risk minimization (ERM) has succeeded at achieving human- level performance at a variety of complex tasks, ERM generalizes poorly to distribution shift. This is partly explained by overfitting to spurious features such as background in images or named entities in natural language. Synthetic data augmentation followed by empirical risk minimization (DA-ERM) is a simple yet powerful solution to remedy this problem. In this paper, we propose data augmented invariant regularization (DAIR). The idea of DAIR is based on the observation that the model performance (loss) is desired to be consistent on the augmented sample and the original one. DAIR introduces a regularizer on DA-ERM to penalize such loss inconsistency. Both theoretically and through empirical experiments, we show that a particular form of the DAIR regularizer consistently performs well in a variety of settings. We apply it to multiple real-world learning problems involving domain shift, namely robust regression, visual question answering, robust deep neural network training, and task-oriented dialog modeling. Our experiments show that DAIR consistently outperforms ERM and DA-ERM with little marginal cost and setting new state-of-the-art results in several benchmarks. Introduction Deep neural networks are widely used in various applications ranging from computer vision to language processing. While deep learning has surpassed human-level performance in numerous tasks, neural networks are extremely vulnerable to overfitting to spurious correlations and therefore fail to generalize even under slight perturbations of the test distribution [Arjovsky et al., 2019]. This observation motivated the research community to tackle the problem of domain generalization (see [Ribeiro et al., 2020] for a detailed literature review). Recent benchmark datasets, such as Rotated MNIST [Arjovsky et al., 2019], Colored MNIST [Arjovsky et al., 2019], PACS [Li et al., 2017], VLCS [Fang et al., 2013], Office-Home [Venkateswara et al., 2017], Terra Incognita [Beery et al., 2018] and DomainNet [Peng et al., 2019], have shown difficulties for the generalization of deep neural network models under distribution shifts, and have sparked invention of many new algorithmic frameworks to address domain generalization. A standard approach for improving out-of-distribution performance is to guarantee that learned representations are invariant to certain transformations. For example, image representations and trained models for computer vision should generally be invariant to rotations, changes in color, or background. There are two main directions for promoting such invariance to transformations, namely data augmentation and geometric deep learning. Data augmentation promotes invariances in learned representations by curating synthetic examples that exhibit the desired invariances. Zhang et al. [2017] introduced mixup to train a neural network on convex combinations of pairs of examples and their labels, which improves the generalization of state-of-the-art neural network architectures. Volpi et al. [2018] proposed an adaptive data augmentation method where adversarial examples are generated at every iteration, offering performance gain over unseen domains. Kuznichov et al. [2019] showed data augmentation can be applied to leaf segmentation by proposing a method that preserves the geometric structure of the data objects and keep the physical appearance of the data-set as close as possible to imaged plants in real agricultural scenes. The proposed method provides state of the art results when applied to the standard benchmark in the field. Tellez et al. [2019] showed stain color augmentation and stain color normalization could be used in computational pathology applications. Goel et al. [2020] proposed an approach to patch a model that fails due to spurious features on a real-world skin cancer dataset by data augmentation. Zhou et al. [2020] showed data augmentation with adversarial images could make the label classifier more robust to unknown domain shifts. Nam et al. [2021] improved domain generalization by reducing the intrinsic style bias of CNNs. This is achieved by training a separate network for randomizing the style of images and generating augmented data during training. Geometric deep learning bakes such invariances into the neural network architecture. For example, convolutional layers [Lecun et al., 1998] are fundamentally preserving translations. There are other specifically designed networks to maintain invariances: Zaheer et al. [2017] studied the problem of designing models for machine learning tasks defined on sets and characterized the permutation invariant functions. Bloem-Reddy and Teh [2020] obtained generative functional representationsprobability distributions that are invariant under the action of a compact group. Finzi et al. [2021] provided an algorithm for solving for the equivariant layers of matrix groups. Besides two main directions mentioned above, researchers have proposed numerous algorithmic solutions to impose invariance and improve domain generalization: Ganin et al. [2016] introduced a new representation learning approach for domain adaptation, in which data at training and test time come from similar but different distributions. [Arjovsky et al., 2019] proposed invariant risk minimization (IRM) Ã¢ÂÂ a novel learning paradigm that estimates nonlinear, invariant, causal predictors from multiple training environments, to enable out-of-distribution generalization. The idea is to learn representations that perform equally well across different environments. Sagawa et al. [2019] introduced distributionally robust optimization (DRO) framework to learn models by minimizing the worst-case training loss over a set of pre-defined groups/environments. Li et al. [2018a] proposed a novel meta-learning method for domain generalization (MLDG), which simulates domain shift during training by synthesizing virtual testing domains within each mini-batch. Correlation alignment for deep domain adaptation [Sun and Saenko, 2016] (Deep CORAL) learns a nonlinear transformation that aligns correlations of layer activations in deep neural networks. Li et al. [2018b] extended adversarial autoencoders by imposing the Maximum Mean Discrepancy (MMD) measure to align the distributions among different domains, and matching the aligned distribution to a prior distribution via adversarial feature learning. Li et al. [2018c] proposed a conditional invariant neural network that minimizes the discrepancy in conditional distribution of images given the class labels across different domains. The approaches listed above are more complex than simple training mechanisms such as empirical risk minimization (ERM) and hence they cannot be easily applied to involved tasks such as training generative models. In contrast, Gulrajani and Lopez-Paz [2020] demonstrated that the simple ERM method can achieve state of the art performance after fine-tuning in various datasets/applications. However, there are still problem instances that ERM performs very poorly. For example, in learning end-to-end dialogue models, Qian et al. [2021] showed 29% performance drop on MultiWOZ [Budzianowski et al., 2018] due to the memorization of named entities. In this paper, we propose a regularization technique, called data augmented invariant regularization (DAIR) that penalizes the inconsistency of loss on augmented samples with respect to the original ones. DAIR is applicable whenever data augmentation is used to promote invariances. DAIR only requires marginal additional cost on top of data augmentation, and is simple and applicable to a wide host of supervised and unsupervised learning tasks, including generative models. We introduce the DAIR formulation, motivate it, and theoretically prove some of its properties in Section 2. We empirically evaluate DAIR on a variety of problem setups ranging from defense against adversarial attacks to domain generalization in the presence of environment shift in Section 3. Our experimental results show that DAIR is competitive with or even outperforms state-of-the-art algorithms specifically designed for imposing invariance in these problems. Conclusion. In this paper, we proposed a simple yet effective regularizer that can be used wherever data augmentation is used to promote invariance. We rigorously showed that our proposed regularizer can recover the optimal solution in certain regression task where simple data augmentation can not. We also compare our DAIR-SQ regularizer with the existing off-the-shelf regularizers such as L1, L2 and KL divergence. We empirically showed that the DAIR-SQ regularizer is competitive with or outperforms state-of-the-art problem-specific baselines in a variety of problem setups. We evaluated DAIR in four different categories of machine learning tasks including regression, visual question answering, and training robust deep neural networks, and task-oriented dialog modeling. This is a major benefit of DAIR-SQ as some of other regularizers cannot be applied to regression tasks. Empirically the proposed algorithm outperforms well in all tasks. Better understanding of the tuning of the corresponding hyperparameter also remains as an open area for research. Finally, a more in-depth theoretical understanding of the properties of DAIR-SQ regularizer that lead to its superior empirical performance are also important questions for future work. While wethat DAIR-SQ boosts existing performance metrics, such as average accuracy, the interplay of DAIR-SQ with other metrics, especially group fairness is also another important area for future research.