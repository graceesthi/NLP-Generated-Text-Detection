Abstract We present a method for using adverb phrases to adjust skill parameters via learned adverb-skill groundings. These groundings allow an agent to use adverb feedback provided by a human to directly update a skill policy, in a manner sim- ilar to traditional local policy search methods. We show that our method can be used as a drop-in replacement for these policy search methods when dense reward from the environ- ment is not available but human language feedback is. We demonstrate improved sample efficiency over modern policy search methods in two experiments. Introduction In collaborative human-robot environments, embodied agents must be capable of integrating language commands into behavior. Natural language instruction can range from feedback on the subtlest of movements to abstract, high- level plans. While humans are generally capable of giving and receiving language feedback regarding all aspects of a task, methods for integrating language understanding into behavior differ based on the level of behavioral abstrac- tion the instruction refers to. Following Patel, Rodriguez- Sanchez, and Konidaris (2020), who argued that the struc- ture of language closely relates to the structure of an agentÃ¢ÂÂs decision process, we focus on grounding adverbsÃ¢ÂÂwords used to describe the quality of a verb (i.e. a skill)Ã¢ÂÂto di- rectly modify skill execution. We adopt the framework of hierarchical reinforcement learning (Barto and Mahadevan 2003), wherein an agentÃ¢ÂÂs behavior is mainly generated by skills responsible for low- level motor control, and learning is primarily concerned with sequencing given skills to solve a task. Much existing re- search on integrating language understanding into hierarchi- cal agents attempts to map language to sequences of abstract skill executions (Andreas, Klein, and Levine 2017; Mei, Bansal, and Walter 2016; Oh et al. 2017). However, agents must also be able to use language to modify their underly- ing skill policies. Commands like Ã¢ÂÂlift the pallet higherÃ¢ÂÂ and Ã¢ÂÂcrack the egg gentlyÃ¢ÂÂ clearly request adjustments to a spe- cific skill execution. Therefore, a key question is how natural language understanding can ground to changes in the lowest levels of behavior. The existence of adverbs that modify discrete verbs calls for agents with a discrete set of skills, with behavior that can be modified by a parameter vector describing how the skill can be executed (Da Silva, Konidaris, and Barto 2012; Masson, Ranchod, and Konidaris 2016). We propose a novel method for grounding adverbs to adjustments in skill parametersÃ¢ÂÂcalled adverb-skill groundingsÃ¢ÂÂwhich, when integrated into policy search, lead to greater sample- efficiency than traditional policy search methods that typi- cally depend on explicit reward from the environment. We demonstrate the effectiveness of adverb-skill groundings for policy search in a toy ball-throwing domain and a domain involving a simulated 7-DoF robot arm. We compare the sample efficiency of our approach to PI2-CMA (Stulp and Sigaud 2012), a state-of-the-art local policy search method. Related Work Natural Language in Reinforcement Learning Most existing research that has used natural language in re- inforcement learning problems can be categorized as either language-conditional (in which agents must interact with language to solve problems) or language-assisted (in which language can be used to facilitate learning) (Luketina et al. 2019). Although our setting is language-conditional, since the agent is presumed only to have access to natural lan- guage feedback, there are related works in both categories. Some previous research has attempted to map language instructions to reward functions. Arumugam et al. (2017) map natural language instructions to goal-state reward func- tions at multiple levels of abstraction within a planning hi- erarchy over an object oriented MDP formalism. While this approach has the advantage of being able to interpret instruc- tions at multiple levels of abstraction, the base-level actions used in the approach are significantly more abstract than mo- tor skills. Goyal, Niekum, and Mooney (2019) shape reward functions by interpreting previous actions to see if they can be described by a natural language instruction, effectively grounding natural language directly to action sequences. In contrast, our work grounds atomic language fragments di- rectly to continuous skill parameters, which allows us to make granular adjustments to the execution of motor skills via language commands. Other research has mapped symbolic instructions directly to policy structure. Andreas, Klein, and Levine (2017) learn a mapping from symbolic policy sketches to sequences of modularin the form of options (Sutton, Pre- cup, and Singh 1999). Shu, Xiong, and Socher (2017) use language instructions to help a hierarchical agent decide whether to use a previous skill or to learn a new one. Hu et al. (2019) similarly map language instructions to macro-actions in a real-time strategy game, which are then performed us- ing a separate model. Tellex et al. (2011) generate high-level plans from the semantic structure of language instructions. Gopalan et al. (2020) derive symbol sketches from demon- strated navigation trajectories which they ground language instructions to. While all of these works ground language to agent behavior, ours is the first to integrate language feed- back at the level of modifying the low-level behavior of motor skills. Semantic Representation While earlier semantic space representations were mainly concerned with encoding individual words or n-grams into vector space (Lund and Burgess 1996; Landauer and Du- mais 1997), there has been recent discussion regarding how to capture phrases and sentences with similar machinery. Mitchell and Lapata (2010) explore this problem, which hinges on linguistic structures as being compositional, i.e. that the meaning of a language fragment is a function of the meanings of its composite parts. Compositionality itself has been accounted for in older logic-based formalisms (Mon- tague 1974), but incorporating compositionality into modern semantic space representations is still an unsolved problem. Baroni and Zamparelli (2010) proposed a candidate so- lution that accounts for compositionality in semantic space models by representing nouns as vectors and adjectives as matrices, and the meaning of their combinations to be their tensor products. Krishnamurthy and Mitchell (2013) ex- pand on this idea by using Combinatory Categorial Gram- mar (CCG) (Steedman 1996) to prescribe tensors of various modes to syntactic categories, whose weights they learn via a training process that utilizes a corpus. While the primary focus of this research is not on semantic models, we firmly believe that core linguistic principlesÃ¢ÂÂsuch as the principle of compositionalityÃ¢ÂÂ should be considered when designing systems for ground- ing language to behavior. Accordingly, we utilized the syn- tax/semantics formulation laid out by Steedman (1996) and the intuition behind more recent compositional distribu- tional semantics research (Baroni and Zamparelli 2010; Kr- ishnamurthy and Mitchell 2013) in our strategy for ground- ing adverbs. Adverbs by the CCG account are functions from verbs to verbs, and adverbs by our account are simi- larly functions from skills to skills. Conclusion We have presented a novel method for efficiently integrating granular natural language feedback into low-level behavior. The method relies on learning adverb-skill groundingsÃ¢ÂÂ mappings of adverbs to adjustments in skill parametersÃ¢ÂÂ which can be learned once using few training examples and do not require the agent to interact directly with environ- ment reward. Using adverb-skill groundings, an agent can integrate adverb feedback into a policy searchÃ¢ÂÂin place of sample-based direct policy search methodsÃ¢ÂÂand achieve an order of magnitude increase in sample efficiency. This work can be extended in several directions. First, the ability of our model to ground adverbs is limited by the pa- rameterization of the skill. If no variation in skill parame- ters could result in the desired effect of an adverb, a new skill parameterization should be learned with more expres- sive power. Second, humans typically do not need to learn how to ground adverbs anew each time they learn a new skill, but our model does. Future work might consider the ability for agents to transfer adverb groundings to new skills. Another important question to address is how to learn lan- guage embeddings which can exclusively capture the mean- ings of language commands as they relate to specific tasks. Our embedding procedure was designed by a human expert with knowledge of the tasks and which adverbs most apply to it. Future work should look to relax this constraint, per- haps by defining a broad and exhaustive set of adverbs of motion which can be applied to any task.