ABSTRACT The use of contrastive loss for representation learning has become prominent in computer vision, and it is now getting attention in Natural Language Processing (NLP). Here, we explore the idea of using a batch-softmax contrastive loss when fine-tuning large-scale pre-trained transformer models to learn better task-specific sentence embeddings for pairwise sentence scoring tasks. We introduce and study a number of variations in the calculation of the loss as well as in the overall train- ing procedure; in particular, we find that data shuffling can be quite important. Our experimental results show sizable improvements on a number of datasets and pairwise sentence scoring tasks including classification, ranking, and regression. Finally, we offer detailed analysis and discussion, which should be useful for re- searchers aiming to explore the utility of contrastive loss in NLP. INTRODUCTION Recent years have seen a revolution in Natural Language Processing (NLP) thanks to the advances in machine learning. While a lot of attention has been paid to the architectures, especially for deep learning, there has been less focus on studying loss functions. At the same time, loss functions based on similar or on the same ideas were reinvented multiple times under different names. This can cause difficulties when solving new problems or when designing new experiments based on previous results. To a greater extent, this applies to Ã¢ÂÂuniversalÃ¢ÂÂ loss functions, which can be applied in different machine learning areas and tasks such as Computer Vision (CV), Recommendation Systems, and NLP. An example of such universal loss function is the batch-softmax contrastive (BSC) loss, which we will discuss below. Our contributions can be summarized as follows: We study the use of a batch-softmax contrastive loss for fine-tuning large-scale transformers to learn better task-specific sentence embeddings for pairwise sentence scoring tasks. We introduce and study a number of variations in the calculation of the loss such as sym- metrization, incorporating labeled negatives, aligning scores on the similarity matrix diago- nal, normalizing over the batch axis, as well as in the overall training procedure, e.g., shuf- fling, trainable temperature, and sequential pre-training. We demonstrate sizable improvements for a number of pairwise sentences scoring tasks such as classification, ranking, and regression. We offer detailed analysis and discussion, which would be useful for future research. RELATED WORK The contrastive loss was proposed by Hadsell et al. (2006) as metric learning that contrasts Eu- clidean distances between embeddings of samples from one class and between samples from dif- ferent classes. Weinberger et al. (2006) suggested the triplet loss, which is based on a similar idea, but uses triplets (anchor, positive, negative), and aims for the difference between the distances for (anchor, positive) and for (anchor, negative) to be larger than a margin. N-pair loss was presented as a generalization of the contrastive and the triplet losses as a way to solve the problem of extensive construction of hard negative pairs and triplets (Sohn, 2016). To this end, a batch of N pairs of examples from N different classes is sampled, and the first element in each pair is considered to be an anchor. Thus, for each anchor, there are one positive and N ÃÂ 1 negative pairs. The loss contrasts the distances simultaneously using the softmax function over dot- product similarities. The approach was used successfully in computer vision (CV) tasks. The same method of Multiple Negative Ranking for training Dot-Product Scoring Models was applied to rank- ing natural language responses to emails (Henderson et al., 2017), where the loss uses labeled pairs. A similar idea, called Negative Sharing, was used to reduce the computational cost when training recommender systems (Chen et al., 2017). Wu et al. (2018) presented an approach with N -pairs like logic, as a Non-Parametric Softmax Classifier, replacing the weights in the softmax with embeddings of samples from such classes. It was also proposed to use L2 normalization and temperature. Yang et al. (2018) proposed to use Multiple Negative Ranking to train general sentence representations on data from Reddit and SNLI. Logeswaran & Lee (2018) presented a Quick-Thoughts approach to learn sentence embeddings, which constructs batches of contiguous sets of sentences, and for each sentence, contrasts the next sentence in the text and all other candidates.lot of subsequent work has focused on maximizing Mutual Information (MI). Oord et al. (2018) presented a loss function based on Noise-Contrastive Estimation, called InfoNCE. It models the ÃÂ¢ÃÂÃÂsimilarityÃÂ¢ÃÂÃÂ function that estimates the MI between the target (future) and the context (present) signals, and maximizes the MI between temporally nearby signals and signals from temporally nearby signals. If this function expresses the dot-product between embeddings of samples from such classes and between samples from dif- ferent classes. Weinberger et al. (2018) presented a loss function based on Noise-Contrastive Estimation, called InfoNCE. It models the ÃÂ¢ÃÂÃÂsimilarityÃÂ¢ÃÂÃÂ function that estimates the MI between the target (future) and the context (present) signals, and maximizes the MI between temporally nearby signals and signals from temporally nearby signals. If this function expresses the dot-product between embeddings of samples from such classes and between samples from dif- ferent classes. Weinberger et al. (2018) presented a loss function based on Contrastive Estimation, called InfoCSE. It models the ÃÂ»Ã¢ÂÂ similarity between embeddings of samples from such classes and between samples from dif- ferent classes. Weinberger et al. (2018) presented a loss function based on Contrastive Learning, called InfoCLS. It models the ÃÂ»Ã¢ÂÂ similarity between embeddings of samples from such classes and between samples from dif- ferent classes. Weinberger et al. (2018) presented a loss function based on Neural Information Systems, called Nys. It models the number of pairs of signals in a batch and in the overall training procedure, e.g., shuf- fling, trainable temperature, and sequential pre-training. Nys (2018) presented a loss function based on Contrastive Learning, called NysST. It models the number of pairs of signals in a batch and in the overall training procedure, e.g., shuf- fling, trainable temperature, and sequential pre-training. Nys (2018) presented a triplet loss function, which models the number of pairs of signals in a batch and in the overall training procedure, e.g., shuf- fling, trainable temperature, and sequential pre-training. Nys (2018) presented a loss function based on L2 normalization and temperature. Yang et al. (2018) presented a loss function based on Contrastive Learning, called Noise-Contrastive Loss, which models the loss function that contrasts the loss function's weights with the embeddings of samples from such classes. It uses L2 normalization and temperature. Yang et al. (2018) presented a loss function based on Noise-Contrastive Learning, called Noise-Contrastive Loss, which models the loss function that contrasts the loss function's weights with the embeddings of samples from such classes. It uses L2 normalization and temperature. Yang et al. (2018) presented a loss function based on Class-Based Loss, called Class-A Loss, which models the loss function that contrasts the loss function's weights with the embeddings of samples from such classes. It uses L2 normalization and temperature. Yang et al. (2018) presented a loss function based on Noise-Contrastive Learning and Class-A Loss, called Class-A Loss, which models the loss function that contrasts the loss function's weights with the embeddings of samples from such classes. It uses L2 normalization and temperature. Yang et al. (2018) presented a loss function based on Noise-Contrastive Learning and Class-A Loss, called Class-A Loss, which models the loss function that contrasts the loss function's weights with the embeddings of samples from such classes. It uses L2 normalization and temperature. Yang et al. (2018) presented a loss function based on Class-A Loss, called Class-A Loss, which models the loss function that contrasts the loss function's weights with the embeddings of samples from such classes. It uses L2 normalization and temperature. Yang et al. (2018) presented a loss function based on Noise-Contrastive Learning and Class-A Loss, called Class-A Loss, which models the loss function that contrasts the loss function's weights with the embeddings of samples from such classes. It uses L2 normalization and temperature. Yang et al. (2018) presentedloss function based on Class-A Loss, called Class-A Loss, which models the loss function that contrasts the loss function's weights with the embeddings of samples from such classes.