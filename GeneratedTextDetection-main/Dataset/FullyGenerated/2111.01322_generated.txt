Abstract Meta-learning considers the problem of learn- ing an efficient learning process that can lever- age its past experience to accurately solve new tasks. However, the efficacy of meta-learning crucially depends on the distribution of tasks available for training, and this is often assumed to be known a priori or constructed from lim- ited supervised datasets. In this work, we aim to provide task distributions for meta-learning by considering self-supervised tasks automati- cally proposed from unlabeled text, to enable large-scale meta-learning in NLP. We design multiple distributions of self-supervised tasks by considering important aspects of task diver- sity, difficulty, type, domain, and curriculum, and investigate how they affect meta-learning performance. Our analysis shows that all these factors meaningfully alter the task distribu- tion, some inducing significant improvements in downstream few-shot accuracy of the meta- learned models. Empirically, results on 20 downstream tasks show significant improve- ments in few-shot learning Ã¢ÂÂ adding up to +4.2% absolute accuracy (on average) to the previous unsupervised meta-learning method, and perform comparably to supervised meth- ods on the FewRel 2.0 benchmark. Introduction Humans show a remarkable capability to accu- rately solve a wide range of problems efficiently Ã¢ÂÂ utilizing a limited amount of computation and experience. Deep learning models, by stark con- trast, can be trained to be highly accurate on a nar- row task while being highly inefficient in terms of the amount of compute and data required to reach that accuracy. Within natural language processing (NLP), recent breakthroughs in unsupervised pre- training have enabled reusable models that can be applied to many NLP tasks, however, learning of new tasks is still inefficient (Yogatama et al., 2019; Bansal et al., 2020a; Linzen, 2020). Meta-learning (Schmidhuber, 1987; Bengio et al., 1992; Thrun and Pratt, 2012) treats the learning process itself as a learning problem from data, with the goal of learning systems that can generalize to new tasks efficiently. This has the potential to produce few- shot learners that can accurately solve a wide range of new tasks. However, meta-learning requires a distribution over tasks with relevant labeled data that can be difficult to obtain, severely limiting the practical utility of meta-learning methods. In the supervised setting, in particular, meta- learning task distribution is often defined by sub- sampling from the classes in a classification prob- lem over a fixed dataset (Vinyals et al., 2016). This not only limits the applicability of meta-learning to the underlying classification problem, but also requires a diverse set of supervised datasets with a large number of classes to enable learning. Self- supervised meta-learning, on the other hand, seeks to propose tasks from unlabelled data (Hsu et al., 2019; Bansal et al., 2020b), and has great po- tential to enable numerous important applications (Hospedales et al., 2020) such as neural architec- ture search, continual learning, hyper-parameter optimization, learning in low-resource settings, etc. Existing work in meta-learning for NLP, how- ever, defaults to task distributions that tend to be overly simplistic, e.g. using existing supervised datasets (Han et al., 2018; Dou et al., 2019; Bansal et al., 2020a) or unsupervised cloze-style tasks with uniform selection of words from the vocabulary (Bansal et al., 2020b). Given the lack of explo- ration on this critical component, we propose to devise and evaluate various task distributions in the context of unsupervised meta-learning for NLP. Specifically, we explore a diverse set of ap- proaches to create task distributions that are in- ductive to better meta-training efficacy. We pro- vide empirical evidence that existing definitions of task distributions are prone to producing tasks that might not be challenging enough for the underlying model to learn useful representations, which in turn translates into poor downstream task performance. We therefore propose several new approaches that instead consider important features of the task dis- tribution including task diversity, difficulty, resem- blance to the downstream tasks, and the curriculum or the order in which tasks are presented during training. When evaluated on a suite of 20 NLP classification tasks, our best unsupervised meta- learning method leads to an absolute increase of up to +4.2% in average few-shot accuracyunsu- pervised baseline results; and it even outperforms supervised meta-learning methods on FewRel 2.0 benchmark (Gao et al., 2019) on 5-shot evaluation. The paper is organized as follows. We start by providing some relevant background (2) on meta- learning and the unsupervised task generation ap- proach in SMLMT. Next, we introduce (3) new approaches to improve the task distribution. We then analyze (4.2) the different unsupervised methods on a wide range of tasks including sentiment classification, entity typing, text classi- fication, sentence-pair classification and relation classification. Finally, we evaluate (4.3, 4.4) the different unsupervised methods on a wide range of tasks including entity typing, sentence-pair classification and relation classification. Related Work Meta-learning applications in NLP have yielded improvements on specific tasks (Gu et al., 2018; Bansal et al., 2019; Linzen, 2020). Unsupervised tasks have been explored in computer vi- sion (Hsu et al., 2019; Bansal et al., 2019; Khodadadeh et al., 2020b), reinforcement learning (Gupta et al., 2019), natural language processing (Bengio et al., 2019), networked learning (Bengio et al., 2019), natural language processing (Bengio et al., 2019), natural language processing for NLP (Hospedales et al., 2020a), and data-driven learning (Gupta et al., 2019). Unsupervised meta-learning has been explored in NLP optimization (Hospedales et al., 2020b), learning systems (Hospedales et al., 2020c), as well as meta-learning methods in NLP that can be applied to many NLP tasks, however, meta-learning methods tend to be overly simplistic, e.g. using existing supervised datasets (Han et al., 2018; Dou et al., 2019; Bansal et al., 2019) or unsupervised cloze-style tasks with uniform selection of words from the vocabulary (Bansal et al., 2019). Contemporary work (Murty et al., 2019) has been explored in NLP meta-learning for NLP, how- ever, is it possible to achieve significant improvements in downstream few-shot accuracy of the meta- learned models? In the supervised setting, in particular, meta- learning task distribution is often defined by sub- sampling from the classes in a classification prob- lem over a fixed dataset (Vinyals et al., 2016). This not only limits the applicability of meta-learning to the underlying classification problem, but also requires a diverse set of supervised datasets with a large number of classes to enable learning. Self- supervised meta-learning, on the other hand, seeks to propose tasks from unlabelled data (Hsu et al., 2019; Bansal et al., 2019), and has great po- tential to enable numerous important applications (Hospedales et al., 2020) such as neural architec- ture search, continual learning, hyper-parameter optimization, learning in low-resource settings, etc. Existing work in meta-learning for NLP, how- ever, defaults to task distributions that tend to be overly simplistic, e.g. using existing supervised datasets (Han et al., 2018; Dou et al., 2019) or unsupervised cloze-style tasks with uniform selection of words from the vocabulary (Bansal et al., 2019).