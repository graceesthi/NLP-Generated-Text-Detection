Abstract Large, pre-trained transformer-based language models such as BERT have drastically changed the Natural Language Processing (NLP) field. We present a survey of re- cent work that uses these large language mod- els to solve NLP tasks via pre-training then fine-tuning, prompting, or text generation ap- proaches. We also present approaches that use pre-trained language models to generate data for training augmentation or other purposes. We conclude with discussions on limitations and suggested directions for future research. Introduction In recent years, large pre-trained transformer-based language models (PLMs), such as the BERT (De- vlin et al., 2019) and GPT (Radford et al., 2018) families of models, have taken Natural Language Processing (NLP) by storm, achieving state-of-the- art performance on many tasks. These large PLMs have fueled a paradigm shift in NLP. Take a classification task p(y|x) (classi- fying textual input x into a label y) as an example: traditional statistical NLP approaches often design hand-crafted features to represent x, and then apply a machine learning model (e.g. SVM (Cortes and Vapnik, 1995), logistic regression) to learn the classification function. Deep learning models learn the latent feature representation via a deep neural network (LeCun et al., 2015) in ad- dition to the classification function. Note that the latent representation needs to be learned afresh for each new NLP task, and that, in many cases, the size of the training data limits the quality of the latent feature representation. Given that the nu- ances of language are common to all NLP tasks, one could posit that we could learn a generic la- tent feature representations from some generic task once, and then share it across all NLP tasks. Lan- guage modeling, where the model needs to learn how to predict the next word given previous words, is such a generic task with abundant naturally oc- curring text to pre-train such a model (hence the name pre-trained language models). In fact, the lat- est, ongoing paradigm shift begins when PLMs are introduced: for numerous NLP tasks, researchers now leverage existing PLMs via fine-tuning for the task of interest, prompting the PLMs to perform the desired task, or reformulating the task as a text gen- eration problem with application of PLMs to solve it accordingly. Advances in these three PLM-based paradigms have continuously established new state- of-the-art performances. This paper surveys recent works that leverage PLMs for NLP. We organize these works into the following three paradigms: Pre-train then fine-tune (ÃÂ§ 2): perform general- purpose pre-training with a large unlabeled corpus, and then perform a small amount of task-specific fine-tuning for the task of interest. Prompt-based learning (ÃÂ§ 3): prompt a PLM such that solving an NLP task is reduced to a task similar to the PLMÃ¢ÂÂs pre-training task (e.g. predicting a missing word), or a simpler proxy task (e.g. textual entailment). Prompt- ing can usually more effectively leverage the knowledge encoded in the PLMs, leading to few-shot approaches. NLP as text generation (ÃÂ§ 4): Reformulate NLP tasks as text generation, to fully leverage knowledge encoded in a generative language model such as GPT-2 (Radford et al., 2019) and T5 (Raffel et al., 2020). Generative PLMs can be also used for text gen- eration tasks. We refer readers to the excellent surveys on text generation such as Li et al. (2021b) and Yu et al. (2021b). This paper, unless otherwise specified, focuses on tasks that are not generative in nature (e.g. classification, sequence labeling and structure prediction) that still cover a broad range of NLP tasks including syntactic or semantic pars- ing of text, Information Extraction (IE), Question Answering (QA), Textual Entailment (TE), senti- ment analysis, and so on. In addition to the three paradigms, there is an- other, complementary method: to indirectly use any of the PLM paradigms above to improve results of target NLP tasks: Data generation (ÃÂ§ 5): run PLMs to automat- ically generate data for NLP tasks. The gen-data can be silver labeled data, where typically the generative PLM is fine-tuned for the task, or some auxiliary data, such as coun- terexamples, clarifications, contexts, or other. In the first case, the silver labeled data can be added to existing labeled data. In the second case, the auxiliary data supports the abl- ity of the first com- prehensive language mod- el. Data can also be added to existing un- labeled data. In any of the cases, the size of the data augmentation and the quality of the data augmentation are not fully established. Introduction In recent years, large pre-trained transformer-based language models (PLMs), such as the BERT (De- vlin et al., 2019) and GPT (Radford et al., 2018) families of models, have taken Natural Language Processing (NLP) by storm, achieving state-of-the- art performance on many tasks. These large PLMs have fueled a paradigm shift in NLP. Take a classification task p(y|x) (classi- fying textual input x into a label y) as an example: traditional statistical NLP approaches often design hand-crafted features to represent x, and then apply a machine learning model (e.g. SVM (Cortes and Vapnik, 1995), logistic regression) to learn the classification function. Deep learning models learn the latent feature representation via a deep neural network (LeCun et al., 2015) in ad- dition to the classification function.