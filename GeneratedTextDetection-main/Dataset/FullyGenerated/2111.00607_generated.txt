Abstract Large language models (e.g., Brown et al., 2020; Patwary et al., 2021) have shown impressive performance on many natural language processing (NLP) tasks in a zero-shot setting. We ask whether these models exhibit commonsense understanding ÃÂ¢ÃÂÃÂ a critical component of NLP applications ÃÂ¢ÃÂÃÂ by evaluating models against four commonsense benchmarks. We find that the impressive zeroshot performance of large language models is mostly due to existence of dataset bias in our benchmarks. We also show that the zeroshot performance is sensitive to the choice of hyper-parameters and similarity of the benchmark to the pre-training datasets. Moreover, we did not observe substantial improvements when evaluating models in a few-shot setting. Finally, in contrast to previous work, we find that leveraging explicit commonsense knowledge does not yield substantial improvement. Introduction Large language models (with more than 1 billion parameters) perform well on a range of natural language processing (NLP) tasks in zero- and few-shot settings, without requiring task-specific supervision (e.g., Radford et al., 2019; Brown et al., 2020; Patwary et al., 2021). This observation suggests that given enough capacity, language models may extract the knowledge required to perform well on these NLP tasks from raw text, simply using the transformer architecture (Vaswani et al., 2017) and auto-regressive language modeling objective. Consequently, various recent efforts have focused on pre-training evergrowing language models exceeding even 350 billion parameters (Brown et al., 2020; Patwary et al., 2021). Tasks that require commonsense understanding are of special interest in such zero- and few-shot evaluation paradigms because it is hard to collect supervised commonsense datasets given the vast, diverse, and growing nature of commonsense knowledge (Elazar et al., 2021). Despite the importance of commonsense understanding in NLP systems (McCarthy et al., 1960; Gunning, 2018; Liu and Singh, 2004), whether commonsense knowledge can be acquired by large language models and through the language modeling objective remains an open question. To answer this question, we evaluate pre-trained language models against multiple-choice questionanswering benchmarks, examining different types of commonsense knowledge, such as social, physical, and temporal (Bisk et al., 2020; Sakaguchi et al., 2020; Zellers et al., 2019b; Sap et al., 2019b). We focus on zero- or few-shot evaluation that enables us to examine the commonsense understanding capacity of pre-trained language models. We compare zero- and few-shot performance of the models with that of different baselines; in particular, we consider an Answer-only baseline where a model choose an answer based on the likelihood of the answer choices alone without considering the question. The high performance of the Answer-only baseline is a sign of dataset bias in a benchmark Ã¢ÂÂ questions are not required to solve an example. First, we evaluate our largest model (with 7 billion parameters) against commonsense benchmarks in a zero-shot way: we find that zero-shot performance is still far from the state-of-the-art fine-tuned results. In fact, zero-shot performance is always closer to that of the Answer-only baseline. We also observe a correlation between zero-shot performance and the similarity of a benchmark to the pre-training data (measured as the perplexity of the benchmark under a pre-trained model). Our results show that the zero-shot performance of pretrained language models on commonsense benchmarks is mostly attributed to the dataset bias in the benchmarks; it also highlights the need of reporting strong baselines in our evaluations which is missing from some of the recent work (Radford et al., 2019; Brown et al., 2020; Patwary et al., 2021). Next we investigate to what extent hyperparameters (such as the prompt format) and increasing model size impact the zero-shot performance. In all commonsense benchmarks, we see a gap between the zero-shot performance of the worst and best hyper-parameter settings (ranging between 2 to 19 accuracy points). This result shows that for commonsense understanding tasks, it is best not to take pre-trained language models as off-the-shelf tools. When increasing model size, across benchmarks, we observe improvements on both zero-shot performance and the Answer-only baseline, suggesting that larger models are better in exploiting the surface cues. Finally, we examine if pre-trained language models benefit from adding more examples in a fewshot setting or leveraging knowledge extracted from existing commonsense knowledge bases. We do not observe a substantialfrom the few-shot evaluation compared to the zero-shot one. Adding commonsense knowledge also does not yield notable improvements, showing that our language models cannot leverage the relevant knowledge when it is simply added to the prompt. It is an exciting time for language research in both academia and industry, with various recent efforts working on pre-training evergrowing language models exceeding even 350 billion parameters (Brown et al., 2020; Patwary et al., 2021). Tasks that require commonsense understanding are of special interest in such zero- and few-shot evaluation paradigms because it is hard to collect supervised commonsense datasets given the vast, diverse, and growing nature of commonsense knowledge (Elazar et al., 2021). Despite the importance of commonsense understanding in NLP systems (McCarthy et al., 1960; Gunning, 2018; Liu and Singh, 2004), whether commonsense knowledge can be acquired by large language models and through the language modeling objective remains an open question. To answer this question, we evaluate pre-trained language models against multiple-choice questionanswering benchmarks, examining different types of commonsense knowledge, such as social, physical, and temporal (Bisk et al., 2020; Sakaguchi et al., 2020; Zellers et al., 2019b; Sap et al., 2019b). We focus on zero- or few-shot evaluation that enables us to examine the commonsense understanding capacity of pre-trained language models. We compare zero- and few-shot performance of the models with that of different baselines; in particular, we consider an Answer-only baseline where a model choose an answer based on the likelihood of the answer choices alone without considering the question. The high performance of the Answer-only baseline is a sign of dataset bias in a benchmark Ã¢ÂÂ questions are not required to solve an example. First, we evaluate our largest model (with 7 billion parameters) against commonsense benchmarks in a zero-shot way: we find that zero-shot performance is still far from the state-of-the-art fine-tuned results. In fact, zero-shot performance is always closer to that of the Answer-only baseline. 