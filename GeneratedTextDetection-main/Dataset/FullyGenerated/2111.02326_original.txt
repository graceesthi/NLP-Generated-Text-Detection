Abstract Sentiment analysis is often a crowdsourcing task prone to subjective labels given by many annotators. It is not yet fully understood how the annotation bias of each annotator can be modeled correctly with state-of-the-art meth- ods. However, resolving annotator bias pre- cisely and reliably is the key to understand annotators’ labeling behavior and to success- fully resolve corresponding individual miscon- ceptions and wrongdoings regarding the anno- tation task. Our contribution is an explana- tion and improvement for precise neural end- to-end bias modeling and ground truth esti- mation, which reduces an undesired mismatch in that regard of the existing state-of-the-art. Classification experiments show that it has po- tential to improve accuracy in cases where each sample is annotated only by one single annotator. We provide the whole source code publicly1 and release an own domain-specific sentiment dataset containing 10,000 sentences discussing organic food products . These are crawled from social media and are singly la- beled by 10 non-expert annotators. Introduction Modeling annotator bias in conditions where each data point is annotated by multiple annotators, be- low referred to as multi-labeled crowdsourcing, has been investigated thoroughly. However, bias mod- eling when every data point is annotated by only one person, hereafter called singly labeled crowd- sourcing, poses a rather specific and difficult chal- lenge. It is in particular relevant for sentiment anal- ysis, where singly labeled crowdsourced datasets are prevalent. This is due to data from the social web which is annotated by the data creators them- selves, e.g., rating reviewers or categorizing image uploaders. This might further include multi-media contents such as audio, video, images, and other forms of texts. While the outlook for such forms of data is promising, end-to-end approaches have not yet been fully explored on these types of crowd- sourcing applications. With these benefits in mind, we propose a neural network model tailored for such data with singly labeled crowdsourced annotations. It computes a latent truth for each sample and the correct bias of every annotator while also considering input feature distribution during training. We modify the loss function such that the annotator bias con- verges towards the actual confusion matrix of the regarding annotator and thus models the annotator biases correctly. This is novel, as previous meth- ods either require a multi-labeled crowdsourcing setting (Dawid and Skene, 1979; Hovy et al., 2013) or do not produce a correct annotator bias during training which would equal the confusion matrix, see Zeng et al. (2018, figure 5) and Rodrigues and Pereira (2018, figure 3). A correct annotator- or annotator-group bias, however, is necessary to de- rive correct conclusions about the respective an- notator behavior. This is especially important for highly unreliable annotators who label a high num- ber of samples randomly – a setting, in which our proposed approach maintains its correctness, too. Our contributions are as follows. We describe the corresponding state-of-the-art for crowdsourc- ing algorithms and tasks in section 2. Our neural network model method for end-to-end crowdsourc- ing modeling is explained in section 3, which in- cludes a mathematical explanation that our linear bias modeling approach yields the actual confusion matrices. The experiments in section 4 underline our proof, show that the model handles annotator bias correctly as opposed to previous models, and demonstrate how the approach impacts classification. Related Work 2.1 Crowdsourcing Algorithms Problemdefinition. The need of data in the Growing research areas of machine learning has given rise to the generalized use of crowdsourcing. This method of data collection increases the amount of data, saves time and money but comes at the poten- tial cost of data quality. One of the key metrics of data quality is annotator reliability, which can be affected by various factors. For instance, the lack of rater accountability can entail spamming. Spam- mers are annotators that assign labels randomly and significantly reduce the quality of the data. Raykar and Yu (2012) and Hovy et al. (2013) addressed this issue by detecting spammers based on rater trustworthiness and the SpEM algorithm. How- ever, spammers are not the only source of label inconsistencies. The varied personal backgrounds of crowd workers often lead to annotator biases that affect the overall accuracy of the models. Sev- eral works have previously ranked crowd workers (Hovy et al., 2013; Whitehill et al., 2009; Yan et al., 2010), clustered annotators (Peldszus and Stede, 2013), captured sources of bias (Wauthier and Jor- dan, 2011) or modeled the varying difficulty of the annotation tasks (Carpenter, 2008; Whitehill et al., 2009; Welinder et al., 2010) allowing for the elimi- nation of unreliable labels and the improvement of the model predictions. Ground truth estimation. One common chal- lenge in crowdsourced datasets is the ground truth estimation. When an instance has been annotated multiple times, a simple yet effective technique is to implement majority voting or an extension thereof (TIAN and Zhu, 2015; Yan et al., 2010). More sophisticated methods focus on modeling la- bel uncertainty (Spiegelhalter and Stovin, 1983) or implementing bias correction (Snow et al., 2008; Camilleri and Williams, 2020). These techniques are commonly used for NLP applications or com- puter vision tasks (Smyth et al., 1995; Camilleri and Williams, 2020). Most of these methods for inferring the ground truth labels use variations of the EM algorithm by Dawid and Skene (1979), which estimates annotator biases and latent labels in turns. We use its recent extension called the Fast Dawid-Skene algorithm (Sinha et al., 2018). End-to-end approaches. The Dawid-Skene al- gorithm models the raters’ abilities as respective bias matrices. Similar examples include GLAD (Whitehill et al., 2009) or MACE (Hovy et al.,  2013), which infer true labels as well as labeler expertise and sample difficulty. These approaches infer the ground truth only from the labels and do not consider the input features. End-to-end ap- proaches learn a latent truth, annotator information, and feature distribution jointly during actual model training (Zeng et al., 2018; Khetan et al., 2017; Rodrigues and Pereira, 2018). Some works use the EM algorithm (Raykar et al., 2009), e.g., to learn sample difficulties, annotator representations and ground truth estimates (Platanios et al., 2020). However, the EM algorithm has drawbacks, namely that it can be unstable and more expensive to train (Chu et al., 2020). LTNet models imperfect anno- tations derived from various image datasets using a single latent truth neural network and dataset- specific bias matrices (Zeng et al., 2018). A similar approach is used for crowdsourcing, representing annotator bias by confusion matrix estimates (Ro- drigues and Pereira, 2018). Both approaches show a mismatch between the bias and how it is modeled, see Zeng et al. (2018, figure 5) and Rodrigues and Pereira (2018, figure 3). We adapt the LTNet archi- tecture (see section 3), as it can be used to model crowd annotators on singly labeled sentiment anal- ysis, which, to our knowledge, is not done yet in the context of annotator bias modeling. Recent works about noisy labeling in sentiment analysis do not consider annotator bias (Wang et al., 2019). 2.2 Crowdsourced Sentiment Datasets SentimentandEmotion. Manyworksusetheterms sentiment and emotion interchangeably (Demszky et al., 2020; Kossaifi et al., 2021), whereas senti- ment is directed towards an entity (Munezero et al., 2014) but emotion not necessarily. Both can be mapped to valence, which is the affective quality of goodness (high) or badness (low). Since emo- tion recognition often lacks annotated data, crowd- sourced sentiment annotations can be beneficial (Snow et al., 2008). Multi-LabeledCrowdsourcedDatasets. Crowd- sourced datasets, such as, Google GoEmotion (Demszky et al., 2020) and the SEWA database (Kossaifi et al., 2021), usually contain multiple la- bels per sample and require their aggregation using ground truth estimation. Multi-labeled datasets are preferable to singly labeled ones on limited data. Snow et al. (2008) proved that many non-expert annotators give a better performance than a few expert annotators and are cheaper in comparison. Singly Labeled Crowdsourced Datasets. Singly labeled datasets are an option given a fixed budget and unlimited data. Khetan et al. (2017) showed that it is possible to model worker quality with single labels even when the annotations are made by non-experts. Thus, multiple annotations can not only be redundant but come at the expense of fewer labeled samples. For singly labeled data, it can be distinguished between reviewer annotators and external annotators. Reviewer annotators rate samples they created themselves. It is common in forums for product and opinion reviews where a review is accompanied by a rating. As an example of this, we utilized the TripAdvisor dataset (Thel- wall, 2018). Further candidates are the Amazon review dataset (Ni et al., 2019), the Large Movie Review Dataset (Maas et al., 2011), and many more comprising sentiment. External annotators anno- tate samples they have not created. Experts are needed for complex annotation tasks requiring do- main knowledge. These are not crowdsourced, since the number of annotators is small and fixed. More common are external non-experts. Snow et al. (2008) showed that multi-labeled datasets anno- tated by non-expert improve performance. Khetan et al. (2017) showed that it also performs well in the singly labeled case. Thus, datasets made of singly labeled non-expert annotations can be cheaper, faster, and obtain performances compara- ble to those comprised of different types of annota- tions. Our organic dataset is annotated accordingly, see section 4.3. Conclusion We showed the efficacy of LTNet for modeling crowdsourced data and the inherent bias accurately and robustly. The bias matrices produced by our modified LTNet improve such that they are more similar to the actual bias between the latent truth and ground truth. Moreover, the produced bias shows high robustness under very noisy condi- tions making the approach potentially usable out- side of lab conditions. The latent truth, which is a hidden layer below all annotator biases, can be used for ground truth estimation in our sin- gle label crowdsourcing scenario, providing al- most identical ground truth estimates as pseudo labeling. Classification on three crowdsourced datasets show that LTNet approaches outperfom naive approaches not considering each annotator separately. The proposed log removal from the loss function showed better results on singly labeled crowdsourced datasets, but this observation needs further experiments to be substantiated. Further- more, there might be many use cases to explore the approach on other tasks than sentiment analysis.