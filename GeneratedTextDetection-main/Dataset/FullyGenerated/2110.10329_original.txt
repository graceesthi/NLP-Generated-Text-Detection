ABSTRACT Unsupervised pre-training is now the predominant approach for both text and speech understanding. Self-attention models pre-trained on large amounts of unannotated data have been hugely successful when fine-tuned on downstream tasks from a variety of domains and languages. This paper takes the universality of unsupervised language pre-training one step further, by unifying speech and text pre-training within a single model. We build a single encoder with the BERT objective on unlabeled text together with the w2v-BERT objective on unlabeled speech. To further align our model representations across modalities, we leverage alignment losses, specifically Translation Language Modeling (TLM) and Speech Text Matching (STM) that make use of supervised speech-text recognition data. We demonstrate that incorporating both speech and text data during pre-training can significantly improve downstream quality on CoVoST 2 speech translation, by around 1 BLEU compared to single-modality pre-trained models, while retaining close to SotA performance on LibriSpeech and SpeechStew ASR tasks. On four GLUE tasks and text-normalization, we observe evidence of capacity limitations and interference between the two modalities, leading to degraded performance compared to an equivalent text-only model, while still being competitive with BERT. Through extensive empirical analysis we also demonstrate the importance of the choice of objective function for speech pre-training, and the beneficial effect of adding additional supervised signals on the quality of the learned representations. INTRODUCTION Self-supervised learning of text and speech representations has been particularly impactful in natural language processing and speech processing. Since GPT (Radford et al., 2018), BERT (Devlin et al., 2019) and their variations (Yang et al., 2019; Conneau & Lample, 2019; Lewis et al., 2020; Raffel et al., 2020; Joshi et al., 2020), performance on natural language understanding downstream tasks (Socher et al., 2013; Rajpurkar et al., 2016; Agirre et al., 2007; Williams et al., 2018) and monolingual (e.g., GLUE (Wang et al., 2019b), SuperGLUE (Wang et al., 2019a)) and multilingual (e.g., XTREME (Hu et al., 2020), XTREME-R (Ruder et al., 2021)) benchmarks has largely improved thanks to evolving pre-trained models, which leverage increasing amounts of unannotated data (Radford et al., 2019; Liu et al., 2019; Conneau et al., 2019; Wenzek et al., 2020; Xue et al., 2021b) and increased model capacity (Brown et al., 2020; Xue et al., 2021b; Lepikhin et al., 2020; Fedus et al., 2021). Similarly for speech, unsupervised pre-training has emerged as a predominant approach. Wav2vec 2.0 (Baevski et al., 2020b) and newer variants (Zhang et al., 2020) initially showed the strength of pre-training on speech recognition (Panayotov et al., 2015; Kahn et al., 2020; Zhang et al., 2021) on multiple domains (Hsu et al., 2021) and languages (Conneau et al., 2020). Self-supervised learning methods in language understanding are designed to be used universally, i.e. a single large pre-trained model for all domains and languages. One big advantage of these universal models is the ability to leverage data skew across domains, tasks and languages; the availability of task or domain-specific data in one language can boost model performance for several languages that the model was pre-trained on. Extending this generalization capability across modalities by having neural networks understand both text and speech at the same time is a natural next step. Jointly pre-training models on speech and text is a natural choice for multimodal self-supervised learning, given the similarities between the two modalities and the abundance of unannotated text data compared to speech. Recent work has also shown that self-supervised speech representations can be aligned to text with little to no supervision (Baevski et al., 2021), suggesting the possibility of learning both modalities within a single neural network. However, past work in multilingual modeling in particular has demonstrated the difficulty of learning representations of different data structures, however similar, within a shared network, exposing the so-called transfer interference problem (Arivazhagan et al., 2019). We show in this work that this trade-off also applies to joint speech-text self-supervised learning. We study a new multimodal speech-text pre-training approach that leverages data from one modality to improve representations of the other, but also suffers from transfer interference and capacity dilution. Our Speech and LAnguage Model (SLAM) consists of a single Conformer (Gulati et al., 2020) trained with the SpanBERT objective for text (Joshi et al., 2020) and the w2v-BERT (Chung et al., 2021) objective for speech. We show that a model using only self-supervised objectives leads to good performance on both modalities, but is outperformed by mono-modal pre-trained models, suffering from significant transfer interference. To reduce the gap, we leverage supervised alignment losses, specifically a translation language model (Conneau & Lample, 2019; Zheng et al., 2021) and speech-text matching (Li et al., 2021) loss. We train our model in a multi-task fashion with the self-supervised and alignment losses. This leads to performance competitive with the state-of-the-art on SpeechStew and LibriSpeech ASR and on CoVoST 2 speech translation tasks. On speech translation, we demonstrate further quality improvements by continuing pre-training on speech-only, outperforming previous approaches by 1 BLEU on average. On text tasks, our joint model loses quality compared to equivalent mono-modal pre-trained models, but remains competitive with initial BERT results (Devlin et al., 2019), demonstrating the capacity limitations with modeling two high-resource modalities simultaneously. To the best of our knowledge, our work is the first to study and underline the benefits and limitations of speech-text unsupervised pre-training over mono-modal models, on various speech and text downstream tasks. Our initial results set a new challenge in multimodal self-supervised language understanding. DISCUSSION In this work, we demonstrate that a single encoder model can be pre-trained to learn strong contextualized representations of speech and text simultaneously. We combine self-supervised learning objectives for text (BERT) and self-supervised approaches for speech (w2v-BERT) to learn a joint Speech and LAnguage Model (SLAM). Downstream evaluations on speech and language understanding tasks, including LibriSpeech and SpeechStew ASR, CoVoST 2 speech translation, four GLUE tasks, and text-normalization uncover significant interference challenges when pre-training simultaneously on high-resource modalities. Using alignment losses such as translation language modeling and speech-text matching which leverage speech-text supervised aligned data, we show that we can improve the cross-modal representation alignment and improve over mono-modal models on the speech translation tasks, while maintaining state-of-the-art performance on speech recognition. We hope that this work would motivate further research on extending the universality of self-supervised learning of language representations to the multimodal speech-text setting.