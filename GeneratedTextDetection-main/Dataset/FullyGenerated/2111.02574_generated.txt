Abstract Robust state tracking for task-oriented dialogue systems currently remains restricted to a few popular languages. This paper shows that given a large-scale dialogue data set in one language, we can automatically produce an effective semantic parser for other languages using machine translation. We propose automatic translation of dialogue datasets with alignment to ensure faithful translation of slot values and eliminate costly human supervision used in previous benchmarks. We also propose a new contextual semantic parsing model, which encodes the formal slots and values, and only the last agent and user utterances. We show that the succinct representation reduces the compounding effect of translation errors, without harming the accuracy in practice. We evaluate our approach on several dialogue state tracking benchmarks. On RiSAWOZ, CrossWOZ, CrossWOZ-EN, and MultiWOZZH datasets we improve the state of the art by 11%, 17%, 20%, and 0.3% in joint goal accuracy. We present a comprehensive error analysis for all three datasets showing erroneous annotations can obscure judgments on the quality of the model. Finally, we present RiSAWOZ English and German datasets, created using our translation methodology. On these datasets, accuracy is within 11% of the original showing that high-accuracy multilingual dialogue datasets are possible without relying on expensive human annotations. Introduction Tremendous effort has gone into the research and development of task-oriented dialogue agents for English and a few other major languages in recent years. A methodology that can transfer the effort to other languages automatically will greatly benefit the large population of speakers of the many other languages in the world. Underlying an effective TOD agent is dialogue state tracking, the task of predicting a formal representation of the conversation sufficient for the dialogue agent to reply, in the form of slots and values. However, DST currently remains restricted to a few popular languages (Razumovskaia et al., 2021). Traditional DST agents require large handannotated Wizard-of-Oz (Kelley, 1984) datasets for training, which are prohibitively labor-intensive to produce in most languages (Gunasekara et al., 2020). Large, multi-domain WOZ datasets are only available in English and Chinese (Quan et al., 2020; Ye et al., 2021a). The contributions of this paper are as follows: 1. We propose an automatic technique to build high-quality multilingual data sets using machine translation. Machine translation has been shown effective for localizing questionanswering agents (Moradshahi et al., 2020). It shows that for open ontology datasets, we need to use an alignment model to properly translate entities in the source language to entities in the target language. This paper shows that alignment is necessary even for closed ontology datasets and dialogues. Furthermore, we improve alignment to address these challenging issues we discovered unique to dialogues: (1) Translation errors accumulate and can prevent a correct parse for the rest of the dialogue; (2) There are logical dependencies between slot values across different turns; (3) Utterances are generally longer and more complex carrying multiple entities. We found that alignment improves the accuracy on the RiSAWOZ benchmark by 45.6%. This technique eliminates the cost of human postediting used on all previous translation benchmarks, and can improve machine translation quality on other tasks too. 2. We created the first automatically obtained, high-quality, translated dialogue data sets: RiSAWOZ-EN-auto (English) and RiSAWOZ-DE-auto (German) datasets. They show only an 8% drop in accuracy compared to the original. RiSAWOZ-EN-auto and RiSAWOZ-DEauto will be released upon publication. 3. We show that the accumulation of translation and annotation errors across turns can be mitigated with a Contextual Semantic Parsing (CSP) model for state tracking. We propose a BART-CSP model, a seq-to-seq based on BART, that encodes the belief state, and the last agent and user utterances, rather than the full history of utterances. BART-CSP improves SOTA on RiSAWOZ (Quan et al., 2020) and CrossWOZ (Zhu et al., 2020), two large-scale multi-domain WoZ dialogue datasets, by 10.7% and 17% in Joint Goal Accuracy(JGA). Notably, BART-CSP is more effective on translated data as evident by bigger performance improvement: onand RiSAWOZ-DE-auto datasets, automatically translated versions of RiSAWOZ, BART-CSP improves SOTA by 32.4% and 52.5%. Related Work 2.1 Dialogue State Tracking Dialogue state tracking (DST) refers to the task of predicting a formal state of a dialogue at its current turn, as a set of slot-value pairs at every turn. State-of-the-art approaches apply large transformer networks (Peng et al., 2020; Hosseini-Asl et al., 2020) to encode the full dialogue history in order to predict slot values. Other approaches include question-answering models (Gao et al., 2019), ontology matching in the finite case (Lee et al., 2019), or pointer-generator networks (Wu et al., 2019). Both zero-shot cross-lingual DST transfer (Ponti et al., 2018; Chen et al., 2018) and multilingual knowledge distillation (Hinton et al., 2015; Tan et al., 2019) have been investigated; however, training with translated data is the dominant approach, outperforming zero-shot and few-shot methods. 2.2 Contextual Semantic Parsing Alternatively to encoding the full dialogue history, previous work has proposed including the state as context (Lei et al., 2018; Heck et al., 2020; Ye et al., 2021b) together with the last agent and user utterances. Recently, Cheng et al. (2020b) proposed replacing the agent utterance with a formal representation as well. Existing models rely on custom encoder architectures and loss functions for the state (Heck et al., 2020). Our formulation of contextual semantic parsing (Cheng et al., 2020b) is different since we encode the formal dialogue state directly as text, which simplifies the architecture and makes better use of the pretrained modelÃ¢ÂÂs understanding of natural text. Previous work also applied rule-based state trackers that compute the state based on the agent and user dialogue acts (Schulz et al., 2017; Zhong et al., 2018; Zhu et al., 2019). Such techniques cannot handle state changes outside of a state machine defined ahead of time and do not achieve state-of-the-art accuracy on the RiSAWOZ benchmark. 2.3 Multilingual Dialogues Several multilingual dialogue benchmarks have been created over the past few years. Dialogue State Tracking Challenge (DSTC) has released several datasets (Kim et al., 2016; Hori et al., 2019; Gunasekara et al., 2020), covering only a few domains and languages. Cross-lingual DST Distillation (Ponti et al., 2018; Heck et al., 2020) and MultiWOZ (Hinton et al., 2015; Tan et al., 2019) have been named for their accuracy; however, no other approach is named for one language. 3. Domain-Level Translation (Gao et al., 2019) and MultiWOZ (Hinton et al., 2015; Tan et al., 2019) Tropes and DSTs are automatically assigned to slot values using machine translation. Domain-level translation has been shown effective for localizing questionanswering agents (Moradshahi et al., 2020). It shows that for open ontology datasets, we need to use an alignment model to properly translate entities in the source language to entities in the target language. This paper shows that alignment is necessary even for closed ontology datasets and dialogues. Furthermore, we improve alignment to address these challenging issues we discovered unique to dialogues: (1) Translation errors accumulate and can prevent a correct parse for the rest of the dialogue; (2) There are logical dependencies between slot values across different turns; (3) Utterances are generally longer and more complex carrying multiple entities. We found that alignment improves the accuracy on the RiSAWOZ benchmark by 45.6%. This technique eliminates the cost of human postediting used on all previous translation benchmarks, and can improve machine translation quality on other tasks too.