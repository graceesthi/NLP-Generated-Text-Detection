ABSTRACT Estimating the quality of machine translation systems has been an ongoing challenge for researchers in this field. Many previous attempts at using round-trip translation as a measure of quality have failed, and there is much disagreement as to whether it can be a viable method of quality estimation. In this paper, we revisit round-trip translation, proposing a system which aims to solve the previous pitfalls found with the approach. Our method makes use of recent advances in language representation learning to more accurately gauge the similarity between the original and round-trip translated sentences. Experiments show that while our approach does not reach the performance of current state of the art methods, it may still be an effective approach for some language pairs. Keywords Quality Estimation ÃÂ· Machine Translation ÃÂ·Introduction The purpose of a quality estimation (QE) system is to determine the quality of a translation without using a reference translation. Traditionally, translation evaluation metrics like BLEU, NIST, METEOR, and TER would be used to assess a machine translation (MT) systemÃ¢ÂÂs translation by comparing the translation output to a reference translation created by an expert. However, quality estimation systems eliminate the need for the time-consuming task of manually annotating sentence translations to create this reference. QE Ã¢ÂÂis concerned about predicting the quality of a systemÃ¢ÂÂs output for a given input, without any information about the expected outputÃ¢ÂÂ [Specia et al., 2010]. Quality can be measured in a number of different ways, but in this paper, we will explore the use of round-trip translation in conjunction with sentence encoding models as a means of measuring this. Our approach is to translate an existing translated sentence back into its original language, generate sentence embedding vectors for both the original and round-trip translated sentences, and measure the similarity between the two vectors as a proxy for translation quality. Our justification behind this approach is that if we use an accurate model to translate the sentence back into the original language, the meaning of the sentence will be maintained. Then when we measure the similarity between the original sentence and the round-trip translated sentence, this score will correlate to the quality of the translation model that created the sentence pairs in the first place. Similar to the approach used in Somers [2005], evaluation metrics, such as BLEU, could then be used to evaluate the similarity of these two same language sentences. Nevertheless, we hypothesise that our approach of using sentence embeddings will prove far more useful than these as it can take the semantics of the sentence into consideration along with judging the sentence as a whole rather than word by word. We also think our approach will be an effective way to get around the poor prediction encountered by Somers [2005]. While alternate methods using more complex models have been used in recent years, we hope that the simplicity of our proposition and the novelty of using sentence encoding will bring new insights to this approach and will re-encourage further exploratory research in this area. The remainder of the paper is organised as follows: In Section 2 we look at existing literature, in Section 3 we present our methods, Section 4 details the data used, Section 5 shows and discusses the results and finally, Section 6 concludes our research and proposes future directions for building on our research. Related Work From researching previous works, we saw from QE systems like QuEst [Specia et al., 2013] and QuEst++ [Specia et al., 2015] that machine learning algorithms, like support vector machines and decision trees, have proved quite successful as quality estimation tools in situations where a lot of work on feature extraction was already done. While techniques like this are no longer state-of-the-art, they paved a useful stepping stone for new approaches. Neural-based QE systems, such as POSTECH [Kim et al., 2017], which require no feature extraction, have become more popular in research in recent years. One downfall of these methods is that they require a lot more data and take far longer to train. This need for large amounts of parallel data can cause problems which is where systems like TransQuest [Ranasinghe et al., 2020] come in. This model bridges the data gap by leveraging pre-trained cross-lingual transformers that are then fine-tuned to the quality estimation task using the smaller amount of training data available. Early attempts at using round-trip translation (RTT) as a means of quality estimation proved unsuccessful, resulting in researchers labeling it as Ã¢ÂÂgood for nothing [Somers, 2005]. Its utility as a viable way to estimate the quality of a machine translation system has been pulled into question by problems such as those highlighted in OÃ¢ÂÂConnell [2001] which assert that round-trip translation relies heavily on a modeltranslating the translated sentence back to its source language. Evaluation metrics cannot tell if translation errors occurred during the first translation or during the translation back to the original language. Moreover, if an error does occur during the first translation, it may cause even more problems when it is translated back. Some researchers have attempted to address this problem by using multiple machine translation models to translate the sentence back into the original language, generating sentence embedding vectors for both the original and round-trip translated sentences, and comparing the two vectors to create this reference. However, some researchers have attempted to address the previous pitfalls found with the approach and have attempted to reconcile the two vectors using a system of vector machines. One downfall of this approach is that it requires a lot more data and takes far longer to train. Many previous attempts at using round-trip translation as a measure of quality have failed, and there is much disagreement as to whether it can be a viable method of quality estimation. In this paper, we revisit round-trip translation, proposing a system which aims to solve the previous pitfalls found with the approach. Our method makes use of recent advances in language representation learning to more accurately gauge the similarity between the original and round-trip translated sentences. Experiments show that while our approach does not reach the performance of current state of the art methods, it may still be an effective approach for some language pairs. Keywords Quality Estimation ÃÂ· Machine Translation ÃÂ·Introduction The purpose of a quality estimation (QE) system is to determine the quality of a translation without using a reference translation. Traditionally, translation evaluation metrics like BLEU, NIST, METEOR, and TER would be used to assess a machine translation (MT) systemÃ¢ÂÂs translation by comparing the translation output to a reference translation created by an expert. However, quality estimation systems eliminate the need for the time-consuming task of manually annotating sentence translations to create this reference. QE Ã¢ÂÂis concerned about predicting the quality of a systemÃ¢ÂÂs output for a given input, without any information about the expected outputÃ¢ÂÂ [Specia et al., 2010]. Quality can be measured in a number of different ways, but in this paper, we will explore the use of round-trip translation in conjunction with sentence encoding models as a means of measuring this. 