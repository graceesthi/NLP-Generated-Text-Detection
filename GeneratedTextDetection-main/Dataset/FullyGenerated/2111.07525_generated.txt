Abstract English research articles (RAs) are an essential genre in academia, so the attempts to employ NLP to assist the development of academic writing ability have received considerable attention in the last two decades. However, there has been no study employing feature engineering techniques to investigate the linguistic features of RAs of different academic impacts (i.e., the papers of high/moderate citation times published in the journals of high/moderate impact factors). This study attempts to extract micro-level linguistic features in high- and moderate-impact journal RAs, using feature engineering methods. We extracted 25 highly relevant features from the Corpus of English Journal Articles through feature selection methods. All papers in the corpus deal with COVID-19 medical empirical studies. The selected features were then validated of the classification performance in terms of consistency and accuracy through supervised machine learning methods. Results showed that 24 linguistic features such as the overlapping of content words between adjacent sentences, the use of third- person pronouns, auxiliary verbs, tense, emotional words provide consistent and accurate predictions for journal articles with different academic impacts. Lastly, the random forest model is shown to be the best model to fit the relationship between these 24 features and journal articles with high and moderate impacts. These findings can be used to inform academic writing courses and lay the foundation for developing automatic evaluation systems for L2 graduate students. Introduction Academic writing has been considered one of the most difficult and challenging parts for postgraduates with English as their second language. To improve the English learnersÃ¢ÂÂ writing ability, many corpus studies compare L2 learnersÃ¢ÂÂ writing with native expertsÃ¢ÂÂ writing in terms of specific linguistic features and point out the progress that L2 learners should make (e.g., Lorenz, 1999; Hyland, 2008; Qin, 2014). Native expertsÃ¢ÂÂ writing in the above studies usually includes high-impact English journal articles (i.e., those with high citations and published in the journals with high-impact factors) because they can be one of the indicators of high-quality academic products and thus become a good reference for assessing studentsÃ¢ÂÂ academic writing. However, native expertsÃ¢ÂÂ publication has been proofread concerning linguistic use, while studentsÃ¢ÂÂ academic writings have more or less grammatical problems or spelling errors. Directly comparing such two types of writing is inevitably involved in those issues, but many studies do not mention such issues when introducing student corpora (e.g., Qin, 2014; Lu, 2012; Lu & Deng, 2019), which might cause the neglect of some word analysis. Nevertheless, comparison of publications with different impacts in academic fields can overcome such a problem, after all, both have been reviewed and proofread by experts. Grammatical problems might not be encountered to a bigger extent. At the same time, the most prominent weighted linguistic features in high-impact journal articles by comparison with those of moderate-impact can also provide a reference for the assessemtn of studentsÃ¢ÂÂ writings. Hence, it could be a new insight for L2 academic writing studies by comparing journal articles with high- and moderate-impacts. As NLP technology has an increasingly close association with English writing classrooms, an increasing number of studies on linguistic use in English research articles introduce machine learning models for analysis. Mover, developed by Anthony and Lashkia (2003), maybe the earliest attempt in automatic analysis of English research article writing. It focuses on the move/step in the abstract sections of academic papers based on the Bag- of-Words model. MAZEA (Multi-label Argumentative Zoning for English Abstracts, Dayrell et al., 2012), also automatically analyzes the move/step of English academic papersÃ¢ÂÂ abstracts. The highest accuracy rate of the software is 69%. Based on SVM (support vector machine), IADE (Intelligent Academic Discourse Evaluator), and Research Writing Tutor (RWT) by the Cotos team of Iowa State University in the United States (Pendar & Cotos, 2008; Babu, 2013; Cotos, 2014) vectorize the move/step in the Bag-of-Words Model. However, current automatic analysis systems on research articles focus more attention on the move/step at the macro-level and neglect the feature extraction at the micro-level of research articles. The importance of micro-level linguistic features have been proved by a series of corpus-based studies of second language acquisition (such as Biber et al., 2011; Biber & Gray, 2016; Chiu et al., 2017; Boutron & Ravaud, 2018; Lei & Yang, 2020; Politzer- Ahles et al., 2020). Liu (2016) and Wang and Liualso proved the importance of linguistic features at micro-levels in research articles. In their automatic classifier of abstracts in applied linguistics journals, classification accuracy (information abstract and descriptive abstract) has been significantly increased to 78.19% by introducing micro-level language indicators (such as sentence length, predicates, and connectives). The result is much higher than the accuracy rate of automatic evaluation systems by comparison (such as the Random Access Memories feature in the above studies) and has been shown to be the best model to fit the relationship between these 24 features and journal articles with high and moderate impacts. These findings can be used to inform academic writing courses and lay the foundation for developing automatic evaluation systems for L2 graduate students. Introduction Academic writing has been considered one of the most difficult and challenging parts for postgraduates with English as their second language. To improve the English learnersÃ¢ÂÂ writing ability, many corpus studies compare L2 learnersÃ¢ÂÂ writing with native expertsÃ¢ÂÂ writing in terms of specific linguistic features and point out the progress that L2 learners should make (e.g., Lorenz, 1999; Hyland, 2008; Qin, 2014). Native expertsÃ¢ÂÂ writing in the above studies usually includes high-impact English journal articles (i.e., those with high citations and published in the journals with high-impact factors) because they can be one of the indicators of high-quality academic products and thus become a good reference for assessing studentsÃ¢ÂÂ academic writing. However, native expertsÃ¢ÂÂ publication has been proofread concerning linguistic use, while studentsÃ¢ÂÂ academic writings have more or less grammatical problems or spelling errors. Directly comparing such two types of writing is inevitably involved in those issues, but many studies do not mention such issues when introducing student corpora (e.g., Qin, 2014; Lu, 2012; Lu & Deng, 2019), which might cause the neglect of some word analysis. Nevertheless, comparison of publications with different impacts in academic fields can overcome such a problem, after all, both have been reviewed and proofread by experts. Grammatical problems might not be encountered to a bigger extent.