Abstract Code-switching (CS), a ubiquitous phe- nomenon due to the ease of communication it offers in multilingual communities still remains an understudied problem in language processing. The primary reasons behind this are: (1) minimal efforts in leveraging large pretrained multilingual models, and (2) the lack of annotated data. The distinguishing case of low performance of multilingual models in CS is the intra-sentence mixing of languages leading to switch points. We first benchmark two sequence labeling tasks Ã¢ÂÂ POS and NER on 4 different language pairs with a suite of pretrained models to identify the problems and select the best performing model, char-BERT, among them (addressing (1)). We then propose a self training method to repurpose the existing pretrained models using a switch-point bias by leveraging unannotated data (addressing (2)). We finally demonstrate that our approach performs well on both tasks by reducing the gap between the switch point performance while retaining the overall performance on two distinct language pairs in both the tasks. Introduction Code-switching (CS) is a phenomenon of switching back and forth between multiple languages and is very common in multilingual communities such as India, Singapore, etc. Understanding mixed language texts has several applications in an in- creasingly online world like hateful content de- tection, maintaining engagement with virtual as- sistants. Despite this pervasive prevalence, CS is often overlooked in language processing research and current models still cannot effectively handle CS. We believe that the reasons behind this are (1) the lack of efforts in leveraging existing large scale multilingual resources or pretrained models and (2) dearth of annotated resources in switching scenar- ios. In this paper, we present solutions to address these two problems specifically. The advent of pretraining techniques marshalled the celebrated successes of several language un- derstanding and generation tasks in English (Dong et al., 2019) and multilingual tasks (Chaudhary et al., 2020). However, the same level of com- mendatory results are not translated to CS scenar- ios; as studied by Aguilar et al. (2020); Khanuja et al. (2020) presenting a preliminary evaluation of multi-lingual pretrained models for CS scenarios. It is still largely unclear if the inadequacies are re- sulting due to dearth of data or ineptitude of quick adoption of multilingual models. We study pre- cisely this problem of identifying the artifacts that hinder the competent performance of pretrained models on CS with a case study on sequence label- ing tasks including Part-Of-Speech (POS) tagging and Named Entity Recognition (NER). Our contributions from this work are as fol- lows: (1) We first conduct a comprehensive bench- marking of different pretrained models for two se- quence labeling tasks across 4 different language pairs. Specifically we evaluate datasets in Hinglish, Tenglish, Benglish and Spanglish CS for the tasks NER and POS. (2) To broaden understanding to- wards the usefulness of different fine-tuning strate- gies, we investigate multitasking, character model- ing uncovering the problematic switch point cases in ÃÂ§4. (3) We propose a novel switch-point bias based self training approach built upon on obser- vations from the benchmarks and demonstrate im- proved results on both tasks. Related Work CS benchmarks: From one of the recent surveys (Sitaram et al., 2019), linguistic CS has been stud- ied in the context of many NLP tasks including language identification (Solorio et al., 2014) (Bali et al., 2014), POS tagging (Soto and Hirschberg, 2018) (Molina et al., 2019) (Das, 2016), NER (Aguilar et al., 2019), parsing (Partanen et al., 2018), sentiment analysis(Vilares et al., 2015), and question answering (Chandu et al., 2019) (Raghavi et al., 2015). Many CS datasets have been made available through the shared-task series FIRE (Choudhury et al., 2014); (Roy et al., 2013) and CALCS (Aguilar et al., 2018), which have focused mostly on core NLP tasks. Additionally, other re- searchers have provided datasets such as humor detection (Khandelwal et al., 2018), sub-word CS detection (Mager et al., 2019) among others. More recently new CS benchmarks (Aguilar et2020) (Khanuja et al., 2020) have been developed to com- pare models across language pairs, domains and general language processing in CS. Pretrained Models for CS: Before the advent of pretrained multingual models, pretrained mono- lingual models were combined in different ways to derive word embeddings (AlGhamdi and Diab, 2019; Pratapa et al., 2018), POS tagging (Bhattu et al., 2020), sentiment analysis (Singh and Lefever, 2020) etc., Similarly, pretrained multilingual mod- els have been explored on various CS tasks like language identification, POS tagging, NER, ques- tion answering and Natural language inference (Khanuja et al., 2020). However, (Winata et al., 2021) show that these pretrained models do not assure high quality representations on CS. We ex- amine prospective reasons for this and present a data augmentation technique to mitigate this. Motivation for our work - Gaps in CS adapta- tion: Building off the prior work, we will briefly discuss primarily three techniques that demon- strated usefulness in adapting models to CS. First, non-standardization of cross-scripting (i.e, translit- eration of words to another language) is identified as one of the major reasons behind the noisiness of CS datasets (Chandu et al., 2019). Prior literature on noisy texts proved the superiority of charac- ter level modeling to combat this problem (Cherry et al., 2018); (Adouane et al., 2018). Secondly, the domains of most of these noisy datasets are still vastly scattered. In order to improve general- ization in CS patterns, prior studies have shown the potency of multitasking with an auxiliary task of language tag prediction (Winata et al., 2018). Thirdly, the dearth of annotated CS data has been a dramatic problem across tasks. (Bhattu et al.,2020) compare pretrained models with fined-tuned models augmented with unlabeled Twitter text to exemplify the improved performance with the lat- ter model. Despite these takeaways, the usefulness of the three points above is not thoroughly inves- tigated in the context of pretrained models for CS. To this end, we adapt these techniques in conjunc- tion with the pretraining strategies and propose a novel bias-based data iterative augmentation tech- nique to get more bang for the buck in terms of the performance to augmented dataset size ratio. Conclusions CS, despite being a natural and prevalent form of communication is still vastly understudied in em- pirical research. This mainly stems from the (1) lack of efforts in re-purposing the celebrated pre-trained models to CS scenarios and (2) lack of annotated resources. We tackle precisely these 2 problems with the main focus on evaluating and improving how these models fare at switch points between languages. First, we benchmark a suite of monolingual and multilingual pretrained models on CS and identify that particular switch points fare poorly. We propose a novel switch point bias based self training method to strategically use unla- beled data to enhance performance at switch points. While improving or retaining the overall perfor- mance compared to finetuning char-BERT and mul- titasking, we show that our approach improves the performance of underperforming switch points as well. We believe that this bias based augmentation technique particularly helps in scenarios with less annotated data. Broader Impact We believe that this work is a step towards effac- ing the hesitation of utilizing large scale pretrained mono and multilingual models for code-switched scenarios. We were able to successfully demon- strate the utility of a switch point based annota- tor model to perform biased data augmentation. We do not foresee any immediate ethical concerns branching directly from our work. However, we cautiously advise anyone using or extending our work for their application or research to bear in mind that we inherit any kinds of biases and tox- icity and privacy concerns that the pretrained lan- guage models bear. Although our end tasks are not directly affected forthwith due to these, we still rec- ommend caution when our self training approach is used for other tasks especially with user interaction such as dialog response generation etc., to ensure the model does not predict toxic content. Overall, we expect the users to benefit from our research to prospectively apply this to scenarios where there is a dearth of annotated resources, thereby economiz- ing on annotations cost and efforts and enabling scaling up to a wealth of crawled data, ifin those language-pairs.