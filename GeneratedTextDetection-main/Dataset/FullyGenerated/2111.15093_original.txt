Abstract Personalizing dialogue agents is important for dialogue systems to generate more specific, consistent, and engaging responses. How- ever, most current dialogue personalization ap- proaches rely on explicit persona descriptions during inference, which severely restricts its application. In this paper, we propose a novel approach that learns to predict persona infor- mation based on the dialogue history to per- sonalize the dialogue agent without relying on any explicit persona descriptions during infer- ence. Experimental results on the PersonaChat dataset show that the proposed method can im- prove the consistency of generated responses when conditioning on the predicted profile of the dialogue agent (i.e. “self persona”), and improve the engagingness of the generated re- sponses when conditioning on the predicted persona of the dialogue partner (i.e. “their per- sona”). We also find that a trained persona pre- diction model can be successfully transferred to other datasets and help generate more rele- vant responses. Introduction Recently, end-to-end dialogue response genera- tion models (Sordoni et al., 2015; Serban et al., 2016; Bordes et al., 2017) based on recent ad- vances of neural sequence-to-sequence learning models (Sutskever et al., 2014; Vaswani et al., 2017) have gained increasing popularity as they can generate fluent responses. However, as the dialogue agent is trained with datasets contain- ing dialogues from many different speakers, it can not generate personalized responses for the current speaker, making the generated responses less rele- vant and engaging (Li et al., 2016b). To address this problem, recent studies attempt to personalize dialogue systems by generating di- alogue responses conditioning on given persona descriptions have been shown to help dialogue agents perform better (Zhang et al., 2018; Mazare ́ et al., 2018). However, a major drawback of the current dialogue agent personalization approaches is that they require explicit persona descriptions in both training and inference stages, which severely limits their application in real-world scenarios be- cause detailed persona descriptions for current speakers are not available in most scenarios. An- other problem is that current dialogue personaliza- tion approaches are not interpretable and the role of additional persona information is unclear. In this paper, we propose a novel dialogue agent personalization approach that automatically infers the speaker’s persona based on the dialogue his- tory which implicitly contains persona informa- tion. Our model generates personalized dialogue responses based on the dialogue history and the inferred speaker persona, alleviating the necessity of the persona description during inference. Specifically, we propose two different ap- proaches to perform persona detection. The first approach learns a “persona approximator” which takes dialogue history as the input and is trained to approximate the output representation of a persona encoder that takes explicit persona description as the input. The second approach instead addresses the persona detection problem as a sequence-to- sequence learning problem and learns a “persona generator” which takes the dialogue history as the input and generates the persona description of the speaker. This approach provides a stronger super- vision signal compared with the first approach and is more interpretable as the encoded persona infor- mation can be decoded to reconstruct the detected persona description. Our proposed approach can be used to incor- porate both “self-persona” which is the persona information of the dialogue agent, and “their- persona” which is the persona information of the dialogue partner. On one hand, generating dialogue responses conditioning on the inferred “self- persona” can help the dialogue agent maintain a consistent persona during the conversation, thus enhancing the consistency of generated responses without the need of a pre-defined persona descrip- tion for every dialogue agent. On the other hand, generating dialogue responses conditioning on the predicted persona of the dialogue partner helps the dialogue model generate more engaging responses that are relevant to its dialogue partner. The abil- ity to automatically infer the persona information of the dialogue partner is particularly attractive be- cause in many real-world application scenarios, the persona information of the user is hardly avail- able before the dialogue starts. In addition, to fa- cilitate training and tackle the problem of lacking training data, we propose to train the persona de- tection model with multi-task learning by sharing layers and training jointly with the dialogue con- text encoder in both approaches. Our experiments on dialogue datasets with and without the persona description demonstrate the effectiveness of the proposed approach and show that a trained persona detection model can be suc- cessfully transferred to datasets without persona description. Related Work Preliminary study on dialogue personalization (Li et al., 2016b) attempts to use a persona-based neu- ral conversation model to capture individual char- acteristics such as background information and speaking style. However, it requires the current speaker during inference to have sufficient dialogue utterances included in the training set, which is quite restricted by the cold-start problem. More recently, Zhang et al. (2018) released the PersonaChat dataset which incorporates per- sona of two speakers represented as multiple sen- tences of profile description to personalize dia- logue agents. They propose a profile memory net- work by considering the dialogue history as in- put and then performing attention over the per- sona to be combined with the dialogue history. Mazare ́ et al. (2018) proposed to train a persona encoder and combine the encoded persona em- bedding with context representation by concate- nation. The combined representation is then fed into the dialogue decoder to generate personal- ized responses. (Yavuz et al., 2019) designed the DeepCopy model, which leverages copy mech- anism to incorporate persona texts and Madotto et al. (2019) propose to use meta-learning to adapt to the current speaker quickly, their approach also requires several dialogues of the speaker to per- form dialogue personalization, which is different from our approach. Wellecketal.(2019)propose a dialogue natural language inference dataset and use it to measure and improve the consistency of the dialogue system. More recently, Zheng et al. (2019) propose personalized dialogue generation with diversified traits. Song et al. (2020) introduce a multi-stage response generation stage to improve the personalization of generated responses. Wu et al. (2020) propose a variational response gener- ator to better exploit persona information. Differ- ent from the aforementioned works, our approach does not require persona information during test time, which makes it more generally applicable. Conclusion In this paper, we propose a novel dialogue per- sonalization approach that automatically infers the current speakers’ persona based on the dialogue history, which enables neural dialogue systems to generate personalized dialogue responses without using persona description at test time. Our exper- iments on the PersonaChat dataset show that the proposed models can improve the model’s con- sistency and engagingness when conditioning on the inferred persona information of the dialogue agent itself or the dialogue partner. We also con- duct experiments on the Dailydialog dataset where persona description is not available and find that pre-trained persona detection models can be suc- cessfully transferred to other datasets without an- notated persona descriptions. This further demon- strates the potential of our approach to enable per- sonalized dialogue response generation for various domains where persona descriptions are not avail- able or expensive to collect.