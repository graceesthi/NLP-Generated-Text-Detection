Abstract Named entity recognition (NER) models gen- erally perform poorly when large training datasets are unavailable for low-resource do- mains. Recently, pre-training a large-scale language model has become a promising di- rection for coping with the data scarcity is- sue. However, the underlying discrepancies between the language modeling and NER task could limit the models’ performance, and pre- training for the NER task has rarely been stud- ied since the collected NER datasets are gen- erally small or large but with low quality. In this paper, we construct a massive NER cor- pus with a relatively high quality, and we pre- train a NER-BERT model based on the cre- ated dataset. Experimental results show that our pre-trained model can significantly outper- form BERT (Devlin et al., 2019) as well as other strong baselines in low-resource scenar- ios across nine diverse domains. Moreover, a visualization of entity representations further indicates the effectiveness of NER-BERT for categorizing a variety of entities. 1 Introduction Named entity recognition1 (NER) plays an im- portant role in information extraction and text processing. Current NER systems heavily rely on large training datasets to achieve good perfor- mance (Lample et al., 2016; Chiu and Nichols, 2016; Ma and Hovy, 2016; Yadav and Bethard, 2018; Li et al., 2020), and a well-designed NER model normally has a poor generalization ability on low-resource domains, where large numbers of training data are unavailable (Jia et al., 2019; Liu et al., 2021b). Given that collecting numer- ous NER training data is not just expensive but also time-consuming, it is essential to construct a NER model that can quickly adapt to low-resource domains using only a few data examples. Recently, pre-training a large-scale language model (Devlin et al., 2019; Liu et al., 2019) has been shown to be effective in a data scarcity sce- nario (Ma et al., 2019; Radford et al., 2019; Chen et al., 2020). However, the underlying discrepan- cies between the language modeling and the NER task could limit the performance of pre-trained lan- guage models on this task. Unfortunately, conduct- ing a NER-specific pre-training has rarely been studied because constructing a large-scale and high- quality corpus for this purpose is not a simple task. Although there are plenty of publicly available NER datasets, they generally have different an- notation schemes and different entity categories. For example, the CoNLL2003 dataset (Sang and De Meulder, 2003) has the “miscellaneous” entity category, which the Broad Twitter dataset (Der- czynski et al., 2016) lacks, and the WNUT2017 dataset (Derczynski et al., 2017) has “corporation” and “group” entity categories, while many other datasets (Sang and De Meulder, 2003; Lu et al., 2018) use the “organization” entity type. Thus, it is difficult to unify the annotation scheme for all datasets, and jointly training models on different schemes will confuse the model in categorizing entities. In addition, the existing NER datasets are much smaller than those of the plain text used for the language modeling task, which will result in a less effective pre-training. Instead of utilizing manually annotated NER datasets, a few previous studies (Cao et al., 2019; Mengge et al., 2020) have focused on leverag- ing weakly-labeled NER data constructed from Wikipedia to enhance the model’s performance. Cao et al. (2019) generated the weakly-labeled data based on Wikipedia anchors and a taxonomy, but the quality of the produced data is relatively low and the number of entity categories is limited. To cope with these issues, Mengge et al. (2020) lever- aged a gazetteer to obtain coarse-grained entities and k-means clustering to further mine the fine-grained entities. However, obtaining fine-grained labels based on clustering algorithms is not stable, which could limit the effectiveness of pre-training. In this work, we first aim to construct a large- scale NER dataset with a relatively high quality and abundant entity categories. After that, our goal is to prove that using the created dataset to pre-train an entity tagging model can outperform pre-trained language models on the low-resource NER task. Similar to Cao et al. (2019), we build the NER dataset based on the Wikipedia corpus. To improve the quality and increase the number of entity cate- gories, we utilize the DBpedia Ontology (Mendes et al., 2012) to assist in categorizing entities in the Wikipedia corpus. Eventually, we obtain around 16 million NER training examples, and then we continue pre-training BERT on the NER task using the constructed data to build NER-BERT. We emphasize that the focus of this paper is not to achieve state-of-the-art results, but to show the effectiveness of entity tagging-based pre-training using our constructed corpus, since current state- of-the-art NER models are constructed on top of pre-trained language models (e.g., BERT (Devlin et al., 2019)) which can be easily replaced by our NER-BERT. Therefore, we simply add a linear layer instead of many complex components on top of the pre-trained models when fine-tuning them on the downstream NER task. We evaluate our model and baselines on nine diverse domains (e.g., litera- ture, biomedical, and Twitter) of the NER task and show that our model can surpass BERT and other strong baselines such as cross-domain language modeling (Jia et al., 2019) and domain-adaptive pre-training (Gururangan et al., 2020; Liu et al., 2021b). Furthermore, we conduct extensive ex- periments in terms of different low-resource levels across multiple diverse target domains and demon- strate that NER-BERT has a powerful few-shot adaptation ability to target domains when only a few training data are available. Additionally, we visualize the entity representations for NER-BERT and baselines to further prove the effectiveness of our NER pre-training. Moreover, we will release our constructed dataset and pre-trained model to facilitate future research in this area. Conclusion In this paper, we first incorporate Wikipedia an- chors and DBpedia Ontology to build a large-scale NER dataset with a relatively high quality. Then, we utilize the constructed dataset to pre-train NER- BERT. Results illustrate that it is essential to lever- age various entity categories for pre-training, and NER-BERT is able to significantly outperform BERT as well as other strong baselines across nine diverse domains. Additionally, we show that NER-BERT is especially effective when only a few pre- training examples are available in target domains. Moreover, the visualization further indicates that NER-BERT possesses good pre-learned knowledge for categorizing a variety of entities.