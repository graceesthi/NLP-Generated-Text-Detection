Abstract Unsupervised domain adaptation (UDA) with pre-trained language models (PrLM) has achieved promising results since these pre-trained models embed generic knowledge learned from various domains. However, fine-tuning all the parameters of the PrLM on a small domain-specific corpus distort the learned generic knowledge, and it is also expensive to deployment a whole fine-tuned PrLM for each domain. This paper explores an adapter-based fine-tuning approach for unsupervised domain adaptation. Specifically, several trainable adapter modules are inserted in a PrLM, and the embedded generic knowledge is preserved by fixing the parameters of the original PrLM at fine-tuning. A domain-fusion scheme is introduced to train these adapters using a mix-domain corpus to better capture transferable features. Elaborated experiments on two benchmark datasets are carried out, and the results demonstrate that our approach is effective with different tasks, dataset sizes, and domain similarities. Introduction Unsupervised domain adaption (UDA) is an essential task in the realm of deep learning since it mitigates the expensive burden of manual annotation by focusing on cheap unlabeled data from target domains [Ramponi and Plank, 2020]. Among all existing approaches for UDA, pre-trained language model (PrLM) based approaches become the de-facto standard [Gururangan et al., 2020, Ben-David et al., 2020, Yu et al., 2021, Karouzos et al., 2021] since these PrLMs are equipped with generic knowledge learned from large corpora [Howard and Ruder, 2018] and lead to promising results. The primary focuses of UDA methods are to capture the transferable features for the target domain while reserving the knowledge learned from the source domain [Blitzer et al., 2006, Pan et al., 2010]. However, most existing pre-training-based UDA approaches are carried out by fine-tuning the entire set of model parameters on domain-specific corpora [Gururangan et al., 2020, Yu et al., 2021, Karouzos et al., 2021], which are usually of limited sizes. Such a setting may easily drift the PrLM to a specified domain and distort the generic knowledge embedded in the original PrLM weights [Pfeiffer et al., 2020, He et al., 2021]. This hinders the model from capturing transferable features between different domains and leads to sub-optimal performance for UDA tasks [Karouzos et al., 2021]. Moreover, it is also expensive to fine-tune and deploy a large model for every single domain [Houlsby et al., 2019]. We observe that the intuition of preserving learned knowledge coincides with recently developed adapter-based tuning methods [Houlsby et al., 2019, Rebuffi et al., 2017], in which several trainable adapter modules are introduced between layers of a pre-trained language model (LM) while parameters from the original LM are fixed. This setting helps preserve the knowledge embedded in the PrLM and alleviates the distortion of features for different domains since the original PrLM remains intact [He et al., 2021, Pfeiffer et al., 2020,, Houlsby et al., 2019]. However, few studies are performed to extend this effective method to tackle UDA tasks. In this paper, we explore to introduce adapter modules in pre-training-based UDA approaches. Specifically, several bottle-necked adapter modules are inserted in a transformer-based PrLM [Vaswani et al., 2017]. These adapters are learned following a two-step process: 1) The domain-fusion training step trains adapters with the Masked-Language-Model (MLM) loss [Devlin et al., 2019] on a mixed corpus containing data from both the source and all the target domains. This step facilitates the capture and fusion of transferable knowledge between different domains; 2) The task fine-tuning step fine-tunes adapters with the task-specific loss on the source domain corpus. Note that parameters of the underlying pre-trained LM are fixed throughout the two learning processes. This helps prevent the drifting of learned generic knowledge and facilitates more effective domain knowledge transferring [Pfeiffer et al., 2020]. In the testing phase, we apply the resulted model to data sampled from the target domain. The results on two benchmark datasets indicate that our method outperforms competitive baselines and is effective in improving the performance of downstream UDA tasks 3 . Our contributions can be summarized as: 1. We apply adapter modules in the pre-training-based UDA approaches. Specifically, trainable adapters arein a PrLM, and a two-step process is introduced to facilitate the learning of these adapters. 2. Elaborated experiments on two benchmark datasets show that our approach outperforms competitive baselines and is more effective to improve the performance of downstream UDA tasks. Related Work Unsupervised Domain Adaption: Existing UDA approaches are carried out by fine-tuning the entire set of model parameters on domain-specific corpora [Gururangan et al., 2020, Yu et al., 2021, Karouzos et al., 2021], which are usually of limited sizes. Such a setting may easily drift the PrLM to a specified domain and distort the generic knowledge embedded in the original PrLM weights [Pfeiffer et al., 2020, He et al., 2021]. This hinders the model from capturing transferable features between different domains and leads to sub-optimal performance for UDA tasks [Karouzos et al., 2021]. Moreover, it is also expensive to fine-tune and deploy a large model for every single domain [Houlsby et al., 2019]. We observe that the intuition of preserving learned knowledge coincides with recently developed adapter-based tuning methods [Houlsby et al., 2019, Rebuffi et al., 2017], in which several trainable adapter modules are introduced between layers of a pre-trained language model (LM) while parameters from the original LM are fixed. This setting helps preserve the knowledge embedded in the PrLM and alleviates the distortion of features for different domains since the original PrLM remains intact [He et al., 2021, Pfeiffer et al., 2020,, Houlsby et al., 2019]. However, few studies are performed to extend this effective method to tackle UDA tasks. In this paper, we explore to introduce adapter modules in pre-training-based UDA approaches. Specifically, several bottle-necked adapter modules are inserted in a transformer-based PrLM [Vaswani et al., 2017]. These adapters are learned following a two-step process: 1) The domain-fusion training step trains adapters with the Masked-Language-Model (MLM) loss [Devlin et al., 2019] on a mixed corpus containing data from both the source and all the target domains. This step facilitates the capture and fusion of transferable knowledge between different domains; 2) The task fine-tuning step fine-tunes adapters with the task-specific loss on the source domain corpus. Note that parameters of the underlying pre-trained LM are fixed throughout the two learning processes.