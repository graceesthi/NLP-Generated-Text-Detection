Abstract Pre-trained language models have been found to capture a surprisingly rich amount of lexical knowledge, ranging from common sense prop- erties of everyday concepts to detailed factual knowledge about named entities. Among oth- ers, this makes it possible to distill high-quality word vectors from pre-trained language mod- els. However, it is currently unclear to what ex- tent it is possible to distill relation embeddings, i.e. vectors that characterize the relationship between two words. Such relation embeddings are appealing because they can, in principle, encode relational knowledge in a more fine- grained way than is possible with knowledge graphs. To obtain relation embeddings from a pre-trained language model, we encode word pairs using a (manually or automatically gen- erated) prompt, and we fine-tune the language model such that relationally similar word pairs yield similar output vectors. We find that the resulting relation embeddings are highly competitive on analogy (unsupervised) and re- lation classification (supervised) benchmarks, even without any task-specific fine-tuning. Introduction One of the most widely studied aspects of word embeddings is the fact that word vector differences capture lexical relations (Mikolov et al., 2013a). While not being directly connected to downstream performance on NLP tasks, this ability of word em- beddings is nonetheless important. For instance, understanding lexical relations is an important pre- requisite for understanding the meaning of com- pound nouns (Turney, 2012). Moreover, the ability of word vectors to capture semantic relations has enabled a wide range of applications beyond NLP, including flexible querying of relational databases (Bordawekar and Shmueli, 2017), schema matching (Fernandez et al., 2018), completion and re- trieval of Web tables (Zhang et al., 2019), ontology completion (Bouraoui and Schockaert, 2019) and information retrieval in the medical domain (Ar- guello Casteleiro et al., 2020). More generally, relational similarity (or analogy) plays a central role in computational creativity (Goel, 2019), le- gal reasoning (Ashley, 1988; Walton, 2010), on- tology alignment (Raad and Evermann, 2015) and instance-based learning (Miclet et al., 2008). Given the recent success of pre-trained language models (Devlin et al., 2019; Liu et al., 2019; Brown et al., 2020), we may wonder whether such mod- els are able to capture lexical relations in a more faithful or fine-grained way than traditional word embeddings. However, for language models (LMs), there is no direct equivalent to the word vector difference. In this paper, we therefore propose a strategy for extracting relation embeddings from pre-trained LMs, i.e. vectors encoding the relation- ship between two words. On the one hand, this will allow us to gain a better understanding of how well lexical relations are captured by these models. On the other hand, this will also provide us with a practical method for obtaining relation embeddings in applications such as the ones mentioned above. Since it is unclear how LMs store relational knowledge, rather than directly extracting relation embeddings, we first fine-tune the LM, such that re- lation embeddings can be obtained from its output. To this end, we need a prompt, i.e. a template to convert a given word pair into a sentence, and some training data to fine-tune the model. To illustrate the process, consider the word pair Paris-France. As a possible input to the model, we could use a sentence such as “The relation between Paris and France is <mask>". Note that our aim is to find a strategy that can be applied to any pair of words, hence the way in which the input is repre- sented needs to be sufficiently generic. We then fine-tune the LM such that its output corresponds to a relation embedding. To this end, we use a crowdsourced dataset of relational similarity judge- ments that was collected in the context of SemEval 2012 Task 2 (Jurgens et al., 2012). Despite the relatively small size of this dataset, we show that the resulting fine-tuned LM allows us to produce high-quality relation embeddings, as confirmed in our extensive evaluation in analogy and relation classification tasks. Importantly, this also holds for relations that are of a different nature than those in the SemEval dataset, showing that this process allows us to distill relational knowledge that is en- coded in the pre-trained LM, rather than merely generalising from the examples that were used for fine-tuning. Related Work Bommasani et al., 2020; Vulic et al., 2020), and var- ious forms of factual and commonsense knowledge (Petroni et al., 2019; Forbes et al., 2019; Davison et al., 2019; Zhou et al., 2020; Talmor et al., 2020; Roberts et al., 2020), among others. The idea of extracting relational knowledge from LMs, in par- ticular, has also been studied. For instance, Petroni et al. (2019) use BERT for link prediction. To this end, they use a manually defined prompt for each relation type, in which the tail entity is replaced by a <mask> token. To complete a knowledge graph triple such as (Dante, born-in, ?) they create the input “Dante was born in <mask>” and then look at the predictions of BERT for the masked token to retrieve the correct answer. It is notable that BERT is thus used for extracting relational knowledge without any fine-tuning. This clearly shows that a substantial amount of factual knowl- edge is encoded in the parameters of pre-trained LMs. Some works have also looked at how such knowledge is stored. Geva et al. (2020) argue that the feed-forward layers of transformer-based LMs act as neural memories, which would suggest that e.g. “the place where Dante is born” is stored as a property of Florence. Dai et al. (2021) present further evidence of this view. What is less clear, then, is whether relations themselves have an ex- plicit representation, or whether transformer mod- els essentially store a propositional knowledge graph. The results we present in this paper sug- gest that common lexical relations (e.g. hypernymy, meronymy, has-attribute), at least, must have some kind of explicit representation, although it remains unclear how they are encoded. Another notable work focusing on link pre- diction is (Bosselut et al., 2019), where GPT is fine-tuned to complete triples from commonsense knowledge graphs, in particular ConceptNet (Speer et al., 2017) and ATOMIC (Sap et al., 2019). While their model was able to generate new knowledge graph triples, it is unclear to what extent this is achieved by extracting commonsense knowledge that was already captured by the pre-trained GPT model, or whether this rather comes from the abil- ity to generalise from the training triples. For the ConceptNet dataset, for instance, Jastrze ̨bski et al. (2018) found that most test triples are in fact minor variations of training triples. In this paper, we also rely on fine-tuning, which makes it harder to de- termine to what extent the pre-trained LM already captures relational knowledge. We address this con- cern by including relation types in our evaluation which are different from the ones that have been used for fine-tuning. Unsupervised Relation Discovery Modelling how different words are related is a long-standing challenge in NLP. An early approach is DIRT (Lin and Pantel, 2001), which encodes the relation be- tween two nouns as the dependency path connect- ing them. Their view is that two such depen- dency paths are similar if the sets of word pairs with which they co-occur are similar. Hasegawa et al. (2004) cluster named entity pairs based on the bag-of-words representations of the contexts in which they appear. Along the same lines, Yao et al. (2011) proposed a generative probabilistic model, inspired by LDA (Blei et al., 2003), in which re- lations are viewed as latent variables (similar to topics in LDA). Turney (2005) proposed a method called Latent Relational Analysis (LRA), which uses matrix factorization to learn relation embed- dings based on co-occurrences of word pairs and dependency paths. Matrix factorization is also used in the Universal Schema approach from Riedel et al. (Riedel et al., 2013), which jointly models the contexts in which words appear in a corpus with a given set of relational facts. The aforementioned works essentially represent the relation between two words by summarising the contexts in which these words co-occur. In recent years, a number of strategies based on dis- tributional models have been explored that rely on similar intuitions but go beyond simple vec- tor operations of word embeddings.2 For instance, Jameel et al. (2018) introduced a variant of the GloVe word embedding model, in which relation vectors are jointly learned with word vectors. In SeVeN (Espinosa-Anke and Schockaert, 2018) and RELATIVE (Camacho-Collados et al., 2019), rela- tion vectors are computed by averaging the embed- dings of context words, while pair2vec (Joshi et al., 2019) uses an LSTM to summarise the contexts in which two given words occur, and Washio and Kato (2018) learn embeddings of dependency paths to encode word pairs. Another line of work is based on the idea that relation embeddings should facil- itate link prediction, i.e. given the first word and a relation vector, we should be able to predict the second word (Marcheggiani and Titov, 2016; Si- mon et al., 2019). This idea also lies at the basis of the approach from Soares et al. (2019), who train a relation encoder by fine-tuning BERT (Devlin et al., 2019) with a link prediction loss. However, it should be noted that they focus on learning relation vectors from individual sentences, as a pre-training task for applications such as few-shot relation ex- traction. In contrast, our focus in this paper is on characterising the overall relationship between two words. Conclusion We have proposed a strategy for learning relation embeddings, i.e. vector representations of pairs of words which capture their relationship. The main idea is to fine-tune a pre-trained language model us- ing the relational similarity dataset from SemEval 2012 Task 2, which covers a broad range of seman- tic relations. In our experimental results, we found the resulting relation embeddings to be of high qual- ity, outperforming state-of-the-art methods on sev- eral analogy and relation classification benchmarks. Among the models tested, we obtained the best re- sults with RoBERTa, when using manually defined templates for encoding word pairs. Importantly, we found that high-quality relation embeddings can be obtained even for relations that are unlike those from the SemEval dataset, such as morphological and encyclopedic relations. This suggests that the knowledge captured by our relation embeddings is largely distilled from the pre-trained language model, rather than being acquired during training.