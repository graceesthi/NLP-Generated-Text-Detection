Abstract In this work, we extensively redesign the newly introduced method of token mixing us- ing Fourier Transforms (FNET) to replace the computationally expensive self-attention mechanism in a full transformer implementa- tion on a long document summarization task (ÃÂÃÂ¿ 512 tokens). As a baseline, we also car- ried out long document summarization using established methods such as Longformer and Big Bird transformer models that are capable of processing over 8000 tokens and are cur- rently the state of the art methods for these type of problems. The original FNET pa- per implemented this in an encoder only ar- chitecture while abstractive summarization re- quires both an encoder and a decoder. Since such a pretrained transformer model does not currently exist in the public domain, we de- cided to implement a full transformer based on this Fourier token mixing approach in an encoder/decoder architecture which we trained starting with Glove embeddings for the indi- vidual words in the corpus. We investigated a number of different extensions to the original FNET architecture and evaluated them on their Rouge F1-score performance on a summariza- tion task. All modifications showed better performance on the summarization task than when using the original FNET encoder in a transformer architecture. Introduction Abstractive summarization has made significant strides since the introduction of the transformer based model in NLP (Vaswani et al., 2017). How- ever, the quadratic computational and memory com- plexities of large transformers have limited their scalability for long document summarization as the token length for a standard transformer is limited to 512 tokens. One can try extractive summarization to reduce the length of the document while retain- ing the key elements of the article then taking an abstractive approach on the reduced document. In the extractive step, only the most important sentences are chosen to reduce the size of the document to fit within the token limits of the transformer model. Another way is to use extractive summarization to summarize the document thus retaining only the salient information. This approach is compu- tationally very expensive. Alternative transformer approaches such as the longformer (Beltagy et al., 2020) and Bigbird (Zaheer et al., 2021) alleviate the computation burden of the self-attention mech- anism by limiting the attention window each token has access to. Longformer was created for this pur- pose, using a pluggable sparse attention mechanism that combines dilated windowed attention for local context with full global attention on some tokens, of which the latter varies per task. This introduces an attention mechanism that grows linearly with sequence length using a sliding window of size w allowing for dealing documents in excess of 8000 tokens. More recently, a group at Google (Lee-Thorp et al., 2021) has introduced a new implementa- tion that replaces the entire self-attention heads in the transformer encoder with a non-parameterized Fourier transform mixing of the tokens that does not suffer from this quadratic computation penalty. We propose to extend this architecture to the long document summarization problem and compare the results to the two current baseline practices: Extracting the salient information then applying ab- stractive summarization using PEGASUS (Zhang et al., 2020) and using a Longformer implementa- tion. On both baseline approaches, we investigated multiple hyperparameter optimization and evalu- ated the summaries relative to their corresponding abstracts. This becomes the method for comparing the performance of each methodology. The primary dataset used for this work is the PubMed dataset (Dernoncourt and Lee, 2017) as there exists several prior work on long document summarization with it that we can compare to. Ac- cording to Zaheer et al. (2021), this dataset has a median token length of 2,715 with the 90th per- centile token length being 6,101. Dernoncourt and Lee (2017) shows how extensive this dataset is, with close to 200,00 articles. We decided to use the most common evaluation technique for document summarization Ã¢ÂÂ ROUGE scores (Lin, 2004). In our analyses, we include F1-scores for Rouge-1, Rouge-2, Rouge-3, and Rouge-l scores for completeness. Conclusion We have demonstrated for the first time that the recently proposed FNET architecture can be ex- tended to a full transformer model on an abstractive summarization task with a PubMed dataset. Even with a toy implementation, we have shown several novel architectural changes to the original proposal that can be used for a variety of tasks requiring low computationalwhile maintaining reasonable accuracy. Our toy architecture yields lower Rouge scores than the baseline for two main reasons. First because the transformer model is much smaller and also because we did not have a pretrained FNET transformer as a starting point. The contribution of this work is the investigation of alternative transformer approaches to the original FNET architecture. === FNET As a baseline, we also car- ried out long document summarization using established methods such as Longformer and Big Bird transformer models that are capable of processing over 8000 tokens and are cur- rently the state of the art methods for these type of problems. Since such a pretrained transformer model does not currently exist in the public domain, we de- cided to implement a full transformer based on this Fourier token mixing approach in an encoder/decoder architecture which we trained starting with Glove embeddings for the indi- vidual words in the corpus. We investigated a number of different extensions to the original FNET architecture and evaluated them on their Rouge scores (red) and performance on a summariza- tion task (green).