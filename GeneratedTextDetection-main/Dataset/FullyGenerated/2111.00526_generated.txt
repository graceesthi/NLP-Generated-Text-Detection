ABSTRACT We introduce a new language representation model in finance called Financial Embedding Analysis of Sentiment (FinEAS). In financial markets, news and investor sentiment are significant drivers of security prices. Thus, leveraging the capabilities of modern NLP approaches for financial sentiment analysis is a crucial component in identifying patterns and trends that are useful for market participants and regulators. In recent years, methods that use transfer learning from large Transformer-based language models like BERT, have achieved state-of-the-art results in text classification tasks, including sentiment analysis using labelled datasets. Researchers have quickly adopted these approaches to financial texts, but best practices in this domain are not well-established. In this work, we propose a new model for financial sentiment analysis based on supervised fine-tuned sentence embeddings from a standard BERT model. We demonstrate our approach achieves significant improvements in comparison to vanilla BERT, LSTM, and FinBERT, a financial domain specific BERT. Introduction Sentiment analysis is a technique where text is classified as conveying positive or negative meaning through the use of natural language processing (NLP). In the field of finance, news and investor sentiment are significant drivers of the individual security prices and the market as a whole. Natural Language Understanding (NLU) capabilities in the financial domain are needed for automating tasks and analysis. Recently, practitioners of Natural Language Processing (NLP) who were involved in financial sentiment used a variety of cutting-edge machine learning algorithms, such as LSTMs. Researchers have quickly adopted modern NLP approaches to the financial domain, based on the latest successes in NLP, which leverage transfer learning from general Transformerbased language models. Nonetheless, there is a lack of knowledge about best practices for using Transformers in this domain. In this article, we propose an effective approach based on the use of Transformer language models that are explicitly developed for sentence-level analysis. More specifically, we build upon the findings from Sentence-BERT (Reimers and Gurevych, 2019). In this work, the authors show that vanilla BERT does not provide strong Ã¢ÂÂout of the boxÃ¢ÂÂ sentence embeddings (unlike the token embeddings, which are either state-of-the-art or close to the state-of-the-art, depending on the task). Since financial sentiment is a sentence-level task, we base our approach on Sentence-BERT. In Section 2, we briefly review related work. Then, in Section 3 we describe our proposed method. In Section 4, we present performance of our model and compare it to both common baselines and a domain-specific baseline. Finally, in Sections 5 and 6 we summarize our results and conclusions. We make the code1 and models2 publicly available. Related Work Background Researchers and practitioners started using the lexicon-based bag-of-words, with a predefined dictionary of positive and negative words. Here, one computes the sentiment score based on the number of matches of words in the text with each of the dictionaries. Indeed, this is exactly the approach taken by one of the first highly influential academic articles in the fields of text analysis in finance. In 2007, Paul Tetlock showed that the frequency of negative words in articles of the Wall Street Journal had predictive power of the future price moves of the Dow Jones Industrial Average Index and the daily volume traded on the New York Stock Exchange (Tetlock, 2007). Some work in the literature use a bag-of-words approach where the sentiment word lists are from the Loughran and McDonald financial dictionary (2009) (Loughran and McDonald, 2011). As a more advanced machine learning approach, researchers later introduced TF-IDF (Term FrequencyÃ¢ÂÂInverse Document Frequency) for encoding text, and then trained supervised learning algorithms, such as SVMs (Support Vector Machines), with labelled datasets. An interesting application to Environmental, Social, and Governance (ESG) and UN Sustainable Development Goals (SDG) investing can be found in Madelyn Antoncic and Noguer-Alonso (2020). Current approaches With the advent of deep learning and its application in NLP, researchers began applying Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs) for text classification. The current state-of-the-art in text classification typically involves a purely attentional architecture, the Transformer architecture (Vaswani et al., 2017), especially by fine-tuning pre-trained models. Specifically for financial sentiment, we highlight(2019a), in which the authors fine-tune a pre-trained Transformer in the Financial Phrasebank dataset (Malo et al., 2014). BERT Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2018) is a Transformer encoder pre-trained on large textual corpora without supervision. The attention mechanism of the Transformer allows obtaining contextual word embeddings, i.e. word embeddings taking into account the rest of the word embeddings in the sentence. For pre-training without supervision, BERT uses two surrogate tasks: Ã¢ÂÂ¢ Masked Language Modeling (MLM): 15% of the tokens are randomly masked (i.e., replaced with the special token , and the model is asked to predicted them. To do so, the model must learn useful representations from the context. In this way, it learns to produce token-level embeddings. Ã¢ÂÂ¢ Next Sentence Prediction (NSP): Each training instance consists of a sentence pair. Half of the time, the second sentence is a random sentence; in the other 50% of occurrences, the second sentence is the actual sentence that appears next to the original sentence. The model must predict, from the embeddings of the special token (class), whether the second sentence is the next one or not. In this fashion, it learns to produce sentence-level embeddings. Domain-specific models Several studies in different domains have demonstrated that domain-specific BERT models can outperform the generic BERT. Perhaps most well-known are BioBERT (Lee et al., 2019) and SciBERT (Beltagy et al., 2019) in the biomedical/scientific domain. In the case of the financial domain, starting from the original English BERT, FinBERT (Araci et al., 2019) is fine-tuned on the Financial Phrasebank (Finow et al., 2019) and FinBERT (Araci et al., 2019) in the financial domain. In contrast, starting from the original Transformer pre-trained on large textual corpora (Malo et al., 2014). Since financial sentiment is a sentence-level task, we base our approach on Sentence-BERT. In Section 2, we briefly review related work. Then, in Section 3 we describe our proposed method. In Section 4, we present performance of our model and compare it to both common baselines and a domain-specific baseline. Finally, in Sections 5 and 6 we summarize our results and conclusions. We make the code1 and models2 publicly available. Related Work Background Researchers and practitioners started using the lexicon-based bag-of-words, with a predefined dictionary of positive and negative words. Here, one computes the sentiment score based on the number of matches of words in the text with each of the dictionaries. Indeed, this is exactly the approach taken by one of the first highly influential academic articles in the fields of text analysis in finance. In 2007, Paul Tetlock showed that the frequency of negative words in articles of the Wall Street Journal had predictive power of the future price moves of the Dow Jones Industrial Average Index and the daily volume traded on the New York Stock Exchange (Tetlock, 2007). Some work in the literature use a bag-of-words approach where the sentiment word lists are from the Loughran and McDonald financial dictionary (2009) (Loughran and McDonald, 2011). As a more advanced machine learning approach, researchers later introduced TF-IDF (Term FrequencyÃ¢ÂÂInverse Document Frequency) for encoding text, and then trained supervised learning algorithms, such as SVMs (Support Vector Machines), with labelled datasets. An interesting application to Environmental, Social, and Governance (ESG) and UN Sustainable Development Goals (SDG) investing can be found in Madelyn Antoncic and Noguer-Alonso (2020).