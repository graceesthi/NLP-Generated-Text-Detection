ABSTRACT Unsupervised pre-training is now the predominant approach for both text and speech understanding. Self-attention models pre-trained on large amounts of unannotated data have been hugely successful when fine-tuned on downstream tasks from a variety of domains and languages. This paper takes the universality of unsupervised language pre-training one step further, by unifying speech and text pre-training within a single model. We build a single encoder with the BERT objective on unlabeled text together with the w2v-BERT objective on unlabeled speech. To further align our model representations across modalities, we leverage alignment losses, specifically Translation Language Modeling (TLM) and Speech Text Matching (STM) that make use of supervised speech-text recognition data. We demonstrate that incorporating both speech and text data during pre-training can significantly improve downstream quality on CoVoST 2 speech translation, by around 1 BLEU compared to single-modality pre-trained models, while retaining close to SotA performance on LibriSpeech and SpeechStew ASR tasks. On four GLUE tasks and text-normalization, we observe evidence of capacity limitations and interference between the two modalities, leading to degraded performance compared to an equivalent text-only model, while still being competitive with BERT. Through extensive empirical analysis we also demonstrate the importance of the choice of objective function for speech pre-training, and the beneficial effect of adding additional supervised signals on the quality of the learned representations. INTRODUCTION Self-supervised learning of text and speech representations has been particularly impactful in natural language processing and speech processing. Since GPT (Radford et al., 2018), BERT (Devlin et al., 2019) and their variations (Yang et al., 2019; Conneau & Lample, 2019; Lewis et al., 2020; Raffel et al., 2020; Joshi et al., 2020), performance on natural language understanding downstream tasks (Socher et al., 2013; Rajpurkar et al., 2016; Agirre et al., 2007; Williams et al., 2018) and monolingual (e.g., GLUE (Wang et al., 2019b), SuperGLUE (Wang et al., 2019a)) and multilingual (e.g., XTREME (Hu et al., 2020), XTREME-R (Ruder et al., 2021)) benchmarks has largely improved thanks to evolving pre-trained models, which leverage increasing amounts of unannotated data (Radford et al., 2019; Liu et al., 2019; Conneau et al., 2019; Wenzek et al., 2020; Xue et al., 2021b) and increased model capacity (Brown et al., 2020; Xue et al., 2021b; Lepikhin et al., 2020; Fedus et al., 2021). Similarly for speech, unsupervised pre-training has emerged as a predominant approach. Wav2vec 2.0 (Baevski et al., 2020b) and newer variants (Zhang et al., 2020) initially showed the strength of pre-training on speech recognition (Panayotov et al., 2015; Kahn et al., 2020; Zhang et al., 2021) on multiple domains (Hsu et al., 2021) and languages (Conneau et al., 2020). Self-supervised learning methods in language understanding are designed to be used universally, i.e. a single large pre-trained model for all domains and languages. One big advantage of these universal models is the ability to leverage data skew across domains, tasks and languages; the availability of task or domain-specific data in one language can boost model performance for several languages that the model was pre-trained on. Extending this generalization capability across modalities by having neural networks understand both text and speech at the same time is a natural next step. Jointly pre-training models on speech and text is a natural choice for multimodal self-supervised learning, given the similarities between the two modalities and the abundance of unannotated text data compared to speech. Recent work has also shown that self-supervised speech representations can be aligned to text with little to no supervision (Baevski et al., 2021), suggesting the possibility of learning both modalities within a single neural network. However, past work in multilingual modeling in particular has demonstrated the difficulty of learning representations of different data structures, however similar, within a shared network, exposing the so-called transfer interference problem (Arivazhagan et al., 2019). We show in thisthat this trade-off also applies to joint speech-text self-supervised learning. We study a new multimodal speech-text pre-training approach that leverages data from one modality to improve representations of the other, but also suffers from transfer interference and capacity dilution. Our Speech and LAnguage Model (SLAM) consists of a single Conformer (Gulati et al., 2019) and a SuperGLUE (Wang et al., 2020; Xue et al., 2021b) that makes use of supervised speech-text recognition data. We demonstrate that incorporating both speech and text data during pre-training can significantly improve downstream quality on CoVoST 2 speech translation, by around 1 BLEU compared to single-modality pre-trained models, while retaining close to SotA performance on LibriSpeech and SpeechStew ASR tasks. On four GLUE tasks and text-normalization, we observe evidence of capacity limitations and interference between the two modalities, leading to degraded performance compared to an equivalent text-only model, while still being competitive with BERT. Through extensive empirical analysis we also demonstrate the importance of the choice of objective function for speech pre-training, and the beneficial effect of adding additional supervised signals on the quality of the learned representations. INTRODUCTION Self-supervised learning of text and speech representations has been particularly impactful in natural language processing and speech processing. Since GPT (Radford et al., 2018), BERT (Devlin et al., 2019) and their variations (Yang et al., 2019; Conneau & Lample, 2019; Lewis et al., 2020; Raffel et al., 2020; Joshi et al., 2020), performance on natural language understanding downstream tasks (Socher et al., 2013; Rajpurkar et al., 2016; Agirre et al., 2007; Williams et al., 2018) and monolingual (e.g., GLUE (Wang et al., 2020), SuperGLUE (Wang et al., 2021b)) and multilingual (e.g., XTREME (Hu et al., 2021), XTREME-R (Ruder et al., 2021)) benchmarks has largely improved thanks to evolving pre-trained models, which leverage increasing amounts of unannotated data (Radford et al., 2019; Liu et al., 2019; Conneau et al., 2019; Wenzek et al., 2020; Xue et al., 2021b) and increased model capacity (Brown et al., 2020; Xue et al., 2021b; Lepikhin et al., 2020; Fedus et al., 2021). Similarly for speech, unsupervised pre-training has emerged as a predominant approach. Wav2vec 2.0 (Baevski et al., 2020b) and newer variants (Zhang et al., 2020) initially showed the strength of pre-training on speech recognition (Panayotov et al., 2015; Kahn et al., 2020; Zhang et al., 2021) on multiple domains (Hsu et al., 2021) and languages (Conneau et al., 2020).