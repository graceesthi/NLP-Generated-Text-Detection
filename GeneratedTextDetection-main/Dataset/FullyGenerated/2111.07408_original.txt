Abstract When an NLP model is trained on text data from one time period and tested or deployed on data from another, the resulting tempo- ral misalignment can degrade end-task perfor- mance. In this work, we establish a suite of eight diverse tasks across different domains (social media, science papers, news, and re- views) and periods of time (spanning five years or more) to quantify the effects of temporal misalignment. Our study is focused on the ubiquitous setting where a pretrained model is optionally adapted through continued domain- specific pretraining, followed by task-specific finetuning. We establish a suite of tasks across multiple domains to study temporal mis- alignment in modern NLP systems. We find stronger effects of temporal misalignment on task performance than have been previously re- ported. We also find that, while temporal adap- tation through continued pretraining can help, these gains are small compared to task-specific finetuning on data from the target time period. Our findings motivate continued research to improve temporal robustness of NLP models. Introduction Changes in the ways a language is used over time are widely attested (Labov, 1994; Altmann et al., 2009; Eisenstein et al., 2014); how these changes will affect NLP systems built from text corpora, and in particular their long-term performance, is not as well understood. This paper focuses on temporal misalignment, i.e., where training and evaluation datasets are drawn from different periods of time. In today’s pretraining-finetuning paradigm, this misalignment can affect a pretrained language model—a situa- tion that has received recent attention (Jaidka et al., 2018; Lazaridou et al., 2021; Peters et al., 2018; Raffel et al., 2020; Röttger and Pierrehumbert, 2021)—or the finetuned task model, or both. We suspect that the effects of temporal misalignment will vary depending on the genre or domain of the task’s text, the nature of that task or application, and the specific time periods. We focus primarily on measuring the extent of temporal misalignment on task performance. We consider eight tasks, each with datasets that span at least five years (§2.4), ranging from summarization to entity typing, a subproblem of entity recognition (Grishman and Borthwick, 1999). Notably, these task datasets span four different domains: social media, scientific articles, news, and reviews. We introduce an easily interpretable metric that summa- rizes the rate at which task performance degrades as function of time. Our research questions are:(Q1) how does temporal misalignment affect downstream tasks over time? (Q2) how does sensitivity to temporal misalign- ment vary with text domain and task? (Q3 )how does temporal misalignment affect lan- guage models across domains and spans of time? (Q4) how effective is temporal adaptation, or ad- ditional pretraining on a target year, in miti- gating temporal misalignment? We find that temporal misalignment affects both language model generalization and task perfor- mance. We find considerable variation in degra- dation across text domains (§3.2) and tasks (§3.1). Over 5 years, classifiers’ F1 score can deteriorate as much as 40 points (political affiliation in Twitter) or as little as 1 point (Yelp review ratings). Two distinct tasks defined on the same domain can show different levels of degradation over time. We explore domain adaptation of a language model, using temporally selected (unannotated) data, as a way to curtail temporal misalignment (Röttger and Pierrehumbert, 2021). We find that this does not offer much benefit, especially relative to performance that can be achieved by finetuning on temporally suitable data (i.e., from the same time period as the test data). We conclude that tem- poral adaptation should not be seen as a substitute for finding temporally aligned labeled data. The evidence and benchmarks we offer motivate careful attention to temporal misalignment in many applications of NLP models, and further research on solutions to this problem. Contributions. To facilitate the study of tempo- ral misalignment phenomenon on downstream ap- plications, we compile a suite of eight diverse tasks across four important language domains. We de- fine an interpretable metric that summarizes tempo- ral misalignment of a model on a task with times- tamped data. Our experiments reveal key factors in how temporal misalignment affects NLP model performance. Conclusion Changes in language use over time, and how lan- guage relates to other quantities of interest in NLP applications, has clear effects on the performance of those applications. We have explored how tem- poral misalignment between training data—both data used to train LMs and annotated data used to finetune them—affects performance across a range of NLP tasks and domains, taking advantage of datasets where timestamps are available. We com- pile these datasets as a benchmark for future re- search as well. We also introduced a summary metric, TD score, that makes it easier to compare models in terms of their temporal misalignment. Our experiments revealed considerable variation in temporal degradation accross tasks, more so than found in previous studies (Röttger and Pierrehum- bert, 2021). These findings motivate continued study of temporal misalignment across applica- tions of NLP, its consideration in benchmark evalu- ations,13 and vigilance on the part of practitioners able to monitor live system performance over time. Notably, we observed that continued training of LMs on temporally aligned data does not have much effect, motivating further research to find effective temporal adaptation methods that are less costly than ongoing collection of annotated/labeled datasets over time.