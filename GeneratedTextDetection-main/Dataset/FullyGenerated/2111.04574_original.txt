Abstract We present the first openly available cor- pus for detecting depression in Thai. Our corpus is compiled by expert verified cases of depression in several online blogs. We experiment with two different LSTM based models and two different BERT based mod- els. We achieve a 77.53% accuracy with a Thai BERT model in detecting depres- sion. This establishes a good baseline for future researcher on the same corpus. Fur- thermore, we identify a need for Thai em- beddings that have been trained on a more varied corpus than Wikipedia. Our cor- pus, code and trained models have been released openly on Zenodo. Introduction Depression is a significant public-health prob- lem and one of the leading causes of disease burden worldwide. In 2010, Depression was ranked as the sixth leading causes of disability- adjusted life years in South-East Asia (Murray et al., 2012). Globally, more than 264 mil- lion people were affected (James et al., 2018). Depression affects 86 million people in South- East Asia region. At its most severe, depres- sion can lead to suicide, which is the second highest cause of death among 15-29 years old in the region (Sharma, 2017). Depression is a growing problem in Thai- land as the country has a globally high depres- sion rate reaching to 1.7 million people (see Kongsuk et al. 2017). At the same time, peo- ple dealing with depression struggle to find help (see Dundon 2006), or worse yet, they might receive insuﬀicient treatment (Lotrakul and Saipanish, 2009). A worst-case scenario, depression can lead to suicide. Depression is one of the leading causes of suicide in Thai adolescents (Sukhawaha and Arunpongpaisal, 2017). In 2019, suicide rate of Thai people aged between 20-29 was 667 persons and aged between 30-39 was 959 persons which was the highest numbers of all aged groups1. While Thai is not what we would call low- resourced (see Hämäläinen 2021), there are currently no datasets available for detecting depression in the Thai language. We present a new dataset based on blog posts and verified cases of depressed bloggers. Furthermore, we establish a baseline for further research on the topic in the Thai language. We have published the dataset, code and models presented in this paper openly on Zenodo2. Related work In this section we present some of the recent related work in more detail. While there have been several digital humanities driven research efforts in better understanding depression in text (Girard and Cohn, 2015; Abd Yusof et al., 2017; Loveys et al., 2018), we only focus on work that has been conducted on depression detection. There are also several approaches to depression detection in other languages (Pi- rina and Çöltekin, 2018; Husseini Orabi et al., 2018; Song et al., 2018). An onset of major depression can be char- acterized and detected by investigating social media data such as Twitter (De Choudhury et al., 2013b,a). By exploring a large cor- pus of Twitter posting by using crowd-sourcing methodology. An SVM classifier was built by divesting a variety of social media measures such as social activity, egonetwork, style, user engagement, emotion, and language. The clas- sifier model predicted with high accuracy (70% and 73 % respectively) predicting ahead of the reported onset of depression and whether or not a post on Twitter could be depressive- indicative postings. Depression levels is de- tected by a proposed social media depression index. Computerized analysis of various kind of texts related to depression reveals signals of psychiatric disorders. Depression is measured by self-reported symptoms (Rude et al., 2002), by clinical interview (Rude et al., 2003). The Scrambled Sentences Test (SST) (Wenzlaff, 1993) was used to measure of cognitive process- ing bias of a large sample of college students. Negative cognitive processing biases in resolv- ing ambiguous verbal information can predict depression and subsequent depression symp- toms. Depression has been detected automatically before in Thai Facebook users (Katchapakirin et al., 2018). The authors train several models on RapidMiner. The models rely on metadata for activity on Facebook such as the number of posts posted on a given week day, the number of day-time and night-time posts and the num- ber of shared posts. The authors did not train the models on text, but rather used numerical features extracted from Facebook posts such as the number of first person pronouns and number of positive words. A screening text-based classification model was also applied to Thai Facebook posts to de- tect depressive disorder (Hemtanon and Kit- tiphattanabawon, 2019). Similarly to the pre- vious approach, the authors apply several tra- ditional machine learning techniques to Thai social media text with pre-extracted features. Kumnunt and Sornil, 2020 present a CNN (convolutional neural network) based approach to depression detection in Thai social media posts. The authors crawl posts tagged with a depression hashtag to build their dataset. Contrary to any of the current work on Thai depression detection, our approach does not deal with social media posts but lengthier blog posts. We also make our dataset available un- like the existing work. Discussion and conclusions In this paper, we have presented a new ex- pert curated dataset for depression detection in Thai blog posts. Our sentence level results are promising and we are sure we can use our models to snowball more depressed blog posts from platforms such as Storylog for further linguistic analysis as a part of our interdisci- plinary research project. Based on our results, we can identify one easy future direction for enhancing the results obtained by our models. Currently, all freely available pretrained Thai embbeddings have been trained on Wikipedia. This is not opti- mal for several reasons, one being the ency- clopedic genre of Wikipedia, the other being the fact that while Wikipedia is written in for- mal ”correct” Thai, blog posts are written in a more colloquial language variety. This means that the vocabulary coverage of Wikipedia data is poor when compared to blog posts. Our blog corpus consists of 21,002 unique tokens while the word2vec model trained on Wikipedia has embeddings for 51,358 words. The blog corpus (training, testing and vali- dation combined) contains 6,488 words that are not present in the word2vec model, this means that around 31% of the words present in our blog depression corpus are simply not in a Wikipedia based model. In the future, it is clear that Thai language calls for openly available models that are trained on a larger and more varied internet corpus than solely on Wikipedia.