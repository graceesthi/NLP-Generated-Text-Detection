ABSTRACT The use of contrastive loss for representation learning has become prominent in computer vision, and it is now getting attention in Natural Language Processing (NLP). Here, we explore the idea of using a batch-softmax contrastive loss when fine-tuning large-scale pre-trained transformer models to learn better task-specific sentence embeddings for pairwise sentence scoring tasks. We introduce and study a number of variations in the calculation of the loss as well as in the overall train- ing procedure; in particular, we find that data shuffling can be quite important. Our experimental results show sizable improvements on a number of datasets and pairwise sentence scoring tasks including classification, ranking, and regression. Finally, we offer detailed analysis and discussion, which should be useful for re- searchers aiming to explore the utility of contrastive loss in NLP. INTRODUCTION Recent years have seen a revolution in Natural Language Processing (NLP) thanks to the advances in machine learning. While a lot of attention has been paid to the architectures, especially for deep learning, there has been less focus on studying loss functions. At the same time, loss functions based on similar or on the same ideas were reinvented multiple times under different names. This can cause difficulties when solving new problems or when designing new experiments based on previous results. To a greater extent, this applies to “universal” loss functions, which can be applied in different machine learning areas and tasks such as Computer Vision (CV), Recommendation Systems, and NLP. An example of such universal loss function is the batch-softmax contrastive (BSC) loss, which we will discuss below. Our contributions can be summarized as follows: We study the use of a batch-softmax contrastive loss for fine-tuning large-scale transformers to learn better task-specific sentence embeddings for pairwise sentence scoring tasks. We introduce and study a number of variations in the calculation of the loss such as sym- metrization, incorporating labeled negatives, aligning scores on the similarity matrix diago- nal, normalizing over the batch axis, as well as in the overall training procedure, e.g., shuf- fling, trainable temperature, and sequential pre-training. We demonstrate sizable improvements for a number of pairwise sentences scoring tasks such as classification, ranking, and regression. We offer detailed analysis and discussion, which would be useful for future research. RELATED WORK The contrastive loss was proposed by Hadsell et al. (2006) as metric learning that contrasts Eu- clidean distances between embeddings of samples from one class and between samples from dif- ferent classes. Weinberger et al. (2006) suggested the triplet loss, which is based on a similar idea, but uses triplets (anchor, positive, negative), and aims for the difference between the distances for (anchor, positive) and for (anchor, negative) to be larger than a margin. N-pair loss was presented as a generalization of the contrastive and the triplet losses as a way to solve the problem of extensive construction of hard negative pairs and triplets (Sohn, 2016). To this end, a batch of N pairs of examples from N different classes is sampled, and the first element in each pair is considered to be an anchor. Thus, for each anchor, there are one positive and N ́ 1 negative pairs. The loss contrasts the distances simultaneously using the softmax function over dot- product similarities. The approach was used successfully in computer vision (CV) tasks. The same method of Multiple Negative Ranking for training Dot-Product Scoring Models was applied to rank- ing natural language responses to emails (Henderson et al., 2017), where the loss uses labeled pairs. A similar idea, called Negative Sharing, was used to reduce the computational cost when training recommender systems (Chen et al., 2017). Wu et al. (2018) presented an approach with N -pairs like logic, as a Non-Parametric Softmax Classifier, replacing the weights in the softmax with embeddings of samples from such classes. It was also proposed to use L2 normalization and temperature. Yang et al. (2018) proposed to use Multiple Negative Ranking to train general sentence representations on data from Reddit and SNLI. Logeswaran & Lee (2018) presented a Quick-Thoughts approach to learn sentence embeddings, which constructs batches of contiguous sets of sentences, and for each sentence, contrasts the next sentence in the text and all other candidates. A lot of subsequent work has focused on maximizing Mutual Information (MI). Oord et al. (2018) presented a loss function based on Noise-Contrastive Estimation, called InfoNCE. It models the “similarity” function that estimates the MI between the target (future) and the context (present) signals, and maximizes the MI between temporally nearby signals. If this “similarity” function expresses the dot-product between embeddings, the InfoNCE loss is equivalent to the N-pair loss up to some constants. It was also shown that InfoNCE is equivalent to the Mutual Information Neural Estimator (MINE) up to a constant (Belghazi et al., 2018), whose minimization maximizes a lower bound on MI. Deep InfoMax (DIM) (Hjelm et al., 2019) improves MINE, and can be modified to incorporate some autoregression as InfoNCE. However, Tschannen et al. (2020) pointed out that the effectiveness of loss functions such as DIM and InfoNCE might be primarily connected not to deep metric learning but rather to MI. The idea gained a lot of popularity in Computer Vision with the advent of SimCLR (a Simple frame- work for Contrastive Learning of visual Representations), which introduced NT-Xent (normalized temperature-scaled cross-entropy loss) (Chen et al., 2020). It uses self-supervised learning, where augmentations of the same image are considered as positive examples and augmentations of dif- ferent images are used as negative examples. Thus, the task is as follows: for each example in a batch, find its paired positive augmentation. Here, the N-pairs loss is modified with a temperature parameter and with an L2 normalization of embeddings to the unit hypersphere. The loss was further extended for supervised learning as SupCon loss (Khosla et al., 2020), which aggregates all positive examples (from the same class) in the softmax numerator. Subsequently, these losses were introduced to the field of Natural Language Processing (NLP). Gunel et al. (2020) combined the SupCon loss with the cross-entropy loss and obtained state-of-the- art results for several downstream NLP tasks using RoBERTa. Giorgi et al. (2020) and Fang & Xie (2020) used NT-Xent to pre-train Transformers, considering spans sampled from the same document and sentences augmented with back-translation as positive examples. Luo et al. (2020) proposed to use NT-Xent in a self-supervised setting to learn noise-invariant sequence representations, where sentences augmented with masking were considered as positive examples. Finally, Gao et al. (2021) introduced the SimCLR loss to NLP under the name SimCSE (Simple Contrastive Learning of Sentence Embeddings), where sentences processed by a neural network with dropout served as augmentations of the original sentences. Here, we explore various ways to use a similar loss function for pairwise sentence scoring tasks. While the above-described loss functions have different names, they are all based on similar ideas. Below, we will use the name Batch-Softmax Contrastive (BSC) loss, which we believe reflects the main idea best. In our experiments below, we will use the “modern” variant of the loss: with tem- perature, normalization, and symmetrization components (described in more detail in Section 3.1). These components were not used for NLP in combination before. We further introduce a number of novel and important modifications in the definition of the loss and in the training procedure, which make it more efficient, and we show that using the resulting loss yields better task-specific sentence embeddings for pairwise sentence scoring tasks. CONCLUSION AND FUTURE WORK We explored the idea of using a batch-softmax contrastive loss for fine-tuning large-scale pre-trained transformers to learn better task-specific sentence embeddings for pairwise sentence scoring tasks. We introduced and studied a number of variations in the calculation of the loss as well as in the overall training procedure. Our experimental results have shown sizable improvements on a number of datasets and pairwise sentence scoring tasks including ranking, classification, and regression. In future work, we want to explore new variations of the loss, and to gain better understanding of when to use which variation. We further plan experiments with a larger set of NLP tasks.