Abstract Definitions are essential for term understand- ing. Recently, there is an increasing interest in extracting and generating definitions of terms automatically. However, existing approaches for this task are either extractive or abstractive– definitions are either extracted from a corpus or generated by a language generation model. In this paper, we propose to combine extrac- tion and generation for definition modeling: first extract self- and correlative definitional in- formation of target terms from the Web and then generate the final definitions by incorpo- rating the extracted definitional information. Experiments demonstrate our framework can generate high-quality definitions for technical terms and outperform state-of-the-art models for definition modeling significantly. Introduction Definitions of terms are highly summarized sen- tences that capture the main characteristics of terms. To understand a term, the most straightforward way is to read its definition. Recently, acquiring defini- tions of terms automatically has aroused increasing interest. There are two main approaches: extractive, corresponding to definition extraction, where defi- nitions of terms are extracted from existing corpora automatically (Anke and Schockaert, 2018; Veyseh et al., 2020; Kang et al., 2020); and abstractive, corresponding to definition generation, where defi- nitions of terms are generated conditioned with the target terms and their contexts (Noraset et al., 2017; Gadetsky et al., 2018; Bevilacqua et al., 2020). However, both extractive and abstractive ap- proaches have their limitations. For instance, ex- tracting high-quality definitions for terms would be difficult due to the incompleteness and low quality of data sources. Generating definitions for terms would be challenging if terms are technical (e.g., need domain knowledge to understand) while the contexts cannot provide sufficient knowledge. Con- sequently, existing models perform poorly on tech- nical terms. In our human evaluation, we find most definitions produced by the state-of-the-art abstrac- tive model contain wrong information. Fortunately, definition extraction and definition generation can complement each other. On one hand, definition generator has the potentials to help the extractor by refining and synthesizing the ex- tracted definitions; on the other hand, definition ex- tractor can retrieve useful definitional information as knowledge for the generator to produce defini- tions. However, surprisingly, existing works are either extractive or abstractive, even do not connect and compare them. Therefore, in this work, we propose to combine definition extraction and definition generation for definition modeling. We achieve this by introduc- ing a framework consisting of two processes: ex- traction, where definitional information of terms are extracted from the Web; and generation, where the final definitions are generated with the help of the extracted definitional information. We build models for extraction and generation based on Pre-Trained Language Models (Devlin et al., 2019; Lewis et al., 2020; Brown et al., 2020). Specifically, for extraction, we propose a BERT- based definition extractor to extract self-definitional information (i.e., definitional sentences of the target term). We also suggest that related terms can help defining the target term and leverage Wikipedia as the external knowledge source to retrieve correl- ative definitional information (i.e., definitions of related terms). For generation, we design a BART- based definition generator to produce the final defi- nition by incorporating the extracted knowledge. From another perspective, we propose to reform the problem of definition modeling, which is pre- viously mainly defined as generating definitions of terms conditioned with a target term and a given context. Instead, we restudy this problem as defin- ing terms with extracted knowledge. This setting is in line with human behavior: to understand a term, compared to reading the given sentence it is used in, it is more straightforward and helpful to search and read its relevant content on the Internet. Our framework for definition modeling is simple and flexible that can easily be further expanded by leveraging more advanced language models. Ex- perimental results demonstrate our simple model outperforms state-of-the-art models significantly (e.g., BLEU score from 8.76 to 22.66, human an- notated score from 2.34 to 4.04), with several inter- esting findings: 1) for computer science terms, our extractive model can achieve performance compa- rable to (even better than) state-of-the-art abstrac- tive models; 2) both self- and correlative defini- tional information are significant to define a term; 3) the quality of definitions generated by our best model is high, while the state-of-the-art models suffer severely from hallucinations, i.e., generating irrelevant or contradicted facts. Our contributions are summarized as follows:  As far as we know, we are the first to connect and combine definition extraction and definition generation– a simple idea that can significantly improve the performance of definition modeling.  We propose to restudy definition modeling as generating definitions of terms with extracted knowledge. We design a novel framework for definition modeling by incorporating both self- and correlative definitional information of terms. We publish two datasets for technical terms, along with definitions of ~75,600 computer science terms generated by our model. Related Work Definition Extraction. Existing works for defini- tion extraction can be roughly divided into three cat- egories: 1) rule-based, which extracts definitions with defined linguistic rules and templates (Klavans and Muresan, 2001; Cui et al., 2004; Fahmi and Bouma, 2006); 2) machine learning-based, which extracts definitions by statistical machine learning with carefully designed features (Westerhout, 2009; Jin et al., 2013); 3) deep learning-based, the state- of-the-art approach for definition extraction, which is based on deep learning models such as CNN, LSTM, and BERT (Anke and Schockaert, 2018; Veyseh et al., 2020; Kang et al., 2020). Definition Generation. Definition generation, or definition modeling, was first introduced in (No- raset et al., 2017), which aims to generate defi- nitions of words with word embeddings. Later works on definition generation put more empha- sis on generating definitions of words/phrases with given contexts (Gadetsky et al., 2018; Ishiwatari et al., 2019; Washio et al., 2019; Li et al., 2020;Reid et al., 2020; Bevilacqua et al., 2020). There are also recent works on definition modeling for other languages, e.g., Chinese, by incorporating the special properties of the specific language (Yang et al., 2020; Zheng et al., 2021). Conclusion In this paper, we combine extraction and gener- ation for definition modeling. We show that, by incorporating extracted self- and correlative def- initional information, the generator can produce high-quality definitions for technical terms. Ex- perimental results demonstrate the effectiveness of our framework. As future work, we plan to apply our methods to more domains and construct several online domain dictionaries.