Abstract This paper proposes a technique for adding a new source or target language to an existing multilingual NMT model without retraining it on the initial set of languages. It consists in replacing the shared vocabulary with a small language-specific vocabulary and fine-tuning the new embeddings on the new languageÃÂ¢ÃÂÃÂs parallel data. Some additional language-specific components may be trained to improve performance (e.g., Transformer layers or adapter modules). Because the parameters of the original model are not modified, its performance on the initial languages does not degrade. We show on two sets of experiments (small-scale on TED Talks, and largescale on ParaCrawl) that this approach performs as well or better as the more costly alternatives; and that it has excellent zero-shot performance: training on English-centric data is enough to translate between the new language and any of the initial languages. Introduction Multilingual Neural Machine Translation models are trained on multilingual data to translate from and/or into multiple languages (Firat et al., 2016; Johnson et al., 2017). Multilingual NMT is a compelling approach in production, as one only needs to train, deploy and maintain one model (instead of 2 ÃÂ N ones, where N is the number of languages). It has also been shown to improve MT quality for low-resource languages (at the cost of a slight degradation for high-resource languages) and it can allow translation between languages that have no aligned data (Ã¢ÂÂzero-shot translationÃ¢ÂÂ). However, such models can be costly to train, as they usually involve larger architectures and large datasets. Moreover, because they are trained jointly on all the languages, they require to know in advance the full set of languages. Adding a new language to an existing model usually means retraining the model on the full multilingual dataset. Naively fine-tuning the original model on the new language is not an option because of vocabulary mismatch (the shared vocabulary needs to be modified to include the new language) and catastrophic forgetting (the model will quickly forget how to translate in the other languages). In this paper, we study the problem of multilingual NMT incremental training or continual learning and propose a novel way to efficiently add a new source or target language. Some desirable properties of an incremental training method are: Ã¢ÂÂ¢ No degradation on the existing language pairs; Ã¢ÂÂ¢ Efficient training (e.g., no re-training on the existing language pairs); Ã¢ÂÂ¢ Minimal amount of added parameters: the approach should scale to many languages and the model fit on a single GPU; Ã¢ÂÂ¢ Minimal degradation in inference speed; Ã¢ÂÂ¢ Good zero-shot performance: when training with X-EN (or EN-X) data, where X is a new language, we would like the model to be able to translate from X to any known language Y (resp. from Y to X). We propose a novel technique for incrementally adding a new source or target language, which consists in substituting the shared embedding matrix with a language-specific embedding matrix, which is fine-tuned on the new languageÃÂ¢ÃÂÃÂs data only while freezing the other parameters of the model. In some cases (e.g., when the new language is on the target size), a small number of additional parameters (e.g., adapter modules) have to be trained to match the performance of the re-training baseline. We perform two sets of experiments, with a 20-language Transformer Base trained on TED Talks, and a 20- language Transformer Big (with deep encoder and shallow decoder) trained on ParaCrawl; and show that this approach is fast and parameter-efficient and that it performs as well or better as the more costly alternatives. We propose a new technique for incrementally training multilingual NMT models on a new source or target language. It consists in creating a new monolingual BPE vocabulary for that language, substituting the shared embedding matrix with language-specific embeddings, and training those while freezing the other model parameters. At inference, translating in any of the initial languages is done by using the initial shared embeddings, and translating in the new language is done by using the newly trained embeddings. This approach does not change performance on the initial languages as the initial parameters are kept aside and not modified. For new source languages, it can achieve close performance to the more costly and less flexible bilingual and re-training baselines. For new target languages, this technique can be combined with language-specific parameters (finetuned Transformer layers or adapter modules) to match baseline performance at a small parameter cost. We validate this technique onsets of experiments: small-scale on TED Talks and largescale on ParaCrawl; and show that it is compatible with two architectures: Transformer Base 6-6 and Big 12-2. We also show that incremental training on data aligned with English is enough to learn to translate between the new language and any of the new languages.