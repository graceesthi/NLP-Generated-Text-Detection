Abstract Despite great success on many machine learn- ing tasks, deep neural networks are still vulner- able to adversarial samples. While gradient- based adversarial attack methods are well- explored in the field of computer vision, it is impractical to directly apply them in natu- ral language processing due to the discrete na- ture of text. To bridge this gap, we propose a general framework to adapt existing gradient- based method to craft textual adversarial sam- ples. In this framework, gradient-based contin- uous perturbations are added to the embedding layer and are amplified in the forward propa- gation process. Then the final perturbed latent representations are decoded with a mask lan- guage model head to obtain potential adversar- ial samples. In this paper, we instantiate our framework with Textual Projected Gradient Descent (TPGD). We conduct comprehensive experiments to evaluate our framework by per- forming transfer black-box attacks on BERT, RoBERTa and ALBERT on three benchmark datasets. Experimental results demonstrate our method achieves an overall better performance and produces more fluent and grammatical ad- versarial samples compared to strong baseline methods. All the code and data will be made public. Introduction Deep learning has achieved great success in various domains, such as computer vision (CV) (He et al., 2016;Chi et al., 2019), natural language processing (NLP) (Vaswani et al., 2017; Devlin et al., 2019) and speech recognition ( Chiu et al., 2018; Park et al., 2019). However, the powerful neural net- works are still vulnerable to adversarial samples, crafted by adding small and human-imperceptible perturbations to the inputs. (Szegedy et al., 2014; Goodfellow et al., 2015). In the field of CV, numerous adversarial attack methods have been proposed to evaluate the robust- ness of DNNs (Papernot et al., 2016a, Madry et al., 2019), and corresponding defence methods are also well-explored (Papernot et al., 2016c; Ross and Doshi-Velez, 2018). Adversarial attacks on images are defined as an optimization problem of maximiz- ing the loss function of model on specific samples, which can be approximated by the gradient ascent algorithms. However, textual adversarial attack is more chal- lenging due to the discrete and non-differentiable nature of the text space. And the methods that directly employ the gradients to craft adversarial samples is not applicable in NLP. Current practices of textual adversarial attacks that employ first-order approximation to find substitute words are less ef- fective for one-off searching and can violate the local linearization assumption (Cheng et al., 2019; Behjati et al., 2019; Xu and Du, 2020). To bridge this gap, we propose a general frame- work to adapt existing gradient-based method to NLP. We successfully obtain high-quality adversar- ial samples by conducting gradient-based search. Specifically, we employ the gradient of loss function with respect to the embeddings of input tokens to make perturbations on token embeddings rather than on the original text, thus transforming the prob- lem of searching for adversarial samples from the discrete text space to the continuous differentiable embedding space. This provides the basis for apply- ing gradient-based methods investigated in CV to craft textual adversarial samples. In this paper, we adapt PGD (Madry et al., 2019) algorithm within our framework to perform textual adversarial at- tacks, denoted as TPGD. We iteratively generate small perturbations following the gradient informa- tion and add them to the embedding layer. Then the forward propagation process will amplify the perturbations. The natural question is how can we transform the perturbed token embeddings back to the dis- crete text? Although there exist some works explor- ing the feasibility of directly perturbing token em- beddings (Miyato et al. (2017); Sato et al. (2018); Cheng et al. (2019); Behjati et al. (2019)), None of them succeed to map all points in the embed- ding space back to words, thus failing to generate human-imperceptible adversarial samples. How- ever, recent work observes that the mask language model (MLM) head can reconstruct input sentences from their hidden states with high accuracy, even after models have been fine-tuned on specific tasks (Kao et al., 2021). Inspired by this, we employ a MLM head to decode the perturbed latent represen- tations. With the extensive linguistic knowledge of MLM head, the coherence and grammaticality of adversarial samples can be guaranteed. We conduct comprehensive experiments to eval- uate the effectiveness of our method by performing transfer black-box adversarial attacks, where only the final decisions of victim models are accessi- ble, against three victim models on three bench- mark datasets. We use a local pre-trained language model to construct potential adversarial samples and then query the victim models for decisions. Ex- perimental results demonstrate the effectiveness of our framework and TPGD algorithm. Specifically, TPGD significantly outperforms all baseline meth- ods in terms of attack success rate, and produces more fluent and grammatical adversarial examples. To summarize, the main contributions of this paper are as follows: We propose a general gradient-based textual ad- versarial attack framework based on continuous perturbations, bridging the gap between CV and NLP on the study of adversarial attacks. Com- mon gradient-based attack methods in CV can be easily adapted to NLP within our framework. We propose a novel adversarial attack method called TPGD within our framework. We employ a local model to construct adversarial samples by iteratively perturbing its embedding layer through the gradient information, and accumu- lating these small perturbations to search for potential adversarial samples. We successfully handle the challenge of black- box attack where only the decisions of models are accessible, which is rarely investigated in NLP. Conclusion and Future Works In this paper, we propose a general framework to adapt gradient-based adversarial attack methods investigated in CV to NLP. In our framework, the problem of searching textual adversarial samples is transformed from the discrete text space to the embedding layer, where continuous gradient-based perturbations can be directly added to. The pertur- bations will be amplified in the forward propaga- tion process. Then a MLM head is employed to decode the final perturbed latent representations. With its extensive linguistic knowledge, the coher- ence and grammaticality of the adversary samples can be guaranteed. We instantiate our framework with TPGD, including the iterative perturbation process and the reconstruction process, to perform decision-based black-box attack. We conduct ex- haustive experiments to evaluate our framework and TPGD algorithm. Experimental results show the superiority of our method, especially in terms of attack success rate and adversarial samples quality. In the future, we will adapt other gradient-based methods in CV with our framework and explore to improve models’ robustness through adversarial training.