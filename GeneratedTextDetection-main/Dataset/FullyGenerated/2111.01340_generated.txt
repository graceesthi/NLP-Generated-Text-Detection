Abstract Natural language understanding (NLU) has made massive progress driven by large benchmarks, paired with research on trans- fer learning to broaden its impact. Bench- marks are dominated by a small set of fre- quent phenomena, leaving a long tail of in- frequent phenomena underrepresented. In this work, we reflect on the question: have transfer learning methods sufficiently ad- dressed performance of benchmark-trained models on the long tail? Since benchmarks do not list included/excluded phenomena, we conceptualize the long tail using macro- level dimensions such as underrepresented genres, topics, etc. We assess trends in transfer learning research through a qualita- tive meta-analysis of 100 representative pa- pers on transfer learning for NLU. Our anal- ysis asks three questions: (i) Which long tail dimensions do transfer learning studies tar- get? (ii) Which properties help adaptation methods improve performance on the long tail? (iii) Which methodological gaps have greatest negative impact on long tail per- formance? Our answers to these questions highlight major avenues for future research in transfer learning for the long tail. Lastly, we present a case study comparing the per- formance of various adaptation methods on clinical narratives to show how systemati- cally conducted meta-experiments can pro- vide insights that enable us to make progress along these future avenues. Introduction Ã¢ÂÂThere is a growing consensus that significant, rapid progress can be made in both text under- standing and spoken language understanding by investigating those phenomena that occur most centrally in naturally occurring unconstrained materials and by attempting to automatically ex- tract information about language from very large corpora.Ã¢ÂÂ (Marcus et al., 1993) Since the creation of the Penn Treebank, using shared benchmarks to measure and drive progress in model development has been instrumental for accumulation of knowledge in the field of natural language processing, and has become a dominant practice. Ideally, we would like shared benchmark corpora to be diverse and comprehensive, which can be addressed at two levels: (i) macro-level di- mensions such as language, genre, topic, etc., and (ii) micro-level dimensions such as specific lan- guage phenomena. However, diversity and com- prehensiveness is not straightforward to achieve. According to ZipfÃ¢ÂÂs law, many micro-level lan- guage phenomena naturally occur infrequently and will be relegated to the long tail, except in cases of intentional over-sampling. Moreover, the advantages of restricting community focus to a specific set of benchmark corpora and limita- tions in resources lead to portions of the macro- level space being under-explored, which can fur- ther cause certain micro-level phenomena to be under-represented. For example, since most popu- lar coreference benchmarks focus on English nar- ratives, they do not contain many instances of zero anaphora, a phenomenon quite common in other languages (e.g., Japanese, Chinese). In such sit- uations, model performance on benchmark cor- pora may not be truly reflective of expected perfor- mance on micro-level long tail phenomena, raising questions about the ability of state-of-the-art mod- els to generalize to the long tail. Most benchmarks do not explicitly catalogue the list of micro-level language phenomena that are included or excluded in the sample, which makes it non-trivial to construct a list of long tail micro-level language phenomena. Hence, we formalize an alternate conceptualization of the long tail: undersampled portions of the macro- level space that can be treated as proxies for long tail micro-level phenomena. These undersampled long tail macro-level dimensions highlight gaps and present potential new challenging directions for the field. Therefore, periodically taking stock of research to identify long tail macro-level di- mensions can help in highlighting opportunities for progress that have not yet been tackled. This idea has been gaining prominence recently; for ex- ample, Joshi et al. (2020) survey languages stud- ied by NLP papers, providing statistical support for the existence of a macro-level long tail of low- resource languages. In this work, our goal is to attempt to charac- terize the macro-level long tail in NLU and ef- forts that have tried to address it from research on transfer learning. Large benchmarks have driven much of the recent methodological progress on NLU (Bowman et al., 2015; Rajpurkar et al., 2016; McCann et al., 2018; Talmor et al., 2019; Wang et al., 2019c,b), but the generalization abilities of benchmark-trained modelsthe long tail have been unclear. In tandem, the NLP community has been successfully developing transfer learn- ing methods to improve generalization of models trained on NLU benchmarks (Ruder et al., 2019). The goal of transfer learning research is to tackle the macro-level long tail in NLU, leading to the question: how far has transfer learning addressed performance of benchmark-trained models on the macro-level long tail, and where do we still fall behind? Probing further, we perform a qualitative meta- analysis of a representative sample of 100 pa- pers on domain adaptation and transfer learning for NLU. We sample these papers based on citation counts and publication venues (ÃÂ§2.1), and docu- ment 7 facets for each paper such as tasks and do- mains studied, adaptation settings evaluated, etc. (ÃÂ§2.2). Adaptation methods proposed (or applied) are documented using a hierarchical categoriza- tion described in ÃÂ§2.3, which we develop by ex- tending the hierarchy from Ramponi and Plank (2020). With this information, our analysis fo- cuses on three questions: Q1: What long tail dimensions do transfer learning studies target? Here di- mensions include tasks, domains, languages and adaptation settings covered in transfer learning research. Q2: Which properties help adaptation meth- ods improve performance on the long tail? Q3: Which methodological gaps have great- est negative impact on long tail performance? The rest of the paper presents thorough answers to these questions, laying out avenues for future research on transfer learning that more effectively address the long tail. We also present a case study to demonstrate that our meta- analysis framework can be use to systematically design and conduct experiments that provide in- sights that enable us to make progress along these avenues. Conclusion This work presents a qualitative meta-analysis of 100 representative papers on domain adaptation and transfer learning for NLU, with the aim of understanding performance of adaptation methods on the long tail. Through this analysis, we assess current trends and highlight methodological gaps that we consider to be major avenues for future re- search in transfer learning for the long tail. We ob- serve that current research has a tendency to side- line certain types of tasks, languages, domains, and adaptation settings, indicating that long tail coverage is far from comprehensive. We also iden- tify two properties that help long tail performance: (i) incorporating source-target domain distance, and (ii) incorporating a nuanced view of domain variation. Additionally, we identify three major gaps that must be addressed to improve long tail performance: (i) combining adaptation methods, (ii) incorporating extra-linguistic knowledge and (iii) application to data-scarce adaptation settings. Finally, we demonstrate the utility of our meta- analysis framework and observations in guiding the design of systematic meta-experiments to ad- dress prevailing open questions by conducting a systematic evaluation of popular adaptation meth- ods for high-expertise domains in a data-scarce setting. This case study reveals interesting insights about the adaptation methods evaluated and shows that significant progress can be made towards de- veloping a better understanding of adaptation for the long tail by conducting such experiments.