Abstract Language Models are the underpin of all modern Natural Language Processing (NLP) tasks. The introduction of the Transformers architecture has contributed significantly into making Language Modeling very effective across many NLP task, leading to significant advancements in the field. However, Transformers come with a big computational cost, which grows quadratically with respect to the input length. This presents a challenge as to understand long texts requires a lot of context. In this paper, we propose a Fine-Tuning framework, named CoreLM, that extends the architecture of current Pretrained Language Models so that they incorporate explicit entity information. By introducing entity representations, we make available information outside the contextual space of the model, which results in a better Language Model for a fraction of the computational cost. We implement our approach using GPT2 and compare the finetuned model to the original. Our proposed model achieves a lower Perplexity in GUMBY and LAMBDADA datasets when compared to GPT2 and a fine-tuned version of GPT2 without any changes. We also compare the models’ performance in terms of Accuracy in LAMBADA and Children’s Book Test, with and without the use of model-created coreference annotations. Introduction Language Models (LMs) have seen significant improvements in performance due to the Transformers architecture (Vaswani et al., 2017). The resulting Pretrained Language Models (PLMs) such as BERT (Devlin et al., 2019), GPT2 (Radford et al., 2019) and XLNet (Yang et al., 2019) have contributed to significant advancements in many Natural Language Processing (NLP) tasks. PLMs, regardless of their training objective and methodology, aim to learn contextualized text representations. As such, the available context during training has a key role in the models performance. The quadratic computation complexity in the attention mechanism of the Transformer architecture, in terms of input sequence length, has been a limiting factor to the amount of contextual information the models can have at each step. To make this architecture more efficient, a plethora of approaches have been introduced (Kitaev et al., 2020; Beltagy et al., 2020; Child et al., 2019, inter alia), aiming to lower the computational complexity without sacrificing performance. Tay et al. (2020) summarizes these approaches and categorizes them based on type of attention mechanism in their survey. Yet, even with linear complexity, physical resources will always limit the amount of contextual information that can be handled simultaneously. In terms of language, entities represent a natural way to tie words together through large pieces of discource. In NLP, Coreference Resolution is the task that aims to identify and group these entity mentions together, when they refer to the same real world entity (Stylianou and Vlahavas, 2021). Therefore, Coreference Resolution presents a natural way to link the context that we are currently handling with distant information, far outside the capabilities model architectures and current hardware resources. However, while this information is present in the text, it is usually not annotated and when annotated are very sparse and in small quantities making it extremely difficult to train large LMs (Kunz and Hardmeier, 2019). In this paper we present a framework to effectively use coreference annotations to further finetune large PLMs, increasing their performance far more than just by fine-tuning on the same data. By using large PLMs we take advantage of existing resources that are expensive to reproduce and come with a big environmental cost (Strubell et al., 2019). What is more, fine-tuning takes advantage of the massive amount of data that essentially initialize the model, making it possible to introduce new capabilities to models with small amounts of annotated data. In our approach, we use GPT2 as our base model and extend its architecture with the addition of a new Entity-Gating layer that handles entity annotations along with a gating mechanism that handles information flow between the base model and the Entity-Gating layer. As such, our approach uses entity representations when they are available through both training and inference, without imposing any constraints to the model’s functionality. For our experiments, we compare the performance of GPT2, post and pre fine-tuning, with and without our changes in a series of relative tasks. Furthermore, since most coreference annotated datasets are either very small or hard to acquire, we use GUMBY (Gessler et al., 2020) as our fine-tuning dataset which is a model annotated corpus. In addition, by using noisy annotations we aim to show the resilience of our approach to noise and the universality of our framework. Our results highlight the effects of our framework in language modeling, modeling long-range dependencies, and in specific word types where our fine-tuned model achieves better performance than GPT2. Conclusions and Future Work In this paper we presented CoreLM, a modular Fine-Tuning framework for PLMs to exploit modelcreated coreference annotations in order to create better mention representations and an overall better LM. In our experiments we showcased a performance increase when evaluating in a zero-shot setting, compared to the similarly fine-tuned model, even when the fine-tuning corpus did not generalize well to the end tasks. Our analysis shows that coreference annotations play a significant role in both Fine-Tuning and in downstream task performance, with correct annotations leading to better performance when used. In addition, our work helps in adding a new frontier to Coreference Resolution through the effective use of coreference annotations in Language Modeling. In this paper we showcased the effects of coreference annotation even when the information is within the context window of the model. Using coreference annotations can further lead to the decrease of the required context window and boost approaches like Shortformer (Press et al., 2020), leading to better and more efficient LMs. In the future we aim to create a more efficient approach to LM through the use of both intralinguistic (Coreference) and extra-linguistic (KG) features. Undeniably, KGs provide a means for structured, high quality information that cannot be found in a single text. We believe that an information fusion from coreference annotation and graph nodes, along with short context window will not be computationally prohibitive and lead in better, information rich, LMs.