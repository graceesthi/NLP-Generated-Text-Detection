Abstract Large language models can produce fluent di- alogue but often hallucinate factual inaccura- cies. While retrieval-augmented models help alleviate this issue, they still face a difficult challenge of both reasoning to provide correct knowledge and generating conversation simul- taneously. In this work, we propose a modu- lar model, Knowledge to Response (K2R), for incorporating knowledge into conversational agents, which breaks down this problem into two easier steps. K2R first generates a knowl- edge sequence, given a dialogue context, as an intermediate step. After this “reasoning step”, the model then attends to its own generated knowledge sequence, as well as the dialogue context, to produce a final response. In de- tailed experiments, we find that such a model hallucinates less in knowledge-grounded dia- logue tasks, and has advantages in terms of interpretability and modularity. In particular, it can be used to fuse QA and dialogue sys- tems together to enable dialogue agents to give knowledgeable answers, or QA models to give conversational responses in a zero-shot setting. Introduction To be regarded as successful, a conversational agent needs to generate utterances that are both knowl- edgeable and factually correct, as well as being conversationally appropriate, fluent and engaging. The pursuit of this goal has led to ever bigger mod- els that store a large amount of knowledge in their parameters (Roller et al., 2021; Adiwardana et al., 2020; Zhang et al., 2020). However, hallucination – wherein a model generates factually inaccurate statements – has remained a problem no matter the size of the model (Shuster et al., 2021a). Recent advances in neural retrieval models have made some inroads into this problem (Lee et al., 2019; Lewis et al., 2020b; Shuster et al., 2021a; Komeili et al., 2021) by generating responses based on both the dialogue context and by learning to re- trieve documents containing relevant knowledge. However, the conversational setting is challenging because these models are required to perform mul- tiple duties all in one shot: to perform reasoning over the returned documents and dialogue history, find the relevant knowledge, and then finally com- bine this into a conversational form pertinent to the dialogue. Perhaps due to this complexity, it has been observed that failure cases include incorporat- ing parts of multiple documents into one factually incorrect response, or failure to include knowledge at all and reverting instead to a generic response using the dialogue context only. In this work, we instead propose to decompose this difficult problem into two easier steps. Specif- ically, by first generating pertinent intermediate knowledge explicitly and then, conditioned on this prediction, generating the dialogue response. We call this model Knowledge to Response (K2R). Using this modular design, we can train and evaluate the reasoning performance of the model indepen- dently from its conversational abilities, increasing the interpretability of our model’s output. This also allows us to plug external knowledge into dialogue systems without any requirement for retraining, for example, from question answering systems. The dialogue response model’s task reduces to incorpo- rating the predicted knowledge in an engaging and context-fitting conversational response. We conduct extensive experiments across multi- ple tasks and datasets. We find that our K2R model effectively improves correct knowledge-utilization and decreases hallucination (Shuster et al., 2021a) in knowledge-grounded dialogue (Dinan et al., 2019). In open-domain dialogue, the K2R model improves the performance on automatic metrics compared to its seq2seq counterpart, along with the additional benefits of increased interpretabil- ity of the model’s output and the possibility for knowledge injections. The modular design allows us to fuse state-of-the-art pre-trained QA models – without any fine-tuning – with dialogue models to generate answers that humans judge as both more knowledgeable and engaging. Our modular system also outperforms multi-tasking approaches. Related Work Improving dialogue systems by increasing their knowledgeability has been tried in several different ways: from integrating knowledge bases (Zhu et al., 2017; Liu et al., 2018; Wang et al., 2020), to larger models that are pre-trained on more data (Roller et al., 2021; Adiwardana et al., 2020; Zhang et al., 2020), and recent neural retrieval models (Shuster et al., 2021a; Thulke et al., 2021). Knowledge- grounded open-domain dialogue datasets (Dinan et al., 2019; Komeili et al., 2021; Zhou et al., 2018;Gopalakrishnan et al., 2019) foster the research and development of knowledge-aware generative dialogue models. A known issue of such mod- els, referred to as “hallucination”, is that they mix up facts and generate factually inaccurate state- ments. Shuster et al. (2021a) try to alleviate hallu- cination by using recent advancements in retrieval- augmented generative models developed for open- domain QA tasks (Lewis et al., 2020b; Izacard and Grave, 2021). These methods still hallucinate to some degree, and their predictions (and hence er- rors) are not easily interpretable. There is also recent work in the space of modular or intermediate generation components for text generation. The approach of text modular networks promises more interpretable answers to multi-hop questions (Khot et al., 2020; Jiang and Bansal, 2019; Gupta et al., 2020). Khot et al. (2020) learn a generative model that decomposes the task in the language of existing QA models for HotpotQA (Yang et al., 2018) and DROP (Dua et al., 2019). Herzig et al. (2021) solve text-to-SQL tasks with intermediate text representations. For storytelling, hierarchical generation procedures have been pro- posed (Fan et al., 2018). In reinforcement learning settings, generating natural language has been used as an intermediate planning step (Sharma et al., 2021; Hu et al., 2019), and in particular in goal- oriented dialogue (Yarats and Lewis, 2018) and open-domain QA (Adolphs et al., 2021) as well. For summarization tasks, the work of Baziotis et al. (2019) proposes an intermediate autoencoder latent representation. Similarly, West et al. (2019) ap- ply the information bottleneck principle to find an intermediate compressed sentence that can best pre- dict the next sentence. For knowledge-grounded dialogue, an approach using internet search can also be seen as a modular intermediate step, where the search query is first generated (Komeili et al., 2021). In that sense retrieval-based QA has also been seen as a modular technique in many studies (Chen et al., 2017; Yan et al., 2019). Previous work has also explored the intersection of QA and dialogue models from multiple different angles. The DREAM dataset (Sun et al., 2019) con- sists of multiple-choice questions about a conver- sation. Yang and Choi (2019) propose a question- answering task based on dialogue histories of the TV show Friends. The QuAC (Choi et al., 2018) and CoQA (Reddy et al., 2019) datasets are de- signed to have the questions asked in the conver- sational flow, with possibly, multiple follow-ups. However, while these datasets require a model to understand a dialogue’s history, the target re- sponses are short-form answers. Therefore, these tasks do not train a dialogue model that gener- ates an engaging, conversationally appropriate re- sponse; instead, they result in a QA model that understands dialogue-structured context. Conclusion In this work, we presented K2R: a modular ap- proach for knowledge-based dialogue models. We showed that by decomposing the knowledge step and response generation into explicit sequence-to- sequence subtasks, we could improve dialogue sys- tems by incorporating knowledge or turning short QA model answers into an appropriate conversa- tional form. In detailed experiments, we showed that this modular system helps with hallucination in knowledge-grounded dialogue, is rated by humans as more knowledgeable and engaging when answer- ing questions, and improves generation metrics on open-domain dialogue. Furthermore, it allows for more interpretable results and supports knowledge injection. Future work should continue to investi- gate methods with modular reasoning steps to help in difficult language tasks.