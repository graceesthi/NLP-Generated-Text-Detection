Abstract Multi-hop QA requires the machine to answer complex questions through finding multiple clues and reasoning, and provide explanatory evidence to demonstrate the machineÃÂ¢ÃÂÃÂs reason- ing process. We propose Relation Extractor- Reader and Comparator (RERC), a three-stage framework based on complex question decom- position. The Relation Extractor decomposes the complex question, and then the Reader an- swers the sub-questions in turn, and finally the Comparator performs numerical compar- ison and summarizes all to get the final an- swer, where the entire process itself constitutes a complete reasoning evidence path. In the 2WikiMultiHopQA dataset, our RERC model has achieved the state-of-the-art performance, with a winning joint F1 score of 53.58 on the leaderboard. All indicators of our RERC are close to human performance, with only 1.95 behind the human level in F1 score of support fact. At the same time, the evidence path pro- vided by our RERC framework has excellent readability and faithfulness. Introduction Multi-hop QA is an important and challenging task in natural language processing (NLP), which re- quires complex reasoning over several paragraphs to reach the final answer and explanatory evidence to demonstrate the reasoning process. Many high- quality multi-hop QA datasets have been intro- duced recently, such as HotpotQA (Yang et al., 2018), ComplexWebQuestions (Talmor and Berant, 2018), QAngaroo WikiHop (Welbl et al., 2018), R4C (Inoue et al., 2020), 2WikiMultiHopQA (Ho et al., 2021), etc. These high-quality multi-hop QA datasets pro- mote many multi-hop QA models (Song et al., 2018; Ding et al., 2019; Xiao et al., 2019; Nishida et al., 2019; Tu et al., 2019; Cao et al., 2019), most of which are end-to-end models based on graph structure or graph neural network (VelicÃÂkovic ÃÂ et al.,2018). Although these works have good perfor- mances in many tasks, they also have some limita- tions to address. First of all, the internal reasoning mechanism of previous end-to-end QA models is a black-box, which usually use an additional dis- criminator to judge whether a sentence is a clue sentence, such as DFGN (Xiao et al., 2019). There is no evidence to show that such additional discrim- inators are strongly correlated with the reasoning results of the end-to-end model, which means not faithful. Secondly, although graph structure is help- ful to multi-hop reasoning in theory, but recent work (Shao et al., 2020) shows that the existing graph neural network is only a special attention mechanism (Bahdanau et al., 2014), and itÃ¢ÂÂs not necessary for multi-hop QA, with the experiments that better results can be achieved by using only transformer network instead of graph neural net- work, as long as the same additional adjacency matrix information is provided. We observed that human reasoning about com- plex questions is not accomplished overnight and itÃ¢ÂÂs usually divided into the steps of question de- composition, answering sub-questions, summariz- ing and comparing. For example, for the complex question, "whose candidate will get more votes in the 2020 U.S. election, Democrats and Repub- licans?" People will not think about the whole question, but firstly decompose the complex ques- tion. Realizing that the subject of the question is "Democrats and Republicans", and the question is about "candidates" and "number of votes", peo- ple can answer those sub-questions progressivelyÃ¢ÂÂ "who is the Democratic candidate?" and "how many votes does ANS get?" The same thinking process was performed for another question sub- ject, "Republican Party". Finally, the two votes were compared to obtain the answer to the entire complex question. Inspired by the way humans answer complex multi-hop questions, in this work we abandoned the end-to-end model structure, but imitated the human reasoning mechanism to propose a three- stage Relation Extractor-Reader and Comparator (RERC) model1. We first build a Relation Extrac- tor, which can automatically extract the subject and key relations of the question from the com- plex unstructured textual representation. For the Relation Extractor,use two different structures, one is classification-type (CRERC), where the evi- dence relation information in the dataset is used as prior knowledge, and the question text is mapped to question relations through the classifier; the other is span-type (SRERC), where the type of question relations is unrestricted, and the Relation Extractor decomposes the complex question, and then the Reader an- swers the sub-questions in turn, and finally the Comparator performs numerical compar- ison and summarizes all to get the final an- swer, where the entire process itself constitutes a complete reasoning evidence path. In the 2WikiMultiHopQA dataset, our RERC model has achieved the state-of-the-art performance, with a winning joint F1 score of 53.58 on the leaderboard. All indicators of our RERC are close to human performance, with only 1.95 behind the human level in F1 score of support fact. At the same time, the evidence path pro- vided by our RERC framework has excellent readability and faithfulness. Introduction Multi-hop QA is an important and challenging task in natural language processing (NLP), which re- quires complex reasoning over several paragraphs to reach the final answer and explanatory evidence to demonstrate the reasoning process. Many high- quality multi-hop QA datasets have been intro- duced recently, such as HotpotQA (Yang et al., 2018), ComplexWebQuestions (Talmor and Berant, 2018), QAngaroo WikiHop (Welbl et al., 2018), R4C (Inoue et al., 2020), 2WikiMultiHopQA (Ho et al., 2021), etc. These high-quality multi-hop QA datasets pro- mote many multi-hop QA models (Song et al., 2018; Ding et al., 2019; Xiao et al., 2019; Nishida et al., 2019; Tu et al., 2019; Cao et al., 2019), most of which are end-to-end models based on graph structure or graph neural network (VelicÃÂkovic ÃÂ et al.,2018). Although these works have good perfor- mances in many tasks, they also have some limita- tions to address. 