Abstract This paper presents a deep neural architec- ture, for Natural Language Sentence Match- ing (NLSM) by adding a deep recursive en- coder to BERT so called BERT with Deep Recursive Encoder (BERT-DRE). Our anal- ysis of model behavior shows that BERT still does not capture the full complexity of text, so a deep recursive encoder is ap- plied on top of BERT. Three Bi-LSTM lay- ers with residual connection are used to de- sign a recursive encoder and an attention module is used on top of this encoder. To obtain the final vector, a pooling layer con- sisting of average and maximum pooling is used. We experiment our model on four benchmarks, SNLI, FarsTail, MultiNLI, Sc- iTail, and a novel Persian religious ques- tions dataset. This paper focuses on im- proving the BERT results in the NLSM task. In this regard, comparisons between BERT- DRE and BERT are conducted, and it is shown that in all cases, BERT-DRE outper- forms only BERT. The BERT algorithm on the religious dataset achieved an accuracy of 89.70%, and BERT-DRE architectures im- proved to 90.29% using the same dataset. Introduction Natural Language Sentence Matching (NLSM) is a fundamental task in Natural Language Pro- cessing(NLP) designed to identify similarities in terms of semantic and conceptual content between two input sentence. A wide range NLP tasks, such as Natural Language Inference(NLI), Para- phrase Identification, Question Answering, and Machine Comprehension (MC), are implemented with NLSM. The NLI, also known as Recog- nizing Textual Entailment (RTE), seeks to deter- mine whether a hypothesis can be deduced from a premise. This requires examining the semantic similarity between the hypothesis and the premise. In Paraphrase Identification, the goal is to deter- mine if two texts are paraphrases or not (Lan and Xu). In Machine Comprehension, the model must match the sentence structure of the passage to the question, pointing out where the answer is located (Lan and Xu). Question answering systems rely on NLSM in two ways: (1) question retrieval, (2) the answer selection. In question retrieval, the matching between the query and existing ques- tions, and in answer selection, the matching be- tween the query and existing answers are deter- mined. In fact, answer selection is used to discover the relationship between the query and answer and ranks all possible answers. These NLSM- based question answering systems can be applied in question-answering forums. Many websites use these question-answering forums and they can use these systems to answer their user’s questions. There are so many repetitive questions in these fo- rums that NLSM-based question answering sys- tems can use FAQs to answer users questions. Users will see the answers similar questions, when a new question is asked in the forum. Using this NLSM-based question answering systems user questions will be answered faster (Singh Tomar et al.). Multiple datasets can be accessed in NLSM, including Stanford Natural Language In- ference (SNLI) (Bowman et al., 2015), Multi- Genre Natural Language Inference (MultiNLI) (Williams et al., 2018), FarsTail (Amirkhani et al., 2021), SciTail (Khot et al., 2018), and more. One of the challenges when using these datasets is that in many existing samples the two input texts have the same and many similar words. In fact, both texts are very similar in appearance but express different meanings, or conversely, two texts have many different words, but the nature and meaning of both questions are very similar. The objective of NLSM models is to predict a category or scale value for a pair of input texts, which indicates their similarity or relationship. To achieve this goal, NLSM models generally use two main steps: (1) designing a model to obtain a proper representation of whatever text will be ana- lyzed so that it can extract semantic features from it. (2) By using the representation obtained from texts, a matching mechanism can be developed to extract complex interactions. In the current NLSM field, deep neural net- works are the preferred approach. To determine semantic features and relationships among sen- tences, researchers use convolutional and recur- rent neural networks. The structure of convo- lutional neural networks (CNNs) (LeCun et al., 1999) make them very capable of extracting local features. To this end, CNN have been extensively used in various areas of natural language process- ing. By considering natural language texts as a sequence of words, it is crucial to extract and ana- lyze temporal features. However Input sequences can be analyzed using recurrent neural networks with their unique structures to extract temporal features. The Long Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) a variant of RNN’s that is capable of extracting long-term de- pendency in appropriate way. It has achieved very good results in many NLP tasks by using the LSTM structure like named entity recognition (Cho et al., 2020), question-answering (Othman et al., 2019), emoji prediction (Tavan et al., 2020) etc. In order to extract text features, deep learn- ing networks require a lot of labeled data, which is one of the challenges of using deep learning, es- pecially in natural language processing. To extract the best textual features today, pre-trained lan- guage models are used, which have been trained using a large amount of data. The advantage of these pre-trained language models is that they do not need labeled data to be trained, so they can be trained on a large amount of unlabeled data and then fine-tuned them in downstream tasks. A tech- nique known as transfer learning is one of the best ways to resolve the challenges of natural language processing in low-resource languages today. In this paper, we used the following steps to de- sign a model based on NLSM principles identify- ing the semantic similarity of the input text pair: 1. Collecting Persian Religious question match- ing dataset for training and evaluating the proposed NLSM model. 2. Investigating related models to the field of NLSM. 3. Implementing ERT with Deep Recursive En- coder (BERT-DRE) model by adding recur- sive encoder module to BERT (Devlin et al., 2019). 4. Evaluating BERT-DRE with related models using the introduced religious and benchmark datasets. Dataset annotated for this study includes 18,000 samples, contains two questions, appropriate an- swers, and a match or not-match label for each question pair, which is used for designing a chat- bot. This dataset is collected by crawling reli- gious questions from religious question-answering websites, and then by annotating two similar ques- tions are annotated for each question, and dissim- ilar questions are generated automatically. It was noted our BERT-DRE model, which used the an- notated religious dataset to train and evaluate, was able to achieve an F1-score of 90.27% on the test data, making it the strongest model among the ones studied. Furthermore, in order to better eval- uate the BERT-DRE model, the model was trained using SNLI, MultiNLI, FarsTail, and SciTail and achieved appropriate F1-scores. Related Works Natural language processing studies have focused on NLSM, an essential part of many natural lan- guage tasks. In this regard, various datasets and models have been developed. There have been dif- ferent models developed for NLI, semantic match- ing, and paraphrase identification. Since all of these tasks are similar, the same model can be used today. In most cases, deep neural networks are used for this purpose. To this end, in the following, we shall examine the relevant works. As stated in (Wang et al., 2017), in contrast to previous work performed in text-similarity that adapted the sentence from one direction or used a word-to-word or sentence-to-sentence corre- spondence only, this study used bilateral multi- perspective matching from several perspectives. As a result, one of the main architectural pil- lars is based on Bidirectional LSTM. Furthermore, they examined the mechanism of different meth- ods, such as attention. This interaction between aggregate words is then examined from multiple perspectives. In (Wang et al., 2017) (BiMPM), sentences are matched bidirectionally with a new function calculating similar vectors. This method can result in similarity in multiple ways, which is called multi-perspective cosine similarity. Language objects internal structures and inter- actions of language objects need to be properly modeled to achieve a successful matching algo- rithm. A component of this goal is achieved in (Hu et al., 2014) through combining ideas from neu- ral networks’ implementation in image and audio processing. Besides demonstrating the hierarchi- cal structures of sentences in layers, this model also maintains rich matching patterns on differ- ent levels. The (He et al., 2015) (MPCNN) pre- sented work based on CNN, analyzing multiple perspectives of sentences. CNN first extracts fea- tures for each sentence, then compares representa- tions of the sentences at various levels of granular- ity, and then uses various pooling techniques. Fol- lowing that, multiple similarity metrics are used to compare sentences at multiple levels of granular- ity. The MPCNN model processes each sentence independently, and there is no interactivity until the final Fully-Connected layer, which results in the loss of a great deal of useful information. For this reason, (Cao et al., 2018) and (He et al., 2016) make changes to the MPCNN architecture. In (Cao et al., 2018) has also improved MPCNN, one of which is the use of pre-trained embedding instead of random embedding. In the next step, characters are used to extract features. Finally, an input layer based on attention is added between the embedding and the multi-perspective sentence modeling layers. Attention-based neural network models have been successfully used in an- swer selection, which is an important sub-task of question answering. These models often represent the question using a vector and match it by refer- ring to the candidate answers. However, questions and answers may be interrelated in complex ways that cannot be represented by single-vector repre- sentations. In (Tran and Niedereée, 2018), the idea of using multipurpose attention networks (MANs) is proposed, which aims to discover this complex relationship for ranking question and answer pairs. Unlike previous models, this paper does not turn the question into a single vector, but focuses on different parts of the question from several vectors to show its general meaning, and applies different stages of attention to learning the representations of the candidate answers. The study (Zhao et al., 2019) proposes an inter- active attention model for semantic text matching that uses the global matching matrix to learn repre- sentations for source and target texts during inter- active attention. This model can take a rich repre- sentation of source and target texts and derive an entirely related encoding. In (Chen et al., 2017), a model with a good performance called ESIM has a non-complex structure of LSTM layers and is cited in many later works as a base model. It has been claimed that recursive architecture can achieve good performance in local inference mod- eling and inference composition. In (Tay et al., 2018), a deep learning model named CAFE is presented in which alignment pairs are compared, compressed, and then spread to the upper layers to increase representation learning. Then, the alignment vectors are ex- pressed in scalar features through factorization layers. One of the other models that was able to achieve good accuracy in SNLI and MultiNLI datasets (Yin et al., 2016) is another deep learning model that is intended for modeling sentence pairs with CNNs named ABCNN. In this study, three different models of the combination of convolu- tional layers along with the attention mechanism have been used and the results of these three mod- els have been examined. Based on BERT representation of sentences and using convolution layers and the semantic role la- beler as features added to the model, (Zhang et al., 2020) has achieved good accuracy on MultiNLI and QNLI (Wang et al., 2018) data. Using a com- bination of CNN, attention mechanism, and resid- ual connection structures for inference, a deep learning method was proposed in (Yang et al., 2019) named RE2. The authors regard attention to pointwise features, previous alignments, and con- textual features as three success principles. This model has the advantage of being fast in calculat- ing model inferences. In comparison with other existing models, this model increases speed by six times and uses fewer parameters. In the (Kim et al., 2018) a deep learning model is proposed using the LSTM structure named DRCN. In this paper, the alignment module is used to extract the relationship between the words of the first and second text sequences. Residual connections are also used to transfer the extracted features to each layer. The embedding layer in this model includes trainable word embedding, non-trainable word embedding, character embedding, and ex- act match feature. In this model, due to the use of residual connections, the dimensions of the ex- tracted feature along with the network increase. In order to reduce the feature dimensions in this model, an autoencoder is used. Conclusion In this paper, we proposed a novel architecture by adding recursive encoder module with BERT. The proposed model uses BERT as an embedding layer and On top of the embedding layer, three bidi- rectional LSTM were used densely to represent the input text semantically. In order to pay more or less emphasis on different words, an attention module is applied to the outputs of LSTMs. This makes the semantic representations more informa- tive. These semantic representations are passed to a pooling layer consisting average and max pooling To make the resulting feature maps more robust to features’ positional changes. To show the potential of this architecture, we evaluated it in four benchmark datasets and got competitive results. The evaluation results in our annotated data show that the accuracy of proposed model is very acceptable. Our empirical results suggest that our proposed model improves the performance on some NLP benchmarks (e.g., FarsTail) with the state-of-the-art pre-trained models (e.g., BERT). This has been developed for the Persian/English language but it could be easily extended to other languages.