Abstract The inception of modeling contextual information using mod- els such as BERT, ELMo, and Flair has significantly im- proved representation learning for words. It has also given SOTA results in almost every NLP task — Machine Trans- lation, Text Summarization and Named Entity Recognition, to name a few. In this work, in addition to using these domi- nant context-aware representations, we propose a Knowledge Aware Representation Learning (KARL) Network for Named Entity Recognition (NER). We discuss the challenges of using existing methods in incorporating world knowledge for NER and show how our proposed methods could be leveraged to overcome those challenges. KARL is based on a Transformer Encoder that utilizes large knowledge bases represented as fact triplets, converts them to a graph context, and extracts essential entity information residing inside to generate contextualized triplet representation for feature augmentation. Experimental results show that the augmentation done using KARL can considerably boost the performance of our NER system and achieve significantly better results than existing approaches in the literature on three publicly available NER datasets, namely CoNLL 2003, CoNLL++, and OntoNotes v5. We also observe better generalization and application to a real-world setting from KARL on unseen entities. Introduction Named Entity Recognition (NER) is the task of locating and classifying named entities in a given piece of text into pre- defined entity categories such as Person (PER), Location (LOC), Organisation (ORG), etc. NER is considered an es- sential preprocessing step that can benefit many downstream applications in Natural Language Processing (NLP), such as Machine Translation (Babych and Hartley 2003), Informa- tion Retrieval (Antony and G S 2015) and Text Classification (Armour, Japkowicz, and Matwin 2005). Over the past few years, Deep Learning has been the key to solving not only NER but many other NLP applications (Le et al. 2018; Kouris, Alexandridis, and Stafylopatis 2019). On the downside, these models also demand a lot of well- structured and annotated data for their training. This restricts the applicability of trained models to a real-world scenario as the model’s behavior and predictions become very specific to the type of data they are trained on. To conquer this, many studies have recently evolved that focus on building models that can incorporate world knowledge for enhanced modeling and inference on the task at hand, such as He et al. (2020) for NER, Denk and Peleteiro Ramallo (2020) for Representation Learning and Kim et al. (2015) for Dependency Parsing, etc. Although recent works in literature have successfully in- corporated world knowledge for Sequence Labeling (He et al. 2020), they come with certain limitations, which we dis- cuss ahead. First, as words in a language can be polysemous (Lin et al. 2002), entities and relations in a knowledge graph can be polysemous too (Xiao, Huang, and Zhu 2016). To introduce Knowledge Graph Embeddings (KGEs), we no- ticed that previously proposed approaches have primarily used pre-trained static embeddings obtained from extensive sources such as Wikidata. KGEs in these models fundamen- tally relies on the assumption that the tail entity is a linear transformation of the head entity and the relation, making them non-contextualized in nature. Second, we noticed that prior work only considered head-entity and relation embed- ding to get the knowledge graph embedding and ignored the tail-entity of the triplet completely. Dropping the tail entity entirely could lead to a potential loss of information. We observed that in addition to carrying information about the triplet itself, the head-relation-tail also helps in understanding and extracting implicit relationships existing between entities across triplets. Therefore, the model must know where the head and the relation are leaning towards to achieve accurate embedding estimation. The final limitation lies in applying a Recurrent architecture to obtain KGEs, introducing time inef- ficiency and a high computation cost (Annervaz, Chowdhury, and Dukkipati 2018). To further understand the importance and our motivation behind using world knowledge for NER, consider a couple of examples mentioned below. A: Google announced the launch of Maps. B: Pichai announced the launch of Maps. In the training phase, the significant contextual overlap between sentences can confuse the model in labeling the named entities correctly. The model is likely to memorize sentence templates rather than learn to predict correct entity labels, leading to misclassifications. Also, suppose we trained our NER model on an out-of-domain NER dataset. In that case, the model would have hardly received any information from the training set that ”Google” is an organization and ”Pichai” indicates a Person. A: Berlin died in Season 2 of Money Heist. B: Messi saved Barcelona with an equalizer. In the first sentence above, ”Berlin” refers to a person, whereas in the second sentence, ”Barcelona” refers to an organization. There are reasonable chances of misclassifi- cation in these two sentences because of a high probability of training data missing such nuance differences in all the possible entity tags for a named entity. From the examples mentioned above, we can infer that for the model to be aware of such subtle differences, we should provide it with the ability to look up relevant details from a re- liable source. Therefore, world knowledge can open the gates for the model to access such information and learn details about entities that it might never come across in the train- ing data. In addition to this, with access to structured world knowledge, far better applicability to a real-world setting can be expected. Setting these points as our objective, in this work, we pro- pose Knowledge Aware Representational Learning Network for Named Entity Recognition using Transformer (KARL- Trans-NER), which 1. Encodes the entities and relations existing in a knowledge base using a self-attention network to obtain Knowledge Graph Embeddings (KGEs). The embeddings thus ob- tained are dynamic and fully contextualized in nature. 2. Takes the encoded contextualized representations for enti- ties and relations and generates a knowledge-aware repre- sentation for words. The representation obtained, which we also call ”Global Representation” for words, can be augmented with the other underlying features to boost the NER model’s performance. 3. Generates sentence embeddings using BERT by fusing task-specific information through NER tag embeddings. 4. And lastly, relies on a Transformer as its context encoder incorporating direction-aware, distance-aware, and un- scaled attention for enhanced encoder representation learn- ing. To verify the effectiveness of our proposed model, we conduct our experiments on three publicly available datasets for NER. These are CoNLL 2003 (Sang and Meulder 2003), CoNLL++ (Wang et al. 2019) and OntoNotes v5 (Pradhan et al. 2013). Experimental results show that the global embed- dings generated for every word using KARL, when used for feature augmentation, can result in significant performance gains of over 0.35-0.5 F1 on all the three NER datasets. Also, to validate the model’s generalizability and applicability in a real-world setting, we generate the model’s prediction on random texts taken from the web. Results suggest that in- corporating world knowledge enables the model to make accurate predictions for every entity in the sentence. Related work The research community in NER moved from approaches us- ing character and word representations (Yao et al. 2015; Zhou et al. 2017; Kuru, Can, and Yuret 2016) to sentence-level con- textual representations (Yang, Zhang, and Dong 2017; Zhang, Liu, and Song 2018), and recently to document-level rep- resentations as proposed by Qian et al. (2018) and Akbik, Blythe, and Vollgraf (2018). Expanding the scope of embed- dings from character and word level to document level has shown significant improvements in the results for many NLP tasks, including NER (Luo, Xiao, and Zhao 2019). To expand the scope further, researchers have explored external knowl- edge bases to learn facts existing in the universe that may not be present in the training data (Annervaz, Chowdhury, and Dukkipati 2018; He et al. 2020). Incorporating information present in Knowledge Graph is an emerging research topic in NLP. While some methods focus on graph structure encoding (Lin et al. 2015; Das et al. 2017), others focus on learning entity-relation embeddings (Wang et al. 2020a; Jiang, Wang, and Wang 2019). Zhong et al. (2015) proposed an alignment model for jointly embedding a knowledge base and a text corpus that achieved better or comparable performance on four NLP tasks: link prediction, triplet classification, relational fact ex- traction, and analogical reasoning. Xiao, Huang, and Zhu (2016) proposed a generative embedding model, TransG, which can discover the latent semantics of a relation and leverage a mixture of related components for generating em- bedding. They also reported substantial improvements over the state-of-the-art baselines on the task of link prediction. Lukovnikov et al. (2017) presented a neural network to an- swer simple questions over large-scale knowledge graphs using a hierarchical word and character-level question en- coder. Annervaz, Chowdhury, and Dukkipati (2018) lever- aged world knowledge in training task-specific models and proposes a novel convolution-based architecture to reduce the attention space over entities and relations. It outperformed other models on text classification and natural language in- ference tasks. Despite producing state-of-the-art results in many NLP tasks, Knowledge Graphs are relatively unexplored for NER. He et al. (2020) introduced a Knowledge-Graph Augmented Word Representation (KAWR). The proposed model encoded the prior knowledge of entities from an external knowledge base into the representation. Though KAWR performed better than its benchmark BERT (Devlin et al. 2018), the model underperformed compared to the SOTA models for NER. Conclusion and Future Work This work proposed a novel world knowledge augmentation technique that leveraged large knowledge bases represented as fact triplets and successfully extracted relevant informa- tion for word-level augmentation. The model was trained and tested in an NER setting. Experimental results showed that knowledge level representation learning outperformed most NER systems in literature and made the model highly applicable to a real-world scenario by accurately predicting entities in random pieces of text. Since we augmented features at the word level, we be- lieve our method could facilitate many other NLP tasks, such as Chunking, Word Sense Disambiguation, Question An- swering, etc. Therefore, as future work, we plan to test the applicability of the proposed methods on other NLP tasks as well. Our intuition says that any system can leverage the pro- posed system as a general knowledge representation learning tool. Moreover, being among the very few works in this di- rection, we see an ample scope of improvement. For instance, the Knowledge Graph Embedding model was trained sepa- rately on a Masked Language Modelling task, and then the trained model was used on the task at hand. This restricted the model from interacting and learning from the task at hand, NER in our case. We believe that a technique to incorporate and train the NER model with the knowledge representation module can be more beneficial. Another improvement that could be made lies in the entity shortlisting step. Although the technique is quite reliable, it does not consider any se- mantic information about the entities. Different entities in a knowledge base can be highly correlated to each other and yet have different names. Therefore, we plan to improve the entity shortlisting technique further for more accurate and robust shortlisting.