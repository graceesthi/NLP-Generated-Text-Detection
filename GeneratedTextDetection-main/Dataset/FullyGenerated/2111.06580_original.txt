Abstract Across many data domains, co-occurrence statis- tics about the joint appearance of objects are powerfully informative. By transforming unsu- pervised learning problems into decompositions of co-occurrence statistics, spectral algorithms provide transparent and efficient algorithms for posterior inference such as latent topic analysis and community detection. As object vocabularies grow, however, it becomes rapidly more expensive to store and run inference algorithms on co-occur- rence statistics. Rectifying co-occurrence, the key process to uphold model assumptions, becomes increasingly more vital in the presence of rare terms, but current techniques cannot scale to large vocabularies. We propose novel methods that si- multaneously compress and rectify co-occurrence statistics, scaling gracefully with the size of vo- cabulary and the dimension of latent space. We also present new algorithms learning latent vari- ables from the compressed statistics, and verify that our methods perform comparably to previous approaches on both textual and non-textual data. Introduction Understanding the underlying geometry of noisy and com- plex data is a fundamental problem of unsupervised learning. Probabilistic models explain data generation processes in terms of low-dimensional latent variables. Inferring a poste- rior distribution for these latent variables provides us with a compact representation for various exploratory analyses and downstream tasks (Bengio et al., 2013). However, exact inference is often intractable due to entangled interactions between the latent variables (Blei et al., 2003; Airoldi et al., 2008; A. Erosheva, 2003; Pritchard et al., 2000). Variational inference transforms the posterior approximation into an optimization problem over simpler distributions with in- dependent parameters (Jordan et al., 1999; Wainwright & Jordan, 2008; Blei et al., 2017), while Markov Chain Monte Carlo enables users to sample from the desired posterior distribution (Neal, 1993; Neal et al., 2011; Robert & Casella, 2013). However, these likelihood-based methods require numerous iterations without any guarantee beyond local improvement at each step (Kulesza et al., 2014). When the data consists of collections of discrete objects, co-occurrence statistics summarize interactions between objects. Collaborative filtering learns low-dimensional rep- resentations of individual items, which are useful for recom- mendation systems, by explicitly decomposing the co-oc- currence of items that are jointly consumed by certain users (Lee et al., 2015; Liang et al., 2016). Word-vector mod- els learn low-dimensional embeddings of individual words, which encode useful linguistic biases for neural networks, by implicitly decomposing the co-occurrence of words that appear together in contexts (Pennington et al., 2014; Levy & Goldberg, 2014). If co-occurrence provides a rich enough set of unbiased moments about an underlying generative model, spectral methods can provably learn posterior con- figurations from co-occurrence information alone, without iterating through individual training examples (Arora et al., 2013; Anandkumar et al., 2012c; Hsu et al., 2012; Anand- kumar et al., 2012b). However, two major limitations hinder users from taking advantage of spectral inference based on co-occurrence. First, the second-order co-occurrence matrix already grows quadratically in the number of words (e.g. objects, items, products). Pruning the vocabulary is an option, but for a retailer selling millions of long-tailed products, learning representations of only a subset of the products is inade- quate. Second, inference quality is poor in real data that does not necessarily follow our generative model. Whereas likelihood-based methods (Blei et al., 2003; Airoldi et al., 2008; A. Erosheva, 2003; Pritchard et al., 2000) have an intrinsic capability to fit the data to the model despite their mismatch, sample noise can easily destroy the performance of spectral methods even if the data is synthesized from the model (Kulesza et al., 2014; Lee et al., 2015). Rectification, a process of projecting empirical co-occur- rence onto a manifold consistent with the posterior geometry of the model, provides a principled treatment that improves the performance of spectral inference in the face of model- data mismatch (Lee et al., 2015). Alternating Projection rectification (AP) has been used to rectify the input co- occurrence matrix to the Anchor Word algorithm (AW), a second-order spectral topic model (Lee et al., 2015; 2017; 2019), but running multiple projections dominates overall inference cost even when the vocabulary is small. AP makes the co-occurrence dense as well, exacerbating storage costs when operating on large vocabularies. In this paper, we propose two efficient methods that si- multaneously compress and rectify the co-occurrence ma- trix, Epsilon Non-Negative rectification (ENN) and Prox- imal Alternating Linearized Minimization rectification (PALM). We also propose the Low-rank Anchor Word algorithm (LAW) that learns the latent topics and their cor- relations only from the compressed statistics, guaranteeing the same performance as the original Anchor Word algo- rithm under a certain condition. Our experiments show that applying LAW after ENN learns topics of quality compara- ble to using AW after AP based on the full co-occurrence. We then introduce the Low-Rank Joint Stochastic Matrix Factorization pipeline (LR-JSMF) that first adopts a ran- domized algorithm to construct a low-rank approximation of the full co-occurrence C directly from the raw data; then performs ENN and LAW. While PALM needs access to the full co-occurrence, ENN can work solely with a low-rank initialization, eliminating the burden to ever construct a full co-occurrence matrix. This new pipeline scales to large vocabularies that were previously intractable for spectral inference, and offers a 10x∼100x speedup over previous methods on various textual and non-textual datasets. Note that second-order spectral topic models often rely on the separability assumption that forces at least one anchor word for each topic. This has led to criticism in theory despite their superior performance in practice compared to probabilistic counterparts (Lee et al., 2017) and third-order tensor models (Lee et al., 2019). As most topic models with large vocabularies are proven separable (Ding et al., 2015), we show that our capability to process large vocabularies not only fits for modern datasets, but also alleviates the theoreti- cal limitation. In addition, we also develop a new approach that helps better interpretation of topics by jointly reading characteristic words as well as traditional prominent words. By defining the characteristic words as the terms that are highly associated with each anchor word, we design a graph- based metric that can measure the degree of incoherence in individual topics. To the best of our knowledge, this work makes the first principled attempt to utilize anchor words for quantitative and qualitative interpretations of topics with the prominent words. Given our on-the-fly methods, users are now capable of efficiently understanding latent topics and their correlations from noisy co-occurrence statistics within time and space complexity linear in the size of vocabulary. Conclusion Spectral algorithms provide appealing alternatives for iden- tifying interpretable low-rank subspaces by simple factoriza- tions of higher-order co-occurrence data. But this simplicity is also a weakness: the size of the co-occurrence limit us to small vocabularies, and these methods perform poorly without rectifications that previously suffered quadratic scal- ing. Anchor words are guaranteed to be exclusive to the corresponding topics, but they are rarely used for topic inter- pretations because they are often chosen as too rare terms. We develop a robust and scalable pipeline: Low-Rank Joint Stochastic Matrix Factorization based on our two comple- mentary on-the-fly rectification methods (ENN/PALM) and a sufficiently general low-rank inference algorithm (LAW). These methods simultaneously compress and rectify the co- occurrence from raw data; learn high-quality topics from the compressed matrix factorization; and achieve low-rank non- negative approximations without quadratic blowup. They also provide orders of magnitude speedups for rectification even on small vocabularies. In addition, we verify that us- ing large vocabularies benefits inference quality by better satisfying the separability assumption. It also improves model interpretability by jointly understanding the promi- nent words with the characteristic words, and by measuring our MST-Incoherence metric for individual topics. Given all these new development, we can now learn and evalu- ate useful low-dimensional structures in high-dimensional datasets on laptop-grade hardware, massively increasing the applicability and potential use of the spectral algorithms.