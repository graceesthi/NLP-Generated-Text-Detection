Abstract While pre-trained language models have achieved great success on various natural language understanding tasks, how to effectively leverage them into nonautoregressive generation tasks remains a challenge. To solve this problem, we present a non-autoregressive generation model based on pre-trained transformer models. To bridge the gap between autoregressive and non-autoregressive models, we propose a simple and effective iterative training method called MIx Source and pseudo Target (MIST). Unlike other iterative decoding methods, which sacrifice the inference speed to achieve better performance based on multiple decoding iterations, MIST works in the training stage and has no effect on inference time. Our experiments on three generation benchmarks including question generation, summarization and paraphrase generation, show that the proposed framework achieves the new state-ofthe-art results for fully non-autoregressive models. We also demonstrate that our method can be used to a variety of pre-trained models. For instance, MIST based on the small pre-trained model also obtains comparable performance with seq2seq models. Our code is available at https://github.com/kongds/MIST. Introduction Pre-trained models, like BERT (Devlin et al. 2018), UniLM (Dong et al. 2019) and RoBERTa (Liu et al. 2019) have been widely applied on natural language process tasks by transferring the knowledge learned from large amount of unlabeled corpus to downstream tasks. Some pre-trained models (Dong et al. 2019; Song et al. 2019) also have proven effective for natural language generation tasks in autoregressive generation. However, low latency is required by an increasing number of real-world generating applications, such as online query rewriting in search engines, limiting the use of these autoregressive models (Mohankumar, Begwani, and Singh 2021). Non-autoregressive models (Gu et al. 2017; Gu, Wang, and Zhao 2019; Ghazvininejad et al. 2019) are proposed to reduce latency by predicting whole sequence simultaneously. Compared with autoregressive models, non-autoregressive models can achieve more than ten times speedup at the cost of accuracy loss (Gu et al. 2017). We observe that little progress has been made to directly finetune an off-the-shelf pre-trained encoder model for nonautoregressive generation. How to effectively leverage them into non-autoregressive generation tasks remains a nontrivial problem. To fully exploit the power of pre-trained models, we propose a new iterative decoding method by mixing source and pseudo target called MIST. Current iterative decoding methods such as Mask-Predict (Ghazvininejad et al. 2019) or Levenshtein transformer (Gu, Wang, and Zhao 2019) focus on improving decoding performance by multiple decoding iterations. However, these methods sacrifice the inference speed. MIST is a simple and effective iterative training strategy that works during the training stage and has no effect on inference speed. During the training stage, the model predicts the entire target sequence first, then we treat the generated target sequence as part of the source tokens and feed the source and pseudo target as source into the model. MIST can be regarded as a dynamical data augmentation method in the training stage. Unlike the traditional data augmentation method, which needs to prepare data before training, MIST enables dynamical data augmentation in the training stage. The term Ã¢ÂÂdynamicÃ¢ÂÂ in MIST refers to the fact that as model training progresses, training data will be changed per epoch. For example, MIST can ensure that the quality of the pseudo target matches the current training model and prevent model overfitting with static pseudo targets generated. For increased data, pseudo targets can provide insight into which tokens can be successfully predicted and help models focus on what is done incorrectly. These Pseudo targets also enables the conditional dependence to help convergence. As our experiments show, we evaluate our method on three generation tasks including question generation, summarization and paraphrase generation. Our method achieves significant performance improvements on all tasks with the lower latency than NAT (Gu et al. 2017). To further evaluate the effect of our method, we compare a variety of pre-trained models including BERT (Devlin et al. 2018), UniLM (Dong et al. 2019), and MiniLM (Wang et al. 2020) in non-autoregressive setting. The experiments show that our method achieves consistent gains in different generation tasks and pre-trained models. Our contributions are listed as follows: Ã¢ÂÂ¢ We propose a new paradigm, adopting pre-trained encoder for non-autoregressive generation without modifyingarchitectures. ÃÂ¢ÃÂÃÂ¢ We propose a simple and novel training method to improve the performance. ÃÂ¢ÃÂÃÂ¢ We empirically verify the effectiveness of our method in different generation tasks and pre-trained models. In this paper, we proposed a new paradigm to adopt pretrained encoders to NAR tasks. Our method uses a transformer pre-trained transformer model called MIx Source and pseudo Target (MIST). Unlike other pre-trained models, which sacrifice the inference speed to achieve better performance based on multiple decoding iterations, MIST works in the training stage and has no effect on inference time. Our method uses a transformer pre-trained model to transform a transformer model into a transformer transformer model.