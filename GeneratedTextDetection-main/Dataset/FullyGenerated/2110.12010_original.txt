Abstract Over the recent years, large pretrained lan- guage models (LM) have revolutionized the field of natural language processing (NLP). However, while pretraining on general lan- guage has been shown to work very well for common language, it has been observed that niche language poses problems. In par- ticular, climate-related texts include specific language that common LMs can not repre- sent accurately. We argue that this short- coming of today’s LMs limits the applica- bility of modern NLP to the broad field of text processing of climate-related texts. As a remedy, we propose CLIMATEBERT, a transformer-based language model that is further pretrained on over 1.6 million paragraphs of climate-related texts, crawled from various sources such as common news, research articles, and climate reporting of companies. We find that CLIMATEBERT leads to a 46% improvement on a masked language model objective which, in turn, leads to lowering error rates by 3.57% to 35.71% for various climate-related down- stream tasks like text classification, senti- ment analysis, and fact-checking. Introduction Researchers working on climate change-related topics increasingly use natural language process- ing (NLP) to automatically extract relevant in- formation from textual data. Many studies in this domain apply traditional NLP methods, such as bag-of-words approaches or simple extensions thereof (e.g., Grüning, 2011). However, such anal- yses face considerable limitations, since climate- related wording could vary substantially by source (Kim and Kang, 2018). Deep learning tech- niques that promise higher accuracy are grad- ually replacing these approaches (e.g., Kölbel et al., 2020; Luccioni et al., 2020; Bingler et al., 2021; Callaghan et al., 2021; Wang et al., 2021; Friederich et al., 2021). Indeed, it has been shown in related domains that deep learning in NLP al- lows for impressive results, outperforming tradi- tional methods by large margins (Varini et al., 2020). These deep learning-based approaches make use of language models (LMs), which are trained on large amounts of textual and unlabelled data. This training on unlabelled data is called pretrain- ing and leads to the model learning representa- tions of words and patterns of common language. One of the most prominent language models is called BERT (Bidirectional Encoder Represen- tations from Transformers) (Devlin et al., 2018) with its successors ROBERTA (Liu et al., 2019), Transformer-XL (Dai et al., 2019) and ELEC- TRA (Clark et al., 2020). These models have been trained on huge amounts of text which was crawled from an unprecedented amount of online resources. After the pretraining phase, most LMs are trained on additional tasks, the downstream task. For the downstream tasks, the LM builds on and benefits from the word representations and lan- guage patterns learned in the pretraining phase. The pre-training benefit is especially large on downstream tasks for which the collection of sam- ples is difficult and, thus, the resulting training datasets are small (hundreds or few thousands of samples). Furthermore, it has been shown that a model that was pretrained on the downstream task- specific text exhibits better performance, com- pared to a model that has been pretrained solely on general text (Araci, 2019; Lee et al., 2020). Hence, a straightforward extension to the stan- dard combination of pretraining is the so-called domain-adaptive pretraining (Gururangan et al., 2020). This approach has recently been studied for various tasks and basically comes in the form of pretraining multiple times — in particular pre- training in the language domain of the downstream task, i.e., pretraining (general domain) + domain-adaptive pretraining (downstream domain) + training (downstream task). To date, regardless of the increase in using NLP for climate change related research, a model with climate domain-adaptive pretraining has not been publicly available, yet. Research so far rather re- lied on models pretrained on general language, and fine-tuned on the downstream task. To fill this gap, our contribution is threefold. First, we intro- duce CLIMATEBERT, a state-of-the-art language model that is specifically pretrained on climate- related text corpora of various sources, namely news, corporate disclosures, and scientific articles. This language model is designed to supports re- searchers of various disciplines in obtaining better performing NLP models for a manifold of down- stream tasks in the climate change domain. Sec- ond, to illustrate the strength of CLIMATEBERT, we highlight the performance improvements using CLIMATEBERT on three standard climate-related NLP downstream tasks. Third, to further promote research at the intersection of climate change and NLP, we make the weights of all trained language models publicly available at climatebert.ai. Conclusion We propose CLIMATEBERT, the first language model that was pretrained on a large scale dataset of over 1.6 million climate-related paragraphs. We study various selection strategies to find samples from our corpus which are most helpful for later tasks. Our experiments reveal that our domain- adaptive pretraining leads to considerably lower masked language modeling loss on our climate corpus. We further find that this improvement is also reflected in predictive performance across three essential downstream climate-related NLP tasks: text classification, the analysis of risk and opportunity statements by corporations, and fact- checking climate-related claims.