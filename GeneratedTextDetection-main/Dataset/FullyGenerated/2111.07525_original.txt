Abstract English research articles (RAs) are an essential genre in academia, so the attempts to employ NLP to assist the development of academic writing ability have received considerable attention in the last two decades. However, there has been no study employing feature engineering techniques to investigate the linguistic features of RAs of different academic impacts (i.e., the papers of high/moderate citation times published in the journals of high/moderate impact factors). This study attempts to extract micro-level linguistic features in high- and moderate-impact journal RAs, using feature engineering methods. We extracted 25 highly relevant features from the Corpus of English Journal Articles through feature selection methods. All papers in the corpus deal with COVID-19 medical empirical studies. The selected features were then validated of the classification performance in terms of consistency and accuracy through supervised machine learning methods. Results showed that 24 linguistic features such as the overlapping of content words between adjacent sentences, the use of third- person pronouns, auxiliary verbs, tense, emotional words provide consistent and accurate predictions for journal articles with different academic impacts. Lastly, the random forest model is shown to be the best model to fit the relationship between these 24 features and journal articles with high and moderate impacts. These findings can be used to inform academic writing courses and lay the foundation for developing automatic evaluation systems for L2 graduate students. Introduction Academic writing has been considered one of the most difficult and challenging parts for postgraduates with English as their second language. To improve the English learners’ writing ability, many corpus studies compare L2 learners’ writing with native experts’ writing in terms of specific linguistic features and point out the progress that L2 learners should make (e.g., Lorenz, 1999; Hyland, 2008; Qin, 2014). Native experts’ writing in the above studies usually includes high-impact English journal articles (i.e., those with high citations and published in the journals with high-impact factors) because they can be one of the indicators of high-quality academic products and thus become a good reference for assessing students’ academic writing. However, native experts’ publication has been proofread concerning linguistic use, while students’ academic writings have more or less grammatical problems or spelling errors. Directly comparing such two types of writing is inevitably involved in those issues, but many studies do not mention such issues when introducing student corpora (e.g., Qin, 2014; Lu, 2012; Lu & Deng, 2019), which might cause the neglect of some word analysis. Nevertheless, comparison of publications with different impacts in academic fields can overcome such a problem, after all, both have been reviewed and proofread by experts. Grammatical problems might not be encountered to a bigger extent. At the same time, the most prominent weighted linguistic features in high-impact journal articles by comparison with those of moderate-impact can also provide a reference for the assessemtn of students’ writings. Hence, it could be a new insight for L2 academic writing studies by comparing journal articles with high- and moderate-impacts. As NLP technology has an increasingly close association with English writing classrooms, an increasing number of studies on linguistic use in English research articles introduce machine learning models for analysis. Mover, developed by Anthony and Lashkia (2003), maybe the earliest attempt in automatic analysis of English research article writing. It focuses on the move/step in the abstract sections of academic papers based on the Bag- of-Words model. MAZEA (Multi-label Argumentative Zoning for English Abstracts, Dayrell et al., 2012), also automatically analyzes the move/step of English academic papers’ abstracts. The highest accuracy rate of the software is 69%. Based on SVM (support vector machine), IADE (Intelligent Academic Discourse Evaluator), and Research Writing Tutor (RWT) by the Cotos team of Iowa State University in the United States (Pendar & Cotos, 2008; Babu, 2013; Cotos, 2014) vectorize the move/step in the Bag-of-Words Model. However, current automatic analysis systems on research articles focus more attention on the move/step at the macro-level and neglect the feature extraction at the micro-level of research articles. The importance of micro-level linguistic features have been proved by a series of corpus-based studies of second language acquisition (such as Biber et al., 2011; Biber & Gray, 2016; Chiu et al., 2017; Boutron & Ravaud, 2018; Lei & Yang, 2020; Politzer- Ahles et al., 2020). Liu (2016) and Wang and Liu (2017) also proved the importance of linguistic features at micro-levels in research articles. In their automatic classifier of abstracts in applied linguistics journals, classification accuracy (information abstract and descriptive abstract) has been significantly increased to 78.19% by introducing micro-level language indicators (such as sentence length, predicates, and connectives). The result is much higher than the accuracy rate of automatic analysis systems by Anthony and Lashkia (2003) and Dayrell et al. (2012). Hence, the micro-level linguistic features of high-impact English academic papers should be considered and analyzed to provide a greater number of dimensions for future automatic analysis and feedback systems of research articles. Thanks to the rapid development of NLP technology, many emerging methods can help automatically extract features with bigger weights beyond corpus-based studies, for example, feature engineering. It has been widely applied in fault detection (e.g., Li et al., 2021), image detection (e.g., Cai, Nee, & Loh, 1996; Cai & Chen, 2011), etc., but has not been tried in the micro-level linguistic feature extraction of research articles. Our study will employ this method to extract and validate the selected features to provide consistent and accurate predictions for journal articles with different academic impacts. Conclusion In summary, we used four feature selection methods to reduce the feature dimensions. We verified the selected 24 features, including word imagery, third-person plural pronouns, all sentence actual word overlap, adjacent sentence actual word overlap, the semantic overlap of all sentences, the semantic overlap of adjacent sentences, the ratio of old information to new information in all sentences, the overlap of all sentence nouns, text ease of vocabulary actuality, content word meaning, content word concretization, all sentence argument overlap, adjacent sentence noun overlap, modal verb, focus past, focus present, focus future, articles, numbers, positive emotions, causal vocabulary can better distinguish journal articles with different impacts. Besides, we found that based on these 24 language features, the random forest model can be more consistent and accurate to classify high- and moderate-impact journal articles. The selected linguistic features at the micro-level and machine learning model with better performance can provide reference for future automatic analysis and feedback systems of English research articles.