Abstract Across many data domains, co-occurrence statis- tics about the joint appearance of objects are powerfully informative. By transforming unsu- pervised learning problems into decompositions of co-occurrence statistics, spectral algorithms provide transparent and efficient algorithms for posterior inference such as latent topic analysis and community detection. As object vocabularies grow, however, it becomes rapidly more expensive to store and run inference algorithms on co-occur- rence statistics. Rectifying co-occurrence, the key process to uphold model assumptions, becomes increasingly more vital in the presence of rare terms, but current techniques cannot scale to large vocabularies. We propose novel methods that si- multaneously compress and rectify co-occurrence statistics, scaling gracefully with the size of vo- cabulary and the dimension of latent space. We also present new algorithms learning latent vari- ables from the compressed statistics, and verify that our methods perform comparably to previous approaches on both textual and non-textual data. Introduction Understanding the underlying geometry of noisy and com- plex data is a fundamental problem of unsupervised learning. Probabilistic models explain data generation processes in terms of low-dimensional latent variables. Inferring a poste- rior distribution for these latent variables provides us with a compact representation for various exploratory analyses and downstream tasks (Bengio et al., 2013). However, exact inference is often intractable due to entangled interactions between the latent variables (Blei et al., 2003; Airoldi et al., 2008; A. Erosheva, 2003; Pritchard et al., 2000). Variational inference transforms the posterior approximation into an optimization problem over simpler distributions with in- dependent parameters (Jordan et al., 1999; Wainwright & Jordan, 2008; Blei et al., 2017), while Markov Chain Monte Carlo enables users to sample from the desired posterior distribution (Neal, 1993; Neal et al., 2011; Robert & Casella, 2013). However, these likelihood-based methods require numerous iterations without any guarantee beyond local improvement at each step (Kulesza et al., 2014). When the data consists of collections of discrete objects, co-occurrence statistics summarize interactions between objects. Collaborative filtering learns low-dimensional rep- resentations of individual items, which are useful for recom- mendation systems, by explicitly decomposing the co-oc- currence of items that are jointly consumed by certain users (Lee et al., 2015; Liang et al., 2016). Word-vector mod- els learn low-dimensional embeddings of individual words, which encode useful linguistic biases for neural networks, by implicitly decomposing the co-occurrence of words that appear together in contexts (Pennington et al., 2014; Levy & Goldberg, 2014). If co-occurrence provides a rich enough set of unbiased moments about an underlying generative model, spectral methods can provably learn posterior con- figurations from co-occurrence information alone, without iterating through individual training examples (Arora et al., 2013; Anandkumar et al., 2012c; Hsu et al., 2012; Anand- kumar et al., 2012b). However, two major limitations hinder users from taking advantage of spectral inference based on co-occurrence. First, the second-order co-occurrence matrix already grows quadratically in the number of words (e.g. objects, items, products). Pruning the vocabulary is an option, but for a retailer selling millions of long-tailed products, learning representations of only a subset of the products is inade- quate. Second, inference quality is poor in real data that does not necessarily follow our generative model. Whereas likelihood-based methods (Blei et al., 2003; Airoldi et al., 2008; A. Erosheva, 2003; Pritchard et al., 2000) have an intrinsic capability to fit the data to the model despite their mismatch, sample noise can easily destroy the performance of spectral methods even if the data is synthesized from the model (Kulesza et al., 2014; Lee et al., 2015). Rectification, a process of projecting empirical co-occur- rence onto a manifold consistent with the posterior geometry of the model, provides a principled treatment that improves the performance of spectral inference in the face of model- data mismatch (Lee et al., 2015). Alternating Projection rectification (AP) has been used to rectify the input co- occurrence matrix to the Anchor Word algorithm (AW), a second-order spectral topic model (Leeal., 2015; 2017; 2019), but running multiple projections dominates overall inference cost even when the vocabulary is small. AP makes the co-occurrence dense as well, exacerbating storage costs when operating on large vocabularies. In this paper, we propose two efficient methods that si- multaneously compress and rectify the co-occurrence ma- trix, scaling gracefully with the size of vo- cabulary and the dimension of latent space. We also propose a new approach that learns posterior con- figurations from the compressed statistics, enabling users to better interpret noisy and com- plex data even if the data is synthesized from the compressed statistics. Our experiments show that applying the p- ulary algorithm on a corpus of jade-white words grows the size of a football field even when the words are synthesized from the compressed statistics, rendering the data useless for neural networks. We also present new algorithms learning latent vari- ables from the compressed statistics, and verify that our methods perform comparably to previous approaches on both textual and non-textual data. Introduction Understanding the underlying geometry of noisy and com- plex data is a fundamental problem of unsupervised learning. Probabilistic models explain data generation processes in terms of low-dimensional latent variables. Inferring a poste- rior distribution for these latent variables provides us with a compact representation for various exploratory analyses and downstream tasks (Bengio et al., 2013). However, exact inference is often intractable due to entangled interactions between the latent variables (Blei et al., 2003; Airoldi et al., 2008; A. Erosheva, 2003; Pritchard et al., 2000). Variational inference transforms the posterior approximation into an optimization problem over simpler distributions with in- dependent parameters (Jordan et al., 1999; Wainwright & Jordan, 2008; Blei et al., 2017), while Markov Chain Monte Carlo enables users to sample from the desired posterior distribution (Neal, 1993; Neal et al., 2011; Robert & Casella, 2013). However, these likelihood-based methods require numerous iterations without any guarantee beyond local improvement at each step (Kulesza et al., 2014). When the data consists of collections of discrete objects, co-occurrence statistics summarize interactions between objects. Collaborative filtering learns low-dimensional rep- resentations of individual items, which are useful for recom- mendation systems, by explicitly decomposing the co-oc- currence of items that are jointly consumed by certain users (Lee et al., 2015; Liang et al., 2016). Word-vector mod- els learn low-dimensional embeddings of individual words, which encode useful linguistic biases for neural networks, by implicitly decomposing the co-occurrence of words that appear together in contexts (Pennington et al., 2014; Levy & Goldberg, 2014). If co-occurrence provides a rich enough set of unbiased moments about an underlying generative model, spectral methods can provably learn posterior con- figurations from co-occurrence information alone, without iterating through individual training examples (Arora et al., 2013; Anandkumar et al., 2012c; Hsu et al., 2012; Anand- kumar et al., 2012b). However, two major limitations hinder users from taking advantage of spectral inference based on co-occurrence. First, the second-order co-occurrence matrix already grows quadratically in the number of words (e.g. objects, items, products). Pruning the vocabulary is an option, but for a retailer selling millions of long-tailed products, learning representations of only a subset of the products is inade- quate. Second, inference quality is poor in real data that does not necessarily follow our generative model. Whereas likelihood-based methods (Blei et al., 2003; Airoldi et al., 2008; A. Erosheva, 2003; Pritchard et al., 2000) have an intrinsic capability to fit the data to the model despite their mismatch, sample noise can easily destroy the performance of spectral methods even if the data is synthesized from the model (Kulesza et al., 2014; Lee et al., 2015). Rectification, a process of projecting empirical co-occur- rence onto a manifold consistent with the posterior geometry of the model, provides a principled treatment that improves the performance of spectral inference in the face of model- data mismatch (Lee et al., 2015).