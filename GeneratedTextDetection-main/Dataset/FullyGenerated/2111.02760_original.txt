Abstract: In this paper we present the first system in Spanish capable of answering questions about medicines for human use, called MeQA (Medicines Question Answering), a project created by the Spanish Agency for Medicines and Health Products (AEMPS, for its acronym in Spanish). Online services that offer medical help have proliferated considerably, mainly due to the current pandemic situation due to COVID-19. For example, websites such as Doctoralia 1 , Savia 2 , or SaludOnNet 3 , offer Doctor Answers type consultations, in which patients or users can send questions to doctors and specialists, and receive an answer in less than 24 hours. Many of the questions received are related to medicines for human use, and most can be answered through the leaflets. Therefore, a system such as MeQA capable of answering these types of questions automatically could alleviate the burden on these websites, and it would be of great use to such patients. Keywords: MeQA, AEMPS, Question Answering.  Introduction The leaflets for medicinal products for human use include their complete composition and instructions for their administration, use, and storage. Adverse effects, their interactions and contraindications are also specified. In addition, the text is written clearly, and they have to pass a readability test 4 so that the number of possible lexical, syntactic, or semantic errors is very low. These characteristics make leaflets a relatively easy resource to process using Natural Language Processing (NLP) techniques. This article presents the description of a project of the AEMPS, for the realization of a system capable of answering questions in relation to medicines for human use, called Medicines Question Answering, MeQA. Systems capable of answering questions posed by users (Question Answering systems, QA) were born around 1960 (Phillips, 1960), and are among the first systems with some intelligence to be developed with computers. A medicine QA system implies that it must be able to answer questions whose answers can be found in the medicine leaflets. Therefore, if the information about which the question is asked is not found in the universe of leaflets, the system should indicate that the answer has not been found, although (perhaps) it does exist. For example, when faced with a question such as ¿El ibuprofeno está contraindicado para los hipertensos? (Is ibuprofen contraindicated for hypertensive patients?) the system should be able to answer, since such an answer is found in some of the leaflets of medicines whose active ingredient is Ibuprofen. However, when faced with the question, ¿el Ibuprofeno acelera la pérdida de memoria? (does Ibuprofen accelerate memory loss?) The system will not find the answer, since these leaflets do not clarify anything about this question. Currently, there are many web pages that offer a service called “Doctor Answers”, in which patients can send questions to doctors, and receive a response in less than 24 hours. For example, the web pages of Doctoralia 5 , Savia 6 , or SaludOnNet 7 . Although MeQA is a project that belongs to the AEMPS, many of these websites can benefit from its use, since, for example, they can redirect queries about medicines for human use (which are many) to MeQA, so it answer them automatically, thus alleviating the workload of these professionals. In section 2 the state of the art in QA is reviewed, in section 3 the main features of MeQA are described, section 4 describes the evaluation process, and finally section 5 contains the conclusions and future work. State of the Art QA systems can be generically divided into two large groups (Jurafsky and Martin, 2000). The first group is made up of QA systems based on information retrieval, which handle a collection of texts. Given a question, the information retrieval system tries to find relevant passages, and then obtain a concrete answer by providing fragments of those passages. The second group, known as knowledge-based QA 8 , builds a semantic representation of the question to a logical representation, and then these representations are used to query structured databases. An alternative approach to doing QA is to query a previously trained language model, forcing the model to answer a question solely from the information stored in its parameters. For example, in (Roberts, Raffel and Shazeer, 2020) they use a language model called T5. Although this solution is not yet complete to answer questions: for example, they don't work as well as classic models, they suffer from misinterpretation (unlike standard QA systems, for example, they currently cannot give users more context telling them which passage the answer came from). However, the study of the answer extraction from language models is an interesting area for future QA research. Currently, QA systems have a lot of interest in the NLP community, and many other tasks have been generated, such as: long-form QA (Fan et al., 2019), where questions require a long answer; community QA, ComQA (Abujabal et al., 2019), which makes use of data sets of pairs of questions and answers created by a certain community such as Quora or Stack Overflow. In addition, this task is also present in other fields of artificial intelligence (AI), such as image processing, called Image Question Answering, IQA (Gordon et al., 2017), or Visual Question Answering, VQA (Antol et al. ., 2015), in which the objective is to answer questions about certain elements present in photographs, such as the color of certain elements, objects, etc. It is such an active field of AI that a new task called Embodied Question Answering (Das et al., 2018) has recently been created, which consists of generating an agent at a random location in a 3D environment and asking it a question (such as ¿What color is the car?). To answer, the agent must first intelligently navigate to explore the environment, collect the necessary visual information through first-person vision, and then answer the question (e.g. orange). This task combines different fields of AI such as language comprehension (LU), visual recognition, active perception, goal-based navigation, common sense reasoning, long-term memory, and conversion of language into actions. Within the biomedical domain, BioASQ (Tsatsaronis et al., 2015) has organized several QA tasks from structured data and free text, as well as the Medical Question Answering tasks organized in the TREC (Ben Abacha et al., 2017). The proposed system, MeQA, has been developed following the paradigm of QA systems based on information retrieval and, as we will see, it combines machine learning and deep learning techniques. It has the advantage of being easily expandable to other languages, as it does not need to annotate large amounts of question-answer pairs, something that QA systems based on complete neural architectures such as encoder-decoders (also known as seq2seq) do need (Sutskever, Vinyals and Quoc, 2014), or those who perform fine-tuning on pre-trained models such as BERT (Devlin et al., 2019). Conclusions and Future Work This paper describes a system developed at the AEMPS, called MeQA, which allows answering questions about medicines through the leaflet. Its architecture has been shown and explained in a general way, and also the modules that compose it. MeQA can be very useful for users, since there are a large number of web pages offering a service called "Doctor Answers", in which most of the questions deal with medicines for human use that can be answered through the leaflet. MeQA has been evaluated both automatically and manually. The automatic evaluation has been carried out in a general way as well as of each of the described modules, obtaining, in general, an F1 performance of 87%. MeQA combines machine learning and deep learning methods, and although it is not a semi-supervised system, it is a low-supervision system. We believe that this is precisely the great advantage of MeQA over other approaches based entirely on deep learning. MeQA hardly needs annotated data to work, only the module that predicts the sections in which the answer is likely to be found uses annotated data, the rest of the modules are unsupervised. As mentioned, the annotation of this information is very simple, and very fast, but not the complete annotation of the answer, which would be needed to develop systems based entirely on deep learning. In the future, improvements are expected to increase the performance of the system. In particular, because the leaflets are divided into sections, MeQA is able to go directly to the predicted sections. However, it would be useful to have a module that allows to go through the entire leaflet (without splitting) and obtain those sections. As explained above, MeQA performs worst on very complex questions. A possible solution to assess would consist of dividing these questions into fragments, analyzing and answering each one of them, and combining the answers into one.