Abstract Short text classification is a fundamental task in natural language processing. It is hard due to the lack of context information and labeled data in practice. In this paper, we propose a new method called SHINE, which is based on graph neural network (GNN), for short text classification. First, we model the short text dataset as a hierarchical heterogeneous graph consisting of word-level component graphs which introduce more semantic and syntactic information. Then, we dynamically learn a short document graph that facilitates effective label propagation among similar short texts. Thus, comparing with existing GNN-based methods, SHINE can better exploit interactions between nodes of the same types and capture similarities between short texts. Extensive experiments on various benchmark short text datasets show that SHINE consistently outperforms state-of-the-art methods, especially with fewer labels.  Introduction Short texts such as tweets, news feeds and web search snippets appear daily in our life (Pang and Lee, 2005; Phan et al., 2008). To understand these short texts, short text classification (STC) is a fundamental task which can be found in many applications such as sentiment analysis (Chen et al., 2019), news classification (Yao et al., 2019) and query intent classification (Wang et al., 2017). STC is particularly hard in comparison to long text classification due to two key issues. The first key issue is that short texts only contain one or a few sentences whose overall length is small, which lack enough context information and strict syntactic structure to understand the meaning of texts (Tang et al., 2015; Wang et al., 2017). For example, it is hard to get the meaning of "Birthday girl is an amusing ride" without knowing "Birthday girl" is a 2001 movie. A harder case is to understand a web search snippet such as "how much Tesla", which usually does not contain word order nor function words (Phan et al., 2008). In addition, real STC tasks usually only have a limited number of labeled data compared to the abundant unlabeled short texts emerging everyday (Hu et al., 2019). Therefore, auxiliary knowledge is required to understand short texts, examples include concepts that can be found in common sense knowledge graphs (Wang et al., 2017; Chen et al., 2019), latent topics extracted from the short text dataset (Hu et al., 2019), and entities residing in knowledge graphs (Hu et al., 2019). However, simply enriching auxiliary knowledge cannot solve the shortage of labeled data, which is another key issue commonly faced by real STC tasks (Pang and Lee, 2005; Phan et al., 2008). Yet the popularly used deep models require large-scale labeled data to train well (Kim, 2014; Liu et al., 2016). Currently, graph neural networks (GNNs) designed for STC obtain the state-of-the-art performance (Hu et al., 2019; Ye et al., 2020). They both take the STC as the node classification problem on a graph with mixed nodes of different types: HGAT (Hu et al., 2019) builds a corpus-level graph modeling latent topics, entities and documents and STGCN (Ye et al., 2020) operates on a corpuslevel graph of latent topics, documents and words. In both works, each document is connected to its nodes of a different type such as entities and latent topics but not to other documents. However, they do not fully exploit interactions between nodes of the same type. They also fail to capture the similarities between short documents, which is both useful to understand short texts (Zhu et al., 2003; Kenter and De Rijke, 2015; Wang et al., 2017) and and important to propagate few labels on graphs (Kipf and Welling, 2016). Besides, both works have large parameter sizes: HGAT (Hu et al., 2019) is a GNN with dual-level attention and STGCN (Ye et al., 2020) merges the node representations with word embeddings obtained via a pretrained BERT (Devlin et al., 2019) via a bidirectional LSTM (Liu et al., 2016). To address the aforementioned problems, we propose a novel HIerarchical heterogeNEous graph representation learning method for STC called SHINE, which is able to fully exploit interactions between nodes of the same types and capture similarity between short texts. SHINE operates on a hierarchically organized heterogeneous corpuslevel graph, which consists of the following graphs at different levels: (i) word-level component graphs model interactions between words, partof-speech (POS) tags and entities which can be easily extracted and carry additional semantic and syntactic information to compensate for the lack of context information; and (ii) short document graph is dynamically learned and optimized to encode similarities between short documents which allows more effective label propagation among connected similar short documents. We conduct extensive experiments on a number of benchmark STC datasets including news, tweets, document titles and short reviews. Results show that the proposed SHINE consistently outperforms the state-of-theart with a much smaller parameter size. Related Works 2.1 Text Classification Text classification assigns predefined labels to documents of variable lengths which may consist of a single or multiple sentences (Li et al., 2020). Traditional methods adopt a two-step strategy: first extract human-designed features such as bagof-words (Blei et al., 2003) and term frequencyinverse document frequency (TF-IDF) (Aggarwal and Zhai, 2012) from documents, then learn classi- fiers such as support vector machine (SVM) (Cortes and Vapnik, 1995). Deep neural networks such as convolutional neural networks (CNN) (Kim, 2014) and long short-term memory (LSTM) (Liu et al., 2016) can directly obtain expressive representations from raw texts and conduct classification in an end-to-end manner. Recently, graph neural networks (GNNs) (Defferrard et al., 2016; Kipf and Welling, 2016) have obtained the state-of-the-art performance on text classification. They can be divided into two types. The first type of GNNs constructs document-level graphs where each document is modeled as a graph of word nodes, then formulates text classification as a whole graph classification problem (Defferrard et al., 2016). Examples are TLGNN (Huang et al., 2019), TextING (Zhang et al., 2020), HyperGAT (Ding et al., 2020), which establish word-word edges differently. In particular, some methods (Liu et al., 2019; Chen et al., 2020) propose to estimate the graph structure of the document-level graphs during learning. However, if only a few documents are labeled, these GNNs cannot work due to the lack of labeled graphs. As is known, GNNs such as graph convolutional network (GCN) (Kipf and Welling, 2016) can conduct semi-supervised learning to solve node classification task on a graph where only a small number of nodes are labeled (Kipf and Welling, 2016). Therefore, another type of GNNs instead operates on a heterogeneous corpus-level graph which takes both text and word as nodes, and classifies unlabeled texts by node classification. Examples include TextGCN (Yao et al., 2019), TensorGCN (Liu et al., 2020), HeteGCN (Ragesh et al., 2021) and TG-Transformer (Zhang and Zhang, 2020) with different strategies to construct and handle heterogeneous nodes and edges. However, these methods cannot work well for short texts of limited length. 2.2 Short Text Classification (STC) Short text classification (STC) is particularly challenging (Aggarwal and Zhai, 2012; Li et al., 2020). Due to limited length, short texts lack context information and strict syntactic structure which are vital to text understanding (Wang et al., 2017). Therefore, methods tailored for STC strive to incorporate various auxiliary information to enrich short text representations. Popularly used examples are concepts existing in external knowledge bases such as Probase (Wang et al., 2017; Chen et al., 2019) and latent topics discovered in the corpus (Zeng et al., 2018). However, simply enriching semantic information cannot compensate for the shortage of labeled data, which is a common problem faced by real short texts such as queries and online reviews (Pang and Lee, 2005; Phan et al., 2008). Thus, GNN-based methods which perform node classification for semi-supervised STC are utilized. HGAT (Hu et al., 2019) applies a GNN with dual-level attention to forward messages on a corpus-level graph modeling topics, entities and documents jointly, where the entities are words linked to knowledge graphs. STGCN (Ye et al., 2020) operates on a corpus-level graph of topics, documents and words, and merges the node representations with word embeddings obtained via a pretrained BERT (Devlin et al., 2019) via a bidirectional LSTM (Liu et al., 2016). Currently, the state-of-the-art method on STC is HGAT (Hu et al., 2019; Yang et al., 2021).  Conclusion In this paper, we propose SHINE, a novel hierarchical heterogeneous graph representation learning method for short text classification. It is particularly useful to compensate for the lack of context information and propagate the limited number of labels efficiently. Specially, SHINE can effectively learn from a hierarchical graph modeling different perspectives of the short text dataset: word-level component graphs are used to understand short texts from the semantic and syntactic perspectives, and the dynamically learned short document graph allows efficient and effective label propagation among similar short documents. Extensive experiments show that SHINE outperforms the others consistently. As for the future works, we plan to search graph structure (Zhao et al., 2021) and utilize automated machine learning (Yao et al., 2018) to improve learning performance.