Abstract Large language models can produce fluent di- alogue but often hallucinate factual inaccura- cies. While retrieval-augmented models help alleviate this issue, they still face a difficult challenge of both reasoning to provide correct knowledge and generating conversation simul- taneously. In this work, we propose a modu- lar model, Knowledge to Response (K2R), for incorporating knowledge into conversational agents, which breaks down this problem into two easier steps. K2R first generates a knowl- edge sequence, given a dialogue context, as an intermediate step. After this Ã¢ÂÂreasoning step, the model then attends to its own generated knowledge sequence, as well as the dialogue context, to produce a final response. In de- tailed experiments, we find that such a model hallucinates less in knowledge-grounded dia- logue tasks, and has advantages in terms of interpretability and modularity. In particular, it can be used to fuse QA and dialogue sys- tems together to enable dialogue agents to give knowledgeable answers, or QA models to give conversational responses in a zero-shot setting. Introduction To be regarded as successful, a conversational agent needs to generate utterances that are both knowl- edgeable and factually correct, as well as being conversationally appropriate, fluent and engaging. The pursuit of this goal has led to ever bigger mod- els that store a large amount of knowledge in their parameters (Roller et al., 2021; Adiwardana et al., 2020; Zhang et al., 2020). However, hallucination wherein a model generates factually inaccurate statements Ã¢ÂÂ has remained a problem no matter the size of the model (Shuster et al., 2021a). Recent advances in neural retrieval models have made some inroads into this problem (Lee et al., 2019; Lewis et al., 2020b; Shuster et al., 2021a; Komeili et al., 2021) by generating responses based on both the dialogue context and by learning to re- trieve documents containing relevant knowledge. However, the conversational setting is challenging because these models are required to perform mul- tiple duties all in one shot: to perform reasoning over the returned documents and dialogue history, find the relevant knowledge, and then finally com- bine this into a conversational form pertinent to the dialogue. Perhaps due to this complexity, it has been observed that failure cases include incorporat- ing parts of multiple documents into one factually incorrect response, or failure to include knowledge at all and reverting instead to a generic response using the dialogue context only. In this work, we instead propose to decompose this difficult problem into two easier steps. Specif- ically, by first generating pertinent intermediate knowledge explicitly and then, conditioned on this prediction, generating the dialogue response. We call this model Knowledge to Response (K2R). Using this modular design, we can train and evaluate the reasoning performance of the model indepen- dently from its conversational abilities, increasing the interpretability of our modelÃ¢ÂÂs output. This also allows us to plug external knowledge into dialogue systems without any requirement for retraining, for example, from question answering systems. The dialogue response modelÃ¢ÂÂs task reduces to incorpo- rating the predicted knowledge in an engaging and context-fitting conversational response. We conduct extensive experiments across multi- ple tasks and datasets. We find that our K2R model effectively improves correct knowledge-utilization and decreases hallucination (Shuster et al., 2021a) in knowledge-grounded dialogue (Dinan et al., 2019). In open-domain dialogue, the K2R model improves the performance on automatic metrics compared to its seq2seq counterpart, along with the additional benefits of increased interpretabil- ity of the modelÃ¢ÂÂs output and the possibility for knowledge injections. The modular design allows us to fuse state-of-the-art pre-trained QA models Ã¢ÂÂ without any fine-tuning Ã¢ÂÂ with dialogue models to generate answers that humans judge as both more knowledgeable and engaging. Our modular system also outperforms multi-tasking approaches. Related Work Improving dialogue systems by increasing their knowledgeability has been tried in several different ways: from integrating knowledge bases (Zhu et al., 2017; Liu et al., 2018; Wang et al., 2020), to larger models that are pre-trained on more data (Roller et al., 2021; Adiwardana et al., 2020; Zhang et al., 2020), and recent neural retrieval(Shuster et al., 2021a; Thulke et al., 2021). Knowledge- grounded open-domain dialogue datasets (Dinan et al., 2019; Komeili et al., 2021; Zhou et al., 2018;Gopalakrishnan et al., 2019) foster the research and development of knowledge-aware generative dialogue models. A known issue of such mod- els, referred to as hallucination, is that they mix up facts and generate factually inaccurate state- ments. Shuster et al. (2021a) try to alleviate this problem by using recent advancements in retrieval- augmented generative models developed for open- domain dialogue (Dinan et al., 2019). These methods still hallucinate to some degree, and their predictions (and hence er- rors) are not easily interpretable. There is also recent work in the space of modular or intermediate generation components for text generation. The approach of text modular networks promises more interpretable answers to multi-hop questions and situations. The modular modelÃ¢ÂÂs task reduces to incorpo- rating the predicted knowledge in an engaging and context-fitting conversational response. We conduct extensive experiments across multi- ple tasks and datasets. We find that our K2R model effectively improves correct knowledge-utilization and decreases hallucination (Shuster et al., 2021a) in knowledge-grounded dialogue (Dinan et al., 2019). In open-domain dialogue, the K2R model improves the performance on automatic metrics compared to its seq2seq counterpart, along with the additional benefits of increased interpretabil- ity of the modelÃ¢ÂÂs output and the possibility for knowledge injections. The modular modelÃ¢ÂÂs task reduces to incorpo- rating the predicted knowledge in an engaging and context-fitting conversational response. We conduct extensive experiments across multi- ple tasks and datasets. We find that our K2R model effectively improves correct knowledge-utilization and decreases hallucination (Shuster et al., 2021a) in knowledge-grounded dialogue (Dinan et al., 2019). In open-domain dialogue, the K2R model improves the performance on automatic metrics compared to its seq2seq counterpart, along with the additional benefits of increased interpretabil- ity of the models output and the possibility for knowledge injections. The modular models task reduces to incorpo- rating the predicted knowledge in an engaging and context-fitting conversational response. We conduct extensive experiments across multi- ple tasks and datasets. We find that our K2R model improves the performance on automatic metrics compared to its seq2seq counterpart, along with the additional benefits of increased interpretabil- ity of the models output and the possibility for knowledge injections. The modular intermediate step Ã¢ÂÂ wherein and due to the dialogue response result in the modified response. While this approach is not required a priori, it is recommended that learners develop and evaluate the modified response in a zero-shot setting. After this reasoning step, the model then attends to its own generated knowledge sequence, as well as the dialogue context, to produce a final response. In de- tailed experiments, we find that such a model hallucinates less in knowledge-grounded dialogue (Dinan et al., 2019). In open-domain dialogue, the modular intermediate step wherein and due to the dialogue response result in the modified response. While this approach is not required a priori, it is recommended that learners develop and evaluate the modified response in a zero-shot setting. After this reasoning step, the model then attends to its own generated knowledge sequence, as well as the dialogue context, to produce a final response. In de- tailed experiments, we find that such a model hallucinates less in knowledge-grounded dialogue (Dinan et al., 2019). 