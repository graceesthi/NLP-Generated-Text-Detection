Abstract Grammatical Error Correction (GEC) is a task of detecting and correcting grammatical errors in sentences. Recently, neural machine transla- tion systems have become popular approaches for this task. However, these methods lack the use of syntactic knowledge which plays an important role in the correction of gram- matical errors. In this work, we propose a syntax-guided GEC model (SG-GEC) which adopts the graph attention mechanism to uti- lize the syntactic knowledge of dependency trees. Considering the dependency trees of the grammatically incorrect source sentences might provide incorrect syntactic knowledge, we propose a dependency tree correction task to deal with it. Combining with data augmen- tation method, our model achieves strong per- formances without using any large pre-trained models. We evaluate our model on public benchmarks of GEC task and it achieves competitive results. Introduction Grammatical Error Correction (GEC) is a task of detecting and correcting grammatical errors in sen- tences. Due to the growing number of language learners of English, there has been increasing atten- tion to the English GEC in the past few years. Considering the outstanding performance of neu- ral network models in machine translation tasks, numerous studies have applied cutting-edge neu- ral machine translation models to GEC task(Zhao et al., 2019; Junczys-Dowmunt et al., 2018) . Be- sides, the adoption of large pre-trained models becomes popular as well (Kaneko et al., 2020; Omelianchuk et al., 2020a). These works have achieved great success, but lack the use of syntac- tic knowledge which plays an important role in the correction of grammatical errors. In this work, we propose a syntax-guided GEC model (SG-GEC) with dependency tree correction task to exploit the syntactic knowledge of depen- dency trees. A dependency tree is a directed graph representing syntactic knowledge of several words towards each other. Inspired by VelicÃÂkovic ÃÂ et al. (2017), we adopt the graph attention mechanism to utilize the syntactic knowledge within dependency trees. Especially, the source sentences in GEC task are sentences with grammatical errors which means the dependency trees of source sentences might provide incorrect syntactic knowledge. So sim- ply applying the graph attention mechanism over source sentences would not work well. Given that, we proposed a dependency tree correction task to construct dependency trees of corrected sentences. Considering a tree can be uniquely determined by relations of nodes, we construct the dependency trees of corrected sentences by predicting the re- lations of nodes instead of the entire tree. By ap- plying this additional task, the model can construct dependency trees of corrected sentences and enrich the model with the corrected syntactic knowledge. We apply the data augmentation method to fur- ther improve the performance of the model. Exper- iments are conducted on the following widely used benchmarks: CoNLL-2014 (Ng et al., 2014), FCE (Yannakoudakis et al., 2011), BEA-2019 (Bryant et al., 2019). Among models without using the large pre-trained models, our model achieves the best F-score on all benchmarks. Comparing with models which incorporate the large pre-trained models, our model achieves very competitive per- formance as well. In general, our model achieves strong performance without using any large pre- trained models. Our contributions are summarized as follows: 1. To the best of our knowledge, we introduce syntactic knowledge into neural GEC model for the first time, by applying graph attention mechanism to utilize the dependency tree. 2. We propose a dependency tree correction task to deal with the problem that the dependency trees of grammatically incorrect source sentences might provide incorrect syntactic knowledge. 3. Without using any large pre-trained model, our SG-GEC model achieves strong performances on public GEC benchmarks. Related Work Early published works in GEC developed models based on manually designed grammar rules (Mu- rata and Nagao, 1994; Bond et al., 1996; Siegel, 1996). After Han et al. (2006) pointed out the limitation of rule-based method, some researchers turned their attention to the statistical machine learning method (Knight and Chander, 1994; Min- nen et al., 2000; Izumi et al., 2003). With the development of deep learning, recent works proposed various neural network models to solve GEC task. Some regarded the GEC task as aproblem and applied cutting-edge neural machine translation model to deal with it (Yuan and Briscoe, 2016; Chollampatt and Ng, 2018). Many recent works (Junczys-Dowmunt et al., 2018; Zhao et al., 2019) made use of the powerful machine translation architecture Transformer (Vaswani et al., 2017). Considering the tremendous performance of pre-trained models, numerous studies have applied cutting-edge neu- ral machine translation models to GEC task(Zhang et al., 2020; Junczys-Dowmunt et al., 2018) . Be- sides, the adoption of large pre-trained models becomes popular as well (Kaneko et al., 2020; Omelianchuk et al., 2020a). These works have achieved great success, but lack the use of syntac- tic knowledge which plays an important role in the correction of grammatical errors. In this work, we propose a syntax-guided GEC model (SG-GEC) with dependency tree correction task to exploit the syntactic knowledge of depen- dency trees. A dependency tree is a directed graph representing syntactic knowledge of several words towards each other. Inspired by VelicÃÂkovic ÃÂ et al. (2017), we adopt the graph attention mechanism to utilize the syntactic knowledge within dependency trees. Especially, the source sentences in GEC task are sentences with grammatical errors which means the dependency trees of source sentences might provide incorrect syntactic knowledge. So sim- ply applying the graph attention mechanism over source sentences would not work well. Given that, we proposed a dependency tree correction task to construct dependency trees of corrected sentences. Considering a tree can be uniquely determined by relations of nodes, we construct the dependency trees of corrected sentences by predicting the re- lations of nodes instead of the entire tree. By ap- plying this additional task, the model can construct dependency trees of corrected sentences and enrich the model with the corrected syntactic knowledge. We apply the data augmentation method to fur- ther improve the performance of the model.