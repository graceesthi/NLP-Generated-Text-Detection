ABSTRACT In recent years BERT shows apparent advantages and great potential in natural language processing tasks. However, both training and applying BERT requires intensive time and resources for computing contextual language representations, which hinders its universality and applicability. To overcome this bottleneck, we propose a deep bidirectional language model by using window masking mechanism at attention layer. This work computes contextual language representations without random masking as does in BERT and maintains the deep bidirectional architecture like BERT. To compute the same sentence representation, our method shows O(n) complexity less compared to other transformer based models with O(n 2 ). To further demonstrate its superiority, computing context language representations on CPU environments is conducted, by using the embeddings from the proposed method, logistic regression shows much higher accuracy in terms of SMS classification. Moverover, the proposed method also achieves significant higher performance in semantic similarity tasks. Keywords Language model· Transformer ÃÂ· Encoding Introduction The basic task of NLP is to build a language model, based on which most of the NLP applications such as machine translation can be carried out (ref). Recently, BERT (Bidirectional Encoder Representations from Transformers) and its variations have brought significant improvements in learning natural language representation, and they have achieved state-of-the-art performances on various downstream tasks such as GLUE benchmark and question answering. This success of BERT continues in various unsupervised tasks such as the N-best list reranking for image captioning and NMT, demonstrating the superiority of bidirectional language models for unsupervised tasks. However, when training the BERT model, it usually requires huge corpus and large parameters, which lead to usage of expensive hardware equipment and long duration of training time. During training, the BERT uses the masked language modeling (MLM) objective, which is to predict the original ids of explicitly masked words from the input. Moreover, due to the random masking mechanism of BERT, for each input sentence, 70% of the tokens in the sentence are randomly masked, only 30% of tokens in the sentence can be trained in one epoch of training. This makes the BERT model training is extremely inefficient. Therefore, it is necessary to increase the training efficiency of BERT model where domain specific applications requires a fast set up and validation. In this paper, we try to solve the above limitation by proposing a deep bidirectional language model named window masking model. The proposed model is trained with a new learning objective named next token prediction based on the token preceding it. This method uses the text preceding the target word as input text to predict the target word, namely, the concatenation of embedding from attention layer where target word is masked and embedding from previous residual input will be used to predict the target word. To learn the proposed objective, we devise a window masking operation and an next prediction mechanism in the residual layer inside the model based on the Transformer encoder (Vaswani et al.,2017). These components enable the proposed model to compute contextualized language representations at once while maintaining the advantages of the deep bidirectional architecture of language model. We conduct a series of experiments on two unsupervised tasks: the N-best list reranking and the unsupervised semantic textual similarity. First, in the training process with GPU, we show that the proposed methodÃ¢ÂÂs loss is 5 times smaller than the BERT-based model given the same training epochs. Second, even with this faster training process, the proposed method achieves competitive performances to BERT on reranking tasks and unsupervised semantic textual similarity tasks. 2 Related Works Rumelhart et al[Rumelhart et al., 1986] proposed that hidden units of neural network can represent features for a specified task after weight adjustment by using back-propagation. Prior to distributed word representations based on neural network, statistical language model is trained by computing the joint probability function of word sequences, based on which statistical language model is trained most efficiently. Moreover, Mikolov et al[Mikolov et al., 2013] trained bidirectional language model with a large corpus and large parameters, which led to that the derived language model improved the state of the art in diverse language understanding problems. Recently, Mikolov et al[Mikolov et al., 2018] proposed that hidden units of neural network can represent features for a specified task after weight adjustment by using back-propagation. To summarize, the above-mentioned advantages and advantages of deep bidirectional language model are presented. The proposed objective is to predict the original ids of explicitly masked words from the input sentence, which lead to usage of-hardware equipment and long duration of training time. Moreover, the improved performance of the proposed objective is attributed to the fact that the explicitly masked words in the sentence are randomly masked. To further demonstrate its superiority, the derived objective achieves competitive performances to BERT on reranking tasks and unsupervised tasks. To further demonstrate its superiority, the derived objective also achieves excellent performance to BERT on reranking tasks and unsupervised tasks. To further demonstrate its superiority, the derived objective also achieves excellent performance to BERT on reranking tasks and unsupervised tasks. To further demonstrate its superiority, the derived objective also achieves excellent performance to BERT on reranking tasks and unsupervised tasks. To further demonstrate its superiority, the derived objective also achieves excellent performance to BERT on reranking tasks and unsupervised tasks. To further demonstrate its superiority, the derived objective also achieves excellent performance to BERT on reranking tasks and unsupervised tasks. To further demonstrate its superiority, the derived objective also achieves excellent performance to BERT on reranking tasks and unsupervised tasks. To further demonstrate its superiority, the derived objective also achieves excellent performance to BERT on reranking tasks and unsupervised tasks. To further demonstrate its superiority, the derived objective also achieves excellent performance to BERT on reranking tasks and unsupervised tasks. To further demonstrate its superiority, the derived objective also achieves excellent performance to BERT on reranking tasks and unsupervised tasks. To further demonstrate its superiority, the derived objective also achieves excellent performance to BERT on reranking tasks and unsupervised tasks. To further demonstrate its superiority, the derived objective also achieves excellent performance to BERT on reranking tasks and unsupervised tasks. To further demonstrate its superiority, the derived objective also achieves excellent performance to BERT on reranking tasks and unsupervised tasks. To further demonstrate its superiority, the derived objective also achieves excellent performance to BERT on reranking tasks and unsupervised tasks. To further demonstrate its superiority, the derived objective also achieves excellent performance to BERT on reranking tasks and unsupervised tasks. To further demonstrate its superiority, the derived objective also achieves excellent performance to BERT on reranking tasks and unsupervised tasks. To further demonstrate its superiority, the derived objective also achieves excellent performance to BERT on reranking tasks and unsupervised tasks. To further demonstrate its superiority, the derived objective also achieves excellent performance to BERT on reranking tasks and unsupervised tasks.