Abstract Recent methods in speech and language tech- nology pretrain very LARGE models which are fine-tuned for specific tasks. However, the benefits of such LARGE models are often lim- ited to a few resource rich languages of the world. In this work, we make multiple con- tributions towards building ASR systems for low resource languages from the Indian sub- continent. First, we curate 17,000 hours of raw speech data for 40 Indian languages from a wide variety of domains including education, news, technology, and finance. Second, using this raw speech data we pretrain several vari- ants of wav2vec style models for 40 Indian languages. Third, we analyze the pretrained models to find key features: codebook vec- tors of similar sounding phonemes are shared across languages, representations across lay- ers are discriminative of the language family, and attention heads often pay attention within small local windows. Fourth, we fine-tune this model for downstream ASR for 9 languages and obtain state-of-the-art results on 3 public datasets, including on very low-resource lan- guages such as Sinhala and Nepali. Our work establishes that multilingual pretraining is an effective strategy for building ASR systems for the linguistically diverse speakers of the Indian subcontinent. Introduction The Indian subcontinent is one of the most linguis- tically diverse regions in the world as well as one of the most populous regions in the world - little won- der that it is called a Ã¢ÂÂsubcontinentÃ¢ÂÂ. It is home to around 2 billion people from 7 countries who speak 100+ major languages (and thousands of minority languages and dialects) belonging to four major language families1,2. Of these, the Indo-Aryan and Dravidian languages are spoken by a large section of the population. These language families have intermingled over a large period of time giving rise to the Indian linguistic area/sprachbund where lan- guages across these families share many features (Emeneau, 1956; Subba ÃÂra ÃÂo, 2012; Kunchukuttan and Bhattacharyya, 2020). Building high-quality ASR models for such a large and diverse pool of languages is challenging, even if we restrict ourselves to 30 odd languages which have more than a million speakers. Many modern ASR models rely on large amounts of la- beled data for each language to build high-quality ASR systems. Such approaches are expensive and not scalable, thus limiting the reach of ASR tech- nologies to some languages and a section of the population. In addition to these challenges on avail- ability of labeled data, Indian languages also face a set of common challenges that need to be addressed. Most Indic scripts have a larger character set than English. The complex inflectional systems of In- dian languages make modelling phonotactics more challenging. The rich inflectional/agglutinative na- ture of Indian languages results in larger vocabu- lary sizes, presenting challenges to incorporating language models. At the same time there are oppor- tunities from a unified perspective. Collection of unlabeled data for pretraining can be undertaken as a joint effort since many sources might be shared amongst these languages. A largely overlapping phoneme inventory, logically overlapping character sets and syntactic similarity can help utilize linguis- tic similarity at various levels to build multilingual models where transfer learning can be effective. In this context, it is important to take note of recent work that has demonstrated the benefits of unsupervised pretraining and multilingual fine- tuning to significantly improve ASR quality for low-resource languages (Baevski et al., 2020b; Conneau et al., 2020a). In particular, the wav2vec model has established two key results for English ASR. One, an end-to-end DNN architecture borrowing the popular Transformer architecture from NLP establishes SOTA results. And two, pre- training on a large corpus of data reduces the re- quirement for labeled fine-tuning from hundreds of hours to few hours and to even tens of min- utes. It is worthwhile to explore if these findings from English ASR transfer to Indic ASR, espe- cially given the diversity and above mentioned challenges with Indic languages. More specifi- cally, would a wav2vec-like model establish SOTA results on available benchmarks across Indic lan- guages? Further, would large pretraining preclude the need for collecting large amounts of labeled data? And finally,pretraining a multilingual model across Indian languages provide positive benefits across these related languages? In order to answer these questions, we make the following contributions: ÃÂ¢ÃÂÃÂ¢ We curate 17,314 hours of raw audio data for pretraining across 40 languages from 4 language families, making it one of the largest and most diverse collections of ASR data for Indian languages. This data has been curated in a short time frame from public sources which have a permissive license. This has resulted in more than a million hours of raw audio data for pretraining across 40 languages from the public domain. This data has been curated in a short time frame from public sources which have a permissive license. This file pretrainers for 40 Indian languages pretrained from a large corpus of data, making it one of the largest and most diverse collections of ASR data for pretraining. It is a significant achievement since it indicates that multilingual pretraining is an effective strategy for building ASR systems for the linguistically diverse speakers of the Indian subcontinent. Introduction The Indian subcontinent is one of the most linguis- tically diverse regions in the world as well as one of the most populous regions in the world - little won- der that it is called a Ã¢ÂÂsubcontinentÃ¢ÂÂ. It is home to around 2 billion people from 7 countries who speak 100+ major languages (and thousands of minority languages and dialects) belonging to four major language families1,2. Of these, the Indo-Aryan and Dravidian languages are spoken by a large section of the population. These language families have intermingled over a large period of time giving rise to the Indian linguistic area/sprachbund where lan- guages across these families share many features (Emeneau, 1956; Subba ÃÂra ÃÂo, 2012; Kunchukuttan and Bhattacharyya, 2020). Building high-quality ASR models for such a large and diverse pool of languages is challenging, even if we restrict ourselves to 30 odd languages which have more than a million speakers. Many modern ASR models rely on large amounts of la- beled data for each language to build high-quality ASR systems. Such approaches are expensive and not scalable, thus limiting the reach of ASR tech- nologies to some languages and a section of the population. In addition to these challenges on avail- ability of labeled data, Indian languages also face a set of common challenges that need to be addressed. Most Indic scripts have a larger character set than English. The complex inflectional systems of In- dian languages make modelling phonotactics more challenging. The rich inflectional/agglutinative na- ture of Indian languages results in larger vocabu- lary sizes, presenting challenges to incorporating language models. At the same time there are oppor- tunities from a unified perspective. Collection of unlabeled data for pretraining can be undertaken as a joint effort since many sources might be shared amongst these languages. A largely overlapping phoneme inventory, logically overlapping character sets and syntactic similarity can help utilize linguis- tic similarity at various levels to build multilingual models where transfer learning can be effective.