Abstract. Ontology-based approach to the Natural Language Understanding (NLU) processing allows to improve questions answering quality in dialogue sys- tems. We describe our NLU engine architecture and evaluate its implementation. The engine transforms user’s input into the SPARQL SELECT, ASK or INSERT query to the knowledge graph provided by the ontology-based data virtualization platform. The transformation is based on the lexical level of the knowledge graph built according to the Ontolex ontology. The described approach can be applied for graph data population tasks and to the question answering systems implementation, including chat bots. We describe the dialogue engine for a chat bot which can keep the conversation context and ask clarifying questions, simulating some aspects of the human logical thinking. Our approach uses graph-based algorithms to avoid gathering datasets, required in the neural nets-based approaches, and provide better explainability of our mod- els. Using question answering engine in conjunction with data virtualization layer over the corporate data sources allows extracting facts from the structured data to be used in conversation. Keywords: natural language understanding, ontology, Ontolex, data virtualization. Introduction The corporate automated system users expect them to be “intellectual” enough to give precise answers to their questions. It implies that a system must give meaningful answers in the dialogue with a user and ask clarifying questions as a human would do. To achieve that, the system must deal with the structured representation of each ques- tion and answer of a dialogue, as well as with a structured data required to give an answer. The knowledge graphs (KGs) are one of the most popular ways to represent the complex structured information. KGs can be populated with information by data mining from text sources or struc- tured data sets – for example, the business application databases in the corporate envi- ronment. Some aspects of the corporate KGs assembly are considered in [Noy, 2019]. Our task is to develop the complex solution (framework) for the corporate use which will provide: 1)  Conceptual model construction for representing the users’ domain knowledge. We do not have a task of automated ontology assembly or enrichment because in the corporate projects the ontology is predominantly composed manually. 2)  Disparate corporate data sets into a virtual Knowledge Graph. 3)  The natural language tools for user interaction with KG. The domain ontology composition and the virtual KG assembly are out of scope of this paper (see: [Gorshkov, 2021]). We will focus on the methods of user’s natural lan- guage question transformation into the SPARQL query to the knowledge graph, and the dialogue system implementation. Task definition We consider the KG question-answering and the task of facts extraction from the natural language text as the task of the representing the text meaning in the form of the graph restricted by a conceptual model. In this paper we focus on the description of the dialogue system which transforms a user’s question into a query to the graph. The func- tional requirements to this system are: 1)  It must find the class or property which instances would be an answer to the user’s question and query the KG to find the appropriate entities matching cri- teria defined in the question. 2)  The system’s answer should be precise, not probabilistic. The system must be able to prove that the found objects are the answers to the user’s query, includ- ing visualization of the relations chain that led to these objects. 3)  If there is no unambiguous answer to the user’s query, the system should ask clarifying questions. 4)  The system should answer user’s clarifying questions, which may be asked if the answer is not comprehensive for the user. It means that the system shall keep the conversation context, i.e., the objects and relations mentioned in the previous questions and answers. These requirements cannot be fulfilled today using the neural networks only. The researchers [Thorne, 2021] note that the contemporary NL processing models cannot scale to non-trivial databases nor answer set-based and aggregation queries. Any neural network model output is probabilistic by its nature and cannot be provided with the proof of correctness. We believe, following [Weikum, 2021], that machine knowledge and machine learning complement and strengthen each other. We aimed to combine optimally the strong points of both approaches, machine learning and logical inference, when designing the text processing pipeline. The neural networks are effective in knowledge graph query answering (KGQA) when dealing with the big graphs containing ambiguities, such as DBPedia. The big datasets are required to train such models. This is often impossible when dealing with corporate tasks in the specific and narrow domains. Due to these factors, we have set a goal of creating the explainable and fully controlled tool. Let us describe a domain which we will use as a field for evaluating our solution. Consider the sample industrial enterprise which is composed of the functional units and sites. Each unit and site have a person or organizational unit which is responsible for some aspect of its safety (fire, industrial, etc.). All this information is gathered into a KG, which also contains data on the telemetry sensors and the parameters they measure. We have created a compact ontology for this domain, which offers diversity of the re- lations between objects and the playground for making queries involving 3-4 related graph vertices. In the real use case such a system will include a much extensive set of facts on the various aspects of the enterprise activity. The facts will be gathered into KG from the variety of data sources, such as corporate applications’ databases, the or- ganizational and administrative documents, and will be consolidated by a data virtual- ization platform. The resulting graph should be available with SPARQL interface. Related works We have used some well-known technologies in our NL processing pipeline. Named entities recognition techniques developed over long time [Shen, 2015]. POS tagging is considered in [Mikic, 2009; Wu, 2020; Huang, 2015; Le, 2018; Piccinno, 2014]. Since our ideas are based on moving from syntactical relations to semantical relations (see: [Melchuk, 1999; Gerd, 2005; Kolshansky, 1980; Banarescu, 2013; Fensel, 2003]), we need a morphological and syntactical analysis of sentences and coreference clusters (chains) finding. Morpho-syntactical properties of tokens, analysis of syntactic struc- ture of a phrase, syntactic relationships discovery between words are considered in [Ju- rafsky, 2008]. Co-reference resolution techniques review is given in [Zheng, 2011]. Pre-trained language models such as BERT [Kuratov, 2019] can be used to deter- mine the context-depending word meaning. Transformer models allow vectorization of the word sequences, which can be used to interpret their meanings [Kalyan, 2021]. The ability to retrieve an entity from a Knowledge Base given a textual input is a key com- ponent for several applications (see: [Ferrucci, 2012; Slawski, 2015; Yang, 2018]). We have used Ontolex1 ontology, developed by W3C Ontology-Lexica Community Group2, to formalize lexical model. This ontology was published in 20163 and it is well documented. Its key features are described in its developer’s publications, for example [Cimiano, 2011], [McCrae, 2017]. Ontolex is often used in the computational linguis- tics tasks, for examples see: [Declerck, 2019, Abgaz, 2020]. It is also used in the inter- disciplinary projects, particularly it was used in the European Commission PMKI pro- ject (Public Multilingual Knowledge Infrastructure) in 20184. Conclusions In the quest of the most effective combination of the machine learning and KG tools we have developed the architecture of the natural language understanding pipeline. The algorithms of establishing links between tokens of the recognized text and the domain ontology play the key role in it. The lexical ontology layer, describing words usage to denote domain concepts, is necessary to make them work. It allows semantic ambigui- ties resolution considering semantic fields in which the words are included. The practical value of the developed architecture significantly increases if the KG queries it generates to answer user questions are processed by the data virtualization platform which can access the huge arrays of the data disparate in the corporate stor- ages. It opens the way to create a dialogue system which allows user to discover previ- ously hidden, implicit, or virtually unavailable information from these storages, and involves it in the business processes including decision making support. This way of KG usage can be described as the valuable component of the true Knowledge Manage- ment System of an enterprise. The further work on our NL solution improvement includes quantitative question answering implementation (object counting, searching for maximal/minimal values, summation, and other aggregation methods), work with the date and numeric intervals, temporal relations recognition. These functions are valuable in the corporate data pro- cessing.