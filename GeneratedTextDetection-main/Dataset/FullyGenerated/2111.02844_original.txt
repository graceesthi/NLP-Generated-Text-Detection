ABSTRACT In recent years BERT shows apparent advantages and great potential in natural language processing tasks. However, both training and applying BERT requires intensive time and resources for computing contextual language representations, which hinders its universality and applicability. To overcome this bottleneck, we propose a deep bidirectional language model by using window masking mechanism at attention layer. This work computes contextual language representations without random masking as does in BERT and maintains the deep bidirectional architecture like BERT. To compute the same sentence representation, our method shows O(n) complexity less compared to other transformerbased models with O(n 2 ). To further demonstrate its superiority, computing context language representations on CPU environments is conducted, by using the embeddings from the proposed method, logistic regression shows much higher accuracy in terms of SMS classification. Moverover, the proposed method also achieves significant higher performance in semantic similarity tasks. Keywords Language model · Transformer · Encoding Introduction The basic task of NLP is to build a language model, based on which most of the NLP applications such as machine translation can be carried out (ref). Recently, BERT (Bidirectional Encoder Representations from Transformers) and its variations have brought significant improvements in learning natural language representation, and they have achieved state-of-the-art performances on various downstream tasks such as GLUE benchmark and question answering. This success of BERT continues in various unsupervised tasks such as the N-best list reranking for image captioning and NMT, demonstrating the superiority of bidirectional language models for unsupervised tasks. However, when training the BERT model, it usually requires huge corpus and large parameters, which lead to usage of expensive hardware equipment and long duration of training time. During training, the BERT uses the masked language modeling (MLM) objective, which is to predict the original ids of explicitly masked words from the input. Moreover, due to the random masking mechanism of BERT, for each input sentence, 70% of the tokens in the sentence are randomly masked, only 30% of tokens in the sentence can be trained in one epoch of training. This makes the BERT model training is extremely inefficient. Therefore, it is necessary to increase the training efficiency of BERT model where domain specific applications requires a fast set up and validation. In this paper, we try to solve the above limitation by proposing a novel bidirectional language model named window masking model. The proposed model is trained with a new learning objective named next token prediction based on the token preceding it. This method uses the text preceding the target word as input text to predict the target word, namely, the concatenation of embedding from attention layer where target word is masked and embedding from previous residual input will be used to predict the target word. To learn the proposed objective, we devise a window masking operation and an next prediction mechanism in the residual layer inside the model based on the Transformer encoder (Vaswani et al.,2017). These components enable the proposed model to compute contextualized language representations at once while maintaining the advantages of the deep bidirectional architecture of language model. We conduct a series of experiments on two unsupervised tasks: the N-best list reranking and the unsupervised semantic textual similarity. First, in the training process with GPU, we show that the proposed method’s loss is 5 times smaller than the BERT-based model given the same training epochs. Second, even with this faster training process, the proposed method achieves competitive performances to BERT on reranking tasks and unsupervised semantic textual similarity tasks. 2 Related Works Rumelhart et al[Rumelhart et al., 1986] proposed that hidden units of neural network can represent features for a specified task after weight adjustment by using back-propagation. This methodology has been well adapted for use in natural language processing domain for developing language models for word embedding. Prior to distributed word representations based on neural network, statistical language model is trained by computing the joint probability function of word sequences, which usually cause curse of dimensionality during testing because of unseen words in training set. Distributed representation is proposed to overcome this limitation like this, for word sequence containing word not seen in training set but similar to words in a seen sentence, the target word sequence can still obtain high probability because of similar word vectors with that of sentence in the training set[Bengio et al., 2003]. Moreover, Mikolov et al[Mikolov et al., 2013] produced many works to distributed representation such as Skip-gram, negative sampling, all of which make word embedding in NLP an essential methodology and corner stone in many tasks including machine translation speech recognition. However, the above-mentioned word level representations cannot capture information arising from various polysemy of word use across linguistic context. To address this issue, Peters et al[Peters et al., 2018] trained bidirectional LSTM with a coupled language model objective for producing word vectors, which led to that the derived representations improved the state of the art in diverse language understanding problems. Radford et al[Radford et al., 2018] proposed a framework with transformer as base architecture for achieving long-range dependency, the ablation study shows that apparent score drop without using transformers. All of the results show that contextualized representation are beneficial in language modelling. Furthermore, Devlin et al[Devlin et al., 2018] devised bidirectional encoder representation transformers which show significant progress in eleven natural language processing tasks. Although deep contextualized language model achieved excellent performance, huge amount of parameters of these models incur extremely high cost of computing hardware and computation time. Another line of research tries to address this problem. Lan et al[Lan et al., 2019] proposed A Lite BERT architecture which has 18 times fewer parameters and 1.7 times faster tranning time than a traditional BERT architecture. While Sanh et al[Sanh et al., 2019] presented a method to pretrain a smaller model that can be finetuned for the downstream task, and achieved a 1.4 times fewer parameter with 1.6 times faster inference. However, none of these studies tried to investigate the effect of attention layer’s information leakage, for the purpose of information recombination, on the language model’s training speed and language representation efficacy for the purpose of decreasing computational resources utilization and training time. Conclusion In this work, we propose a attention based bidirectional language model named text denoised autoencoder, in order to save the training time for bidirectional language models as well as reduce the computation time of context language representations for unsupervised applications. We conduct both reranking test and the semantic textual similarity tasks to validate the proposed method in downstream applications, the result of which demonstrate that the proposed text denoised autoencoder is apparently faster than the conventional BERT based method in terms of producing contextualized representation. Moreover, the proposed method yields context representations which have more beneficial effect for downstream applications, demonstrating its improved encoding ability when compared with that of BERT and GPT.