Abstract We describe the 2021 Key Point Analysis (KPA-2021) shared task on key point analysis that we organized as a part of the 8th Workshop on Argument Mining (ArgMining 2021) at EMNLP 2021. We outline various approaches and discuss the results of the shared task. We expect the task and the findings reported in this paper to be relevant for researchers working on text summarization and argument mining. Introduction Key Point Analysis (KPA) was introduced in BarHaim et al. (2020a,b) as a challenging NLP task with tight relations to Computational Argumentation, Opinion Analysis, and Summarization, and with many practical applications (Bar-Haim et al., 2021). Given a potentially large collection of relatively short, opinionated texts focused on a topic of interest, the goal of KPA is to produce a succinct list of the most prominent key-points (KPs) in the input corpus, along with their relative prevalence. Thus, the output of KPA is a bullet-like summary, with an important quantitative angle. Successful solutions to KPA can be used to gain better insights from public opinions as expressed in social media, surveys, and so forth, giving rise to a new form of a communication channel between decision makers and people that might be impacted by the decision. Various requirements govern the value of the KPA output. KPs are expected to be succinct, nonredundant, capturing points that are central to the topic of interest, and reflecting a clear stance towards that topic. Ideally, they should be at the right granularity for summarising the input data Ã¢ÂÂ not too specific and yet still informative and not overly general. In addition, accurate mapping of input texts to KPs is obviously essential. First, to ensure a reliable estimate of the prevalence of each key point. And second, to enable the user to drill-down, to gain a deeper understanding of the issues underlying each key point, as expressed by the input texts mapped to that key point. The goal of the KPA-2021 shared task was to further increase the attention of the NLP community to this emerging task, while enriching the space of existing KPA solutions. Since providing a complete KPA solution is challenging, we divided the task into two tracks, enabling teams to participate only in the first, relatively simpler track. Specifically, in the first track, referred to as Matching Track, KPs are given as part of the input, and the task is focused on mapping input text to these KPs. In contrast, in the second track, referred to as Generation Track, no KPs are provided and the task requires to further generate the KPs. The data being considered for evaluation on both tracks are crowd sourced arguments on three debatable topics. The nature of the KPA task requires to consider various evaluation measures to estimate the quality of the results across different dimensions. In this paper we report and analyze the results of 22 models submitted by 17 teams to the KPA-2021 shared task across these different evaluation measures. We also discuss lessons learned from these multiple submissions, that can guide future development of new KPA algorithms and associated evaluation methods. Discussion We presented KPA-2021, the first shared task focused on key point analysis. The shared task received 22 submissions from 17 teams, covering both the Matching Track and the Generation Track. We presented the submitted models, their performance on different evaluation measures, and further analyzed the results. As expected by its simpler nature, Matching Track received more submissions. However, evidently, success in this intermediate matching task, does not guarantee success in the overall KPA task, which also requires generating the KPs. Future work should determine whether future KPA shared tasks should focus on the Generation Track, or perhaps modify the evaluation measures of the intermediate Matching Track, such that they better reflect the model performance in the full KPA task. Given the inherent subjective nature of matching arguments to KPs, we opted for a ternary label, allowing argument-KP pairs to receive an "undecided" label, if the annotators votes were inconclusive. These undecided pairs are eventually considered either as positive or as negative matches, resulting with two gold standards, one potentially too strict, while the other perhaps too lenient. In future tasks, it may be valuable to consider a binary labeling scheme that will give rise to a single gold standard. Such labeling may be created conditionally, such that an undecided pair is marked as a positive match if and only if a minimum number of annotators marked it as positive and no other KP was labeled as matching to that argument. A further point for future consideration, is the choice of a pre-defined threshold ofthat guided our evaluation. Although, this has the advantage of not requiring submissions to tune a con- fidence threshold, it has the limitation that it complicates the evaluation, since the ground truth coverage depends on the arguments, topic and stance. A possible alternative would be to require each model to set its own minimum confidence threshold to determine if an argument should not be considered a valid input. Finally, in this task, we used comparative evaluations to estimate the quality of the sets of generated texts. We expect the task and the findings reported in this paper to be relevant for researchers working on text summarization and argument mining. Introduction Key Point Analysis (KPA) was introduced in BarHaim et al. (2020a,b) as a challenging NLP task with tight relations to Computational Argumentation, Opinion Analysis, and Summarization, and with many practical applications (Bar-Haim et al., 2021).