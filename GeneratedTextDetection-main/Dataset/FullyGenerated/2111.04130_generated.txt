Abstract Pretrained language models have become the standard approach for many NLP tasks due to strong performance, but they are very expen- sive to train. We propose a simple and effi- cient learning framework TLM that does not rely on large-scale pretraining1. Given some labeled task data and a large general corpus, TLM uses task data as queries to retrieve a tiny subset of the general corpus and jointly opti- mizes the task objective and the language mod- eling objective from scratch. On eight classifi- cation datasets in four domains, TLM achieves results better than or similar to pretrained lan- guage models (e.g., RoBERTa-Large) while re- ducing the training FLOPs by two orders of magnitude. With high accuracy and efficiency, we hope TLM will contribute to democratizing NLP and expediting its development. Introduction Pretrained language models (PLMs) have drawn much attention from the natural language process- ing (NLP) community. Neural networks based on the Transformer architecture (Vaswani et al., 2017) are trained on large general corpora for self-supervised language modeling tasks such as masked language modeling (Devlin et al., 2018; Liu et al., 2019; Raffel et al., 2019), autoregressive language modeling (Radford et al., 2018; Brown et al., 2020), permutation language modeling (Yang et al., 2020), etc, and then are finetuned on a small amount of labeled data for specific downstream tasks. This pretraining-finetuning framework has significantly improved the performance of many NLP tasks. However, while considered effective, large-scale pretraining is usually computationally expensive. For example, RoBERTa-Large (Liu et al., 2019), a widely-used PLM, consumes a computational cost of 4.36ÃÂ1021 FLOPs3. Larger PLMs such as GPT- 3 (Brown et al., 2020) consume 50 times more FLOPs for training than RoBERTa-Large. The expensiveness of large-scale pretraining prevents many research groups with limited budgets from pretraining customized language models, exploring new neural architectures, or improving pretrain- ing loss functions. In contrast, a large number of NLP researchers resort to improving the finetuning algorithms, whose performance is largely upper- bounded by the pretraining procedure. This creates a high barrier of NLP research and might not be ideal for the long-term development of the field. Even though there have been efforts devoted to studying and improving the efficiency of language model pretraining (So et al., 2021; Tay et al., 2021; Chen et al., 2021; Clark et al., 2020), most of them focus on designing sample-efficient self-supervised tasks or discovering efficient Transformer architec- tures suitable for pretraining. Their improvements are limited, with a reduction of computational costs (in terms of FLOPs) less than one order of mag- nitude. Another line of works target reducing the sizes of PLMs using distillation (Jiao et al., 2019; Sanh et al., 2019) to improve the efficiency of in- ference, but these methods rely on pretraining a large PLM before distillation. Moreover, distilled models often do not perform as well as some of the best non-distilled PLMs such as RoBERTa-Large (Jiao et al., 2019; Sanh et al., 2019). In this work, we explore alternatives to the stan- dard pretraining-finetuning paradigm, aiming at more drastic efficiency improvement without per- formance drop. We propose a simple, efficient, pretraining-free framework, Task-driven LanguageModeling (TLM). Given a large general corpus and some labeled task data, TLM directly trains a model from scratch without relying on PLMs. TLM is motivated by two key ideas. First, hu- mans master a task by using only a small portion of world knowledge (e.g., students only need to re- view a few chapters, among all books in the world, to cram for an exam). We hypothesize that there is much redundancy in the large corpus for a spe- cific task. Second, training on supervised labeled data is much more data efficient for downstream performance than optimizing the language model- ing objective on unlabeled data. Based on these motivations, TLM uses the task data as queries to retrieve a tiny subset of the general corpus. This is followed by jointly optimizing a supervised task objective and a language modelingusing both the retrieved data and the task data. We evaluate TLM on eight different tasks cov- ering the domains of news, review, computer sci- ence, and biomedical science, following the setting of Gururangan et al. (2020). TLM achieves results better than or similar to BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) while reducing the training FLOPs by two orders of magnitude. Related work Pretrained Language Models Pretrained lan- guage models have become the de-facto solution to many of the NLP tasks (Devlin et al., 2018; Liu et al., 2019; Raffel et al., 2019; Radford et al., 2018; Brown et al., 2020; Yang et al., 2020). Those mod- els are usually pretrained on a large-scale corpus in a self-supervised manner to learn a contextual- ized representation of tokens in natural language, and then are fine-tuned with labeled data for spe- cific tasks. BERT (Devlin et al., 2018), one of the most popular PLMs, is pretrained on a 16GB En- glish corpus using a masked language modeling objective (i.e. predicting randomly masked tokens). RoBERTa (Liu et al., 2019) inherits the training objective of BERT, but is pretrained on a larger cor- pus consisting of 160GB English texts with larger batch size and dynamic token masking. In this work, we take both BERT and RoBERTa as our major baselines. Efficient Pretraining for NLP There is a line of work dedicated to improving the efficiency of pretraining language models. You et al. (2019) and Shoeybi et al. (2019) utilized the data and model parallelism across different computational devices to accelerate the pretraining process. However, ac- celerating through parallelism does not actually reduce computational costs in terms of FLOPs for training models at large scale. Chen et al. (2021) and So et al. (2021) tried to identify efficient neu- ral network architectures for language model pre- training, based on the lottery ticket hypothesis and neural architecture search. Such modifications on architecture can bring about 50% Ã¢ÂÂ¼ 70% reduc- tion in computational costs. Clark et al. (2020) and He et al. (2020) incorporated manually designed mechanisms into language model pretraining, such as adversarial training and disentangled represen- tation of content and position, which brings about 50% Ã¢ÂÂ¼ 75% reduction in computational costs. In this work, orthogonal to the aforementioned works, we investigate improving efficiency by reducing training data redundancy. Our approach also re- sults in more drastic improvements. Efficient Inference of Pretrained Models An- other line of work aims at improving inference efficiency of PLMs. Some works improve in- ference efficiency by distilling large PLMs into small-sized models and using the distilled mod- els for inference, such as TinyBERT (Jiao et al., 2019), DistilBERT (Sanh et al., 2019), Mobile- BERT (Sun et al., 2020), FastBERT (Liu et al., 2020), BORT (de Wynter and Perry, 2020), and BERT-of-Theseus (Xu et al., 2020). Other works speed up inference by quantizing PLMs with low- precision representations during inference, such as Q8-BERT (Zafrir et al., 2019), Q-BERT (Shen et al., 2019), and I-BERT (Kim et al., 2021). An- other type of works, such as (Wang et al., 2019; Michel et al., 2019; Gordon et al., 2020), adopt pruning by removing parts of PLMs to make it smaller and faster. However, these methods rely on large PLMs, and the performance after distil- lation, pruning, or quantization often decreases to a certain extent compared with some of the best PLMs (e.g., RoBERTa-Large). In contrast, our ap- proach doesnÃ¢ÂÂt rely on large-scale pre-training and achieves better or at least comparable performance. Domain and Task Adaptation for Pretrained Models Domain-adaptive finetuning is a method that finetunes a pretrained model on in-domain data using a language modeling objective. It has been shown to be effective for domain and task adapta- tion (Gururangan et al., 2020; Zhang et al.,Li et al., 2020; Lee et al., 2020). There are a few crucial differences between domain-adaptive fine- tuning and TLM. First, TLM is a general method to improve training efficiency that does not use any additional domain data. It only utilizes the general corpus as in BERT and RoBERTa. In BERT, all books in the world, to use the example of Boston Globe, ", to improve the efficiency of in- ference, but these methods rely on pretraining a large general corpus. In contrast, a large number of NLP researchers resort to improving the finetuning algorithms, whose performance is largely upper- bounded by the pretraining procedure. This creates a high barrier of NLP research and might not be ideal for the long-term development of the field. Second, while there have been efforts devoted to studying and improving the efficiency of language model pretraining (So et al., 2021; Tay et al., 2021; Chen et al., 2021; Clark et al., 2020), most of them focus on designing sample-efficient self-supervised tasks or discovering efficient Transformer architec- tures suitable for pretraining. Their improvements are limited, with a reduction of computational costs (in terms of FLOPs) less than one order of mag- nitude. Another line of works target reducing the sizes of PLMs using distillation (Jiao et al., 2019; Sanh et al., 2019) to improve the efficiency of in- ference, but these methods rely on pretraining a large PLM before distillation. Moreover, distilled models often do not perform as well as some of the best non-distilled PLMs such as RoBERTa-Large (Jiao et al., 2019; Sanh et al., 2019).