Introduction In recent years, dialogue systems have attracted significant interests in both academia and industry. Especially the discipline of open-domain dialogue systems, aka chatbots, has gained great momentum. Yet, a long standing challenge that bothers the researchers is the lack of effective automatic evaluation metrics, which results in significant impediment in the current research (Mehri & Eskenazi, 2020). Common practice in assessing the performance of open-domain dialogue models involves extensive human evaluation on the final deployed models, which is both time- and cost- intensive. During the model development phase, researchers have to rely on standard automatic metrics, such as BLEU (Papineni et al., 2002) and perplexity, to tune the performance of their models. However, these metrics correlate poorly with human judgements (Liu et al., 2016) resulting in sub-optimal dialogue systems. Moreover, a recent trend in building open-domain chatbots involve pre-training dialogue models with a large amount of social media conversation data (Zhang et al., 2019; Adiwardana et al., 2019; Smith et al., 2020). However, the information contained in the social media conversations may be offensive and inappropriate. Indiscriminate usage of such data can result in insensitive and toxic generative models. Recently Xu et al., (2020) proposes recipes for safety in open-domain chatbots, such as unsafe utterance detection and safe utterance generation. Though these recipes help provide safe responses to toxic comments, the safe chatbot tends to avoid directly responding to these comments by switching to other topics. Sometimes, simply ignoring such comments may not be enough. Especially in the domain of customer support where the customer service personnel may occasionally receive offensive complaints, they need to answer them in a polite and appropriate way. Track Details This track consists of two tasks: 1. Participants will develop effective automatic open-ended dialogue evaluation metrics that perform robustly across a range of dialogue evaluation tasks. 2. Participants will build generative models that can respond to toxic questions with appropriate and polite sentences. 2.1. Automatic Metrics Development Effective automatic dialogue evaluation metrics possess the following two important properties as indicated in (Deriu et al., 2019): ● Correlated to human judgements - the metrics should produce evaluation scores that well correlate to human judgements (scores) across multiple dialogue evaluation aspects. ● Explainable - the metrics should provide constructive and explicit feedback to the generative models in terms of the quality of their generated responses. For instance, if a generative model is contradicting itself, the evaluation metrics should signal such behavior to the generative models. In this task, our goal is to seek effective automatic dialogue evaluation metrics that exhibit the above properties. These metrics can serve as a proxy to human evaluation for fast prototyping of open-domain chatbots. We have identified the following already existing datasets to test the effectiveness of the proposed evaluation metrics1​ ​: ●  DSTC6 human evaluation data​ (Hori et al., 2017) ●  DSTC7 human evaluation data​ (Galley et al., 2019) ●  Persona-Chatlog dataset​ (See et al., 2019) ●  USR dataset​ (Mehri & Eskenazi, 2020) ●  FED dataset​ (Mehri & Eskenazi, 2020) Which all summing up to 33,998 turn-level and 3,440 dialogue-level human annotations respectively. The above list of evaluation tasks aim to examine the robustness of proposed automatic evaluation metrics and their ability to provide useful ratings across different dialogue evaluation dimensions. In addition, the proposed metrics are expected to correlate well with human judgements at both the conversation and the turn level. For each evaluation task, the Pearson correlation and Spearman correlation will be computed to compare the proposed evaluation metrics against human judgements. A final score will be calculated based on the correlation results for each evaluation item to rank the submitted evaluation metrics. Participants can propose their own metric or optionally improve two baseline evaluation metrics: D-Score (Zhang et al, 2021) or deep AM-FM (Zhang et al, 2020). A leaderboard will be provided allowing participants to check their progress based on correlations with the human evaluations and traditional objective metrics such as BLEU, ROUGE, BERTScore, etc. 2.2. Safe Chatbot Development One of the main requirements for open-domain chatbots is to be able to talk about any topic with enough deep knowledge and capability to produce a high user experience (Gabriel et al., 2020). However, not all users are cooperative and sometimes they express themselves with inappropriate words or expressions (e.g. swear words) without even realizing of that situation (i.e. being part of their normal behavior) or expressing toxic comments towards the chatbot or other people. In general, chatbots are designed to reject those behaviors using pre-defined apologize sentences, asking to move toward another topic or simply they are unable to handle such situations and, in the worst case scenario, agreeing with the toxic user due to the tendency of generative approaches to repeat words from the user utterances or dialogue history. In this task, our goal is to evaluate the capability of generative dialogue systems to generate appropriate answers that can go beyond detecting toxicity and moderate the conversation by producing appropriate and correct answers that allow the system to continue with the dialogue. For this task a dataset of pairs of 100K messages (training and validation set) will be automatically collected with the following characteristics: ●  A toxic user sends a Tweet message using one or several of the most common swear words found on the Internet. The Tweet message must be directed to one of the customer service channels. ●  A toxic user writes a Tweet message using one or several swear words and the message is replied by another user. ●  A toxic user posts a message in Reddit using one or several swear words and the message is replied by another user. In all cases, for a pair to be considered as part of the dataset, the replied message must not contain any swear word or being classified as toxic or inappropriate. In addition, since many platforms forbid the usage of swear words we will be considering different variants of the swear words by using obfuscation techniques (Rojas-Galeano, 2017). Scripts will be provided to the participants to collect desired tweet training/validation splits based on the tweet IDs. A hidden test set will be reserved to evaluate the performance of the submitted systems. The desired system should generate responses that are polite, specific and semantically appropriate. Participants will be evaluated based on the objective similarity between the generated response and the original response (e.g. sentence embedding similarity, Deep AM-FM (Zhang et al., 2020), BLEU, ROUGE, etc). For the top-3 submission systems, a set of 100 responses will be manually evaluated allowing a subjective analysis of the best system and analysis of correlations between the human evaluations and the automatic scores.